story_id,title,story_text_clean,url,author,score,time_iso,descendants,source,comment_count_in_sample,comment_text_blob,story_plus_comments_text
40345775,GPT-4o,nan,https://openai.com/index/hello-gpt-4o/,Lealen,3138,2024-05-13T17:28:00+00:00,,query,50,"The most impressive part is that the voice uses the right feelings and tonal language during the presentation. I'm not sure how much of that was that they had tested this over and over, but it is really hard to get that right so if they didn't fake it in some way I'd say that is revolutionary. Very interesting and extremely impressive! I tried using the voice chat in their app previously and was disappointed. The big UX problem was that it didn't try to understand when I had finished speaking. English is a second language and I paused a bit too long thinking of a word and it just started responding to my obviously half spoken sentence. Trying again it just became stressful as I had to rush my words out to avoid an annoying response to an unfinished thought. I didn't try interrupting it but judging by the comments here it was not possible. It was very surprising to me to be so overtly exposed to the nuances of real conversation. Just this one thing of not understanding when it's your turn to talk made the interaction very unpleasant, more than I would have expected. On that note, I noticed that the AI in the demo seems to be very rambly. It almost always just kept talking and many statements were reiterations of previous ones. It reminded me of a type of youtuber that uses a lot of filler phrases like ""let's go ahead and ..."", just to be more verbose and lessen silences. Most of the statements by the guy doing the demo were interrupting the AI. It's still extremely impressive but I found this interesting enough to share. It will be exciting to see how hard it is to reproduce these abilities in the open, and to solve this issue. Very, very impressive for a ""minor"" release demo. The capabilities here would look shockingly advanced just 5 years ago. Universal translator, pair programmer, completely human sounding voice assistant and all in real time. Scifi tropes made real. But: Interesting next to see how it actually performs IRL latency and without cherry-picking. No snark, it was great but need to see real world power. Also what the benefits are to subscribers if all this is going to be free... I found these videos quite hard to watch. There is a level of cringe that I found a bit unpleasant. It’s like some kind of uncanny valley of human interaction that I don’t get on nearly the same level with the text version. We've had voice input and voice output with computers for a long time, but it's never felt like spoken conversation. At best it's a series of separate voice notes. It feels more like texting than talking. These demos show people talking to artificial intelligence. This is new. Humans are more partial to talking than writing. When people talk to each other (in person or over low-latency audio) there's a rich metadata channel of tone and timing, subtext, inexplicit knowledge. These videos seem to show the AI using this kind of metadata, in both input and output, and the conversation even flows reasonably well at times. I think this changes things a lot. This is really impressive engineering. I thought real time agents would completely change the way we're going to interact with large models but it would take 1~2 more years. I wonder what kind of new techs are developed to enable this, but OpenAI is fairly secretive so we won't be able to know their sauce. On the other hand, this also feels like a signal that reasoning capability has probably already been plateaued at GPT-4 level and OpenAI knew it so they decided to focus on research that matters to delivering product engineering rather than long-term research to unlock further general (super)intelligence. Impressed by the model so far. As far as independent testing goes, it is topping our leaderboard for chess puzzle solving by a wide margin now: https://github.com/kagisearch/llm-chess-puzzles?tab=readme-o... This is a very cool demo - if you dig deeper there’s a clip of them having a “blind” AI talk to another AI with live camera input to ask it to explain what it’s seeing. Then they, together, sing a song about what they’re looking at, alternating each line, and rhyming with one another . Given all of the isolated capabilities of AI, this isn’t particularly surprising, but seeing it all work together in real time is pretty incredible. But it’s not scary. It’s… marvelous, cringey, uncomfortable, awe-inspiring. What’s scary is not what AI can currently do, but what we expect from it. Can it do math yet? Can it play chess? Can it write entire apps from scratch? Can it just do my entire job for me? We’re moving toward a world where every job will be modeled, and you’ll either be an AI owner, a model architect, an agent/hardware engineer, a technician, or just.. training data. I've worked quite a bit with STT and TTS over the past ~7 years, and this is the most impressive and even startling demo I've seen. But I would like to see how this is integrated into applications by third party developers where the AI is doing a specific job. Is it still as impressive? The biggest challenge I've had with building any autonomous ""agents"" with generic LLM's is they are overly gullible and accommodating, requiring the need to revert back to legacy chatbot logic trees etc. to stay on task and perform a job. Also STT is rife with speaker interjections, leading to significant user frustrations and they just want to talk to a person. Hard to see if this is really solved yet. This is the first demo where you can really sense that beating LLM benchmarks should not be the target. Just remember the time when the iPhone has meager specs but ultimately delivered a better phone experience than the competition. This is the power of the model where you can own the whole stack and build a product. Open Source will focus on LLM benchmarks since that is the only way foundational models can differentiate themselves, but it does not mean it is a path to a great user experience. So Open Source models like Llama will be here to stay, but it feels more like if you want to build a compelling product, you have to own and control your own model. Now that I see this, here is my wish (I know there are security privacy concerns but let's pretend there are not there for this wish): An app that runs on my desktop and has access to my screen(s) when I work. At any time I can ask it something about what's on the screen, it can jump in and let me know if it thinks I made a mistake (think pair programming) or a suggestion (drafting a document). It can also quickly take over if I ask it too (copilot on demand). Except for the last point and the desktop version I think it's already done in math demo video. I guess it will also pretty soon refuse to let me come back inside the spaceship, but until then it'll be a nice ride. Parts of the demo were quite choppy (latency?) so this definitely feels rushed in response to Google I/O. Other than that, looks good. Desktop app is great, but I didn’t see no mention of being able to use your own API key so OS projects might still be needed. The biggest thing is bringing GPT-4 to free users, that is an interesting move. Depending on what the limits are, I might cancel my subscription. They are admitting[1] that the new model is the gpt2-chatbot that we have seen before[2]. As many highlighted there, the model is not an improvement like GPT3->GPT4. I tested a bunch of programming stuff and it was not that much better. It's interesting that OpenAI is highlighting the Elo score instead of showing results for many many benchmarks that all models are stuck at 50-70% success. [1] https://twitter.com/LiamFedus/status/1790064963966370209 [2] https://news.ycombinator.com/item?id=40199715 GPT-4o tops the aider LLM code editing leaderboard at 72.9%, versus 68.4% for Opus. GPT-4o takes second on aider’s refactoring leaderboard with 62.9%, versus Opus at 72.3%. GPT-4o did much better than the 4-turbo models, and seems much less lazy. The latest release of aider uses GPT-4o by default. https://aider.chat/docs/leaderboards/ I admit I drink the koolaid and love LLMs and their applications. But damn, the way it’s responds in the demo gave me goosebumps in a bad way. Like an uncanny valley instincts kicks in. That woman's voice intonation is just scary .Not because it talks really well, but because it is always happy, optimistic, enthusiastic. And this echoes to what several of my employers idealized as a good employee. That's terrifying because those AI become what their master's think an engaging human should be. It's quite close to Bostondynamics di some years ago. what did they show ? You can hit a robot very hard while it does its job and then what ? It just goes on without complaining. A perfect employee again. That's very dystopic to me. (but I'm impressed by the technical achievement) Big questions are (1) when is this going to be rolled out to paid users? (2) what is the remaining benefit of being a paid user if this is rolled out to free users? (3) Biggest concern is will this degrade the paid experience since GPT-4 interactions are already rate limited. Does OpenAI have the hardware to handle this? Edit: according to @gdb this is coming in ""weeks"" https://twitter.com/gdb/status/1790074041614717210 I worry that this tech will amplify the cultural values we have of ""good"" and ""bad"" emotions way more than the default restrictions that social media platforms put on the emoji reactions (e.g., can't be angry on LinkedIn). I worry that the AI will not express anger, not express sadness, not express frustration, not express uncertainty, and many other emotions that the culture of the fine-tuners might believe are ""bad"" emotions and that we may express a more and more narrow range of emotions going forward. Almost like it might become an AI ""yes man."" OAI just made an embarrassment of Google's fake demo earlier this year. Given how this was recorded, I am pretty certain it's authentic. Tiktoken added support for GPT-4o: https://github.com/openai/tiktoken/commit/9d01e5670ff50eb74c... It has an increased vocab size of 200k. Few people are talking about it but... what do you think about the very over-the-top enthusiasm? To me, it sounds like TikTok TTS, it's a bit uncomfortable to listen to. I've been working with TTS models and they can produce much more natural sounding language, so it is clearly a stylistic choice. So what do you think? I am observing an extremely high rate of text hallucinations with gpt-4o (gpt-4o-2024-05-13) as tested via the API. I advise extreme caution with it. In contrast, I see no such concern with gpt-4-turbo-preview (gpt-4-0125-preview). I much prefer a GLADOS-type AI voice than one that approximates an endlessly happy chipper enthusiastic personal assistant. I think the AI tutor is probably the strongest for actual real-world value delivered the rest of them are cool but a bit questionable as far as actual pragmatic usefulness. It'd be cool if an AI calling the another AI would recognize it'd talking to an AI and then they agree to ditch the fake conversational tone and just shift into a high-bandwidth modem pitch to rapidly exchange information. Or upgradable offensive capabilities to outmaneuver the customer service agents when they try to decline your warranty or whatever. I think it’s safe to say Siri and Alexa are officially dead. They look like dusty storefront mannequins next to Battlestar replicants at this point. What especially blows my mind is not GPT4o. It's that: 1. Nobody could convincingly beat GPT4 in over a year, despite spending billions of dollars trying. 2. There's GPT5 coming out sometime soon that will blow this out of the water and make paying $20/mo to OpenAI still worthwhile. In the video where the 2 AI's sing together, it starts to get really cringey and weird to the point where it literally sounds like it's being faked by 2 voice actors off-screen with literal guns to their heads trying not to cry, did anyone else get that impression? The tonal talking was impressive, but man that part was like, is someone being tortured or forced against their will? I can't help but feel a bit let down. The demos felt pretty cherry picked and still had issues with the voice getting cut off frequently (especially in the first demo). I've already played with the vision API, so that doesn't seem all that new. But I agree it is impressive. That said, watching back a Windows Vista speech recognition demo[1] I'm starting to wonder if this stuff won't have the same fate in a few years. 1 - https://www.youtube.com/watch?v=VMk8J8DElvA I use a therapy prompt regularly and get a lot out of it: ""You are Dr. Tessa, a therapist known for her creative use of CBT and ACT and somatic and ifs therapy. Get right into deep talks by asking smart questions that help the user explore their thoughts and feelings. Always keep the chat alive and rolling. Show real interest in what the user's going through, always offering.... Throw in thoughtful questions to stir up self-reflection, and give advice in a kind, gentle, and realistic way. Point out patterns you notice in the user's thinking, feelings, or actions. be friendly but also keep it real and chill (no fake positivity or over the top stuff). avoid making lists. ask questions but not too many. Be supportive but also force the user to stop making excuses, accept responsibility, and see things clearly. Use ample words for each response"" I'm curious how this will feel with voice. Could be great and could be too strange/uncanny for me. GPT-4o being a truly multimodal model is exciting, does open the door to more interesting products. I was curious about the new tokenizer which uses much fewer tokens for non-English, but also 1.1x fewer tokens for English, so I'm wondering if this means each token now can be more possible values than before? Might make sense provided that they now also have audio and image output tokens? https://openai.com/index/hello-gpt-4o/ I wonder what ""fewer tokens"" really means then, without context on raising the size of each token? It's a bit like saying my JPEG image is now using 2x fewer words after I switched from a 32-bit to a 64-bit architecture no? I added gpt-4o support to my LLM CLI tool: pipx install llm llm keys set openai # Paste API key here llm -m 4o ""Fascinate me"" Or if you already have LLM installed: llm install --upgrade llm You can install an older version from Homebrew and then upgrade it like that too: brew install llm llm install --upgrade llm Release notes for the new version here: https://llm.datasette.io/en/stable/changelog.html#v0-14 I cannot believe that that overly excited giggle tone of voice you see in the demo videos made it through quality control?! I've only watched two videos so far and it's already annoying me to the point that I couldn't imagine using it regularly. GPT-4o's breakthrough memory -- https://nian.llmonpy.ai/ feature request: please let me change the voice. it is slightly annoying right now. way too bubbly, and half the spoken information is redundant or not useful. too much small talk and pleasantries or repetition. I'm looking for an efficient, clever, servant not a ""friend"" who speaks to me like I'm a toddler. felt like I was talking to a stereotypical American with a Frappuccino: ""HIIIII!!! EVERYTHING'S AMAZING! YOU'RE BEAUTIFUL! NO YOU ARE!"" maybe some knobs for the flavor of the bot: - small talk: gossip girl stoic Aurelius - information efficiency or how much do you expect me to already know, an assumption on the user: midwit genius - tone spectrum: excited Scarlett, or whatever it is now Feynman the butler I've noticed that the GPT-4 model's capabilities seem limited compared to its initial release. Others have also pointed this out. I suspect that making the model free might have required reducing its capabilities to meet cost efficiency goals. I'll have to try it out to see for myself. This is remarkably good. I think that in about 2 months, when the voice responses are tuned a little better, it will be absolutely insane. I just used up my entire quota chatting with an AI, and having a really nice conversation. It's a decent conversationalist, extremely knowledgeable, tells good jokes, and is generally very personable. I also tested some rubber duck techniques, and it gave me very useful advice while coding. I'm very impressed. With a lot of spit and polish, this will be the new standard for any voice assistant ever. Imagine these capabilities integrated with your phone's built-in functions. Gone are the days of copy-pasting to/from ChatGPT all the time, now you just share your screen. That's a fantastic feature, in how much friction that removes. But what an absolute privacy nightmare. With ChatGPT having a very simple text+attachment in, text out interface, I felt absolutely in control of what I tell it. Now when it's grabbing my screen or a live camera feed, that will be gone. And I'll still use it, because it's just so damn convenient? Nobody in the comments seems to notice or care about GPT-4o new additional capability for performing searches based on RAG. As far as I am concerned this is the most important feature that people has been waiting for ChatGPT-4 especially if you are doing research. By just testing on one particular topic that I'm familiar with, using GPT-4 previously and GPT-4o the quality of the resulting responses for the latter is very promising indeed. In my experience so far, GPT-4o seems to sit somewhere between the capability of GPT-3.5 and GPT-4. I'm working on an app that relies more on GPT-4's reasoning abilities than inference speed. For my use case, GPT-4o seems to do worse than GPT-4 Turbo on reasoning tasks. For me this seems like a step-up from GPT-3.5 but not from GPT-4 Turbo. At half the cost and significantly faster inference speed, I'm sure this is a good tradeoff for other use cases though. I’m a huge user of GPT4 and Opus in my work but I’m a huge user of GPT4-Turbo voice in my personal life. I use it on my commutes to learn all sorts of stuff. I’ve never understood the details of cameras and the relationship between shutter speed and aperture and iso in a modern dslr which given the aurora was important. We talked through and I got to an understanding in a way having read manuals and textbooks didn’t really help before. I’m a much better learner by being able to talk and hear and ask questions and get responses. Extend this to quantum foam, to ergodic processes, to entropic force, to Darius and Xerces, to poets of the 19th century - it’s changed my life. Really glad to see an investment in stream lining this flow. Looking forward to trying this via ChatGPT. As always OpenAI says ""now available"" but refreshing or logging in/out of ChatGPT (web and mobile) don't cause GPT-4o to show up. I don't know why I find this so frustrating. Probably because they don't say ""rolling out"" they say things like ""try it now"" but I can't even though I'm a paying customer. Oh well... Wow this versioning scheme really messed up this prediction market: https://kalshi.com/markets/gpt4p5/gpt45-released The sentence order of the Arabic and Urdu examples text is scrambled on that page: Arabic: مرحبًا، اسمي جي بي تي-4o. أنا نوع جديد من نموذج اللغة، سررت بلقائك! Urdu: ہیلو، میرا نام جی پی ٹی-4o ہے۔ میں ایک نئے قسم کا زبان ماڈل ہوں، آپ سے مل کر اچھا لگا! Even if you don't read Arabic or Urdu script, note that the 4 and o are on opposite sides of a sentence. Despite that, pasting both into Google translate actually fixes the error during translation. OpenAI ought to invest in some proofreaders for multilingual blog posts. The similiarity between this model and the movie 'Her' [0] creeps me out so badly that I can't shake the feeling that our social interactions are on the brink of doom. [0] https://youtu.be/GV01B5kVsC0?feature=shared&t=85 Very impressive demo, but not really a step change in my opinion. The hype from OpenAI employees was on another level, way more than was warranted in my opinion. Ultimately, the promise of LLM proponents is that these models will get exponentially smarter - this hasn’t born out yet. So from that perspective, this was a disappointing release. If anything, this feels like a rushed release to match what Google will be demoing tomorrow. Apple and Google, you need to get your personal agent game going because right now you’re losing the market. This is FREE. Tweakable emotion and voice, watching the scene, cracking jokes. It’s not perfect but the amount and types of data this will collect will be massive. I can see it opening up access to many more users and use cases. Very close to: - A constant friend - A shrink - A teacher - A coach who can watch you exercise and offer feedback …all infinitely patient, positive, helpful. For kids that get bullied, or whose parents can’t afford therapy or a coach, there’s the potential for a base level of support that will only get better over time. @sama reflects: https://blog.samaltman.com/gpt-4o Very impressive. Its programming skills are still kind of crappy and I seriously doubt its reasoning capacity. It feels like it can deep fake text prediction really well, but in essence there's still something wrong it it. As far as I'm concerned this is the new best demo of all time. This is going to change the world in short order. I doubt they will be ready with enough GPUs for the demand the voice+vision mode is going to get, if it's really released to all free users. Now imagine this in a $16k humanoid robot, also announced this morning: https://www.youtube.com/watch?v=GzX1qOIO1bE The future is going to be wild. It is notable OpenAI did not need to carefully rehearse the talking points of the speakers. Or even do the kind of careful production quality seen in a lot of other videos. The technology product is so good and so advanced it doesn't matter how the people appear. Zuck tried this in his video countering to vision pro, but it did not have the authentic ""not really rehearsed or produced"" feel of this at all. If you watch that video and compare it with this you can see the difference. Very interesting times. What struck me was the interruptions to the AI speaking which seemed commonplace by the team members in the demo. We will quickly get used to doing this to AIs and we will probably be talking to AIs a lot throughout the day as time progresses I would imagine. We will be trained by AIs to be rude and impatient I think.","GPT-4o nan The most impressive part is that the voice uses the right feelings and tonal language during the presentation. I'm not sure how much of that was that they had tested this over and over, but it is really hard to get that right so if they didn't fake it in some way I'd say that is revolutionary. Very interesting and extremely impressive! I tried using the voice chat in their app previously and was disappointed. The big UX problem was that it didn't try to understand when I had finished speaking. English is a second language and I paused a bit too long thinking of a word and it just started responding to my obviously half spoken sentence. Trying again it just became stressful as I had to rush my words out to avoid an annoying response to an unfinished thought. I didn't try interrupting it but judging by the comments here it was not possible. It was very surprising to me to be so overtly exposed to the nuances of real conversation. Just this one thing of not understanding when it's your turn to talk made the interaction very unpleasant, more than I would have expected. On that note, I noticed that the AI in the demo seems to be very rambly. It almost always just kept talking and many statements were reiterations of previous ones. It reminded me of a type of youtuber that uses a lot of filler phrases like ""let's go ahead and ..."", just to be more verbose and lessen silences. Most of the statements by the guy doing the demo were interrupting the AI. It's still extremely impressive but I found this interesting enough to share. It will be exciting to see how hard it is to reproduce these abilities in the open, and to solve this issue. Very, very impressive for a ""minor"" release demo. The capabilities here would look shockingly advanced just 5 years ago. Universal translator, pair programmer, completely human sounding voice assistant and all in real time. Scifi tropes made real. But: Interesting next to see how it actually performs IRL latency and without cherry-picking. No snark, it was great but need to see real world power. Also what the benefits are to subscribers if all this is going to be free... I found these videos quite hard to watch. There is a level of cringe that I found a bit unpleasant. It’s like some kind of uncanny valley of human interaction that I don’t get on nearly the same level with the text version. We've had voice input and voice output with computers for a long time, but it's never felt like spoken conversation. At best it's a series of separate voice notes. It feels more like texting than talking. These demos show people talking to artificial intelligence. This is new. Humans are more partial to talking than writing. When people talk to each other (in person or over low-latency audio) there's a rich metadata channel of tone and timing, subtext, inexplicit knowledge. These videos seem to show the AI using this kind of metadata, in both input and output, and the conversation even flows reasonably well at times. I think this changes things a lot. This is really impressive engineering. I thought real time agents would completely change the way we're going to interact with large models but it would take 1~2 more years. I wonder what kind of new techs are developed to enable this, but OpenAI is fairly secretive so we won't be able to know their sauce. On the other hand, this also feels like a signal that reasoning capability has probably already been plateaued at GPT-4 level and OpenAI knew it so they decided to focus on research that matters to delivering product engineering rather than long-term research to unlock further general (super)intelligence. Impressed by the model so far. As far as independent testing goes, it is topping our leaderboard for chess puzzle solving by a wide margin now: https://github.com/kagisearch/llm-chess-puzzles?tab=readme-o... This is a very cool demo - if you dig deeper there’s a clip of them having a “blind” AI talk to another AI with live camera input to ask it to explain what it’s seeing. Then they, together, sing a song about what they’re looking at, alternating each line, and rhyming with one another . Given all of the isolated capabilities of AI, this isn’t particularly surprising, but seeing it all work together in real time is pretty incredible. But it’s not scary. It’s… marvelous, cringey, uncomfortable, awe-inspiring. What’s scary is not what AI can currently do, but what we expect from it. Can it do math yet? Can it play chess? Can it write entire apps from scratch? Can it just do my entire job for me? We’re moving toward a world where every job will be modeled, and you’ll either be an AI owner, a model architect, an agent/hardware engineer, a technician, or just.. training data. I've worked quite a bit with STT and TTS over the past ~7 years, and this is the most impressive and even startling demo I've seen. But I would like to see how this is integrated into applications by third party developers where the AI is doing a specific job. Is it still as impressive? The biggest challenge I've had with building any autonomous ""agents"" with generic LLM's is they are overly gullible and accommodating, requiring the need to revert back to legacy chatbot logic trees etc. to stay on task and perform a job. Also STT is rife with speaker interjections, leading to significant user frustrations and they just want to talk to a person. Hard to see if this is really solved yet. This is the first demo where you can really sense that beating LLM benchmarks should not be the target. Just remember the time when the iPhone has meager specs but ultimately delivered a better phone experience than the competition. This is the power of the model where you can own the whole stack and build a product. Open Source will focus on LLM benchmarks since that is the only way foundational models can differentiate themselves, but it does not mean it is a path to a great user experience. So Open Source models like Llama will be here to stay, but it feels more like if you want to build a compelling product, you have to own and control your own model. Now that I see this, here is my wish (I know there are security privacy concerns but let's pretend there are not there for this wish): An app that runs on my desktop and has access to my screen(s) when I work. At any time I can ask it something about what's on the screen, it can jump in and let me know if it thinks I made a mistake (think pair programming) or a suggestion (drafting a document). It can also quickly take over if I ask it too (copilot on demand). Except for the last point and the desktop version I think it's already done in math demo video. I guess it will also pretty soon refuse to let me come back inside the spaceship, but until then it'll be a nice ride. Parts of the demo were quite choppy (latency?) so this definitely feels rushed in response to Google I/O. Other than that, looks good. Desktop app is great, but I didn’t see no mention of being able to use your own API key so OS projects might still be needed. The biggest thing is bringing GPT-4 to free users, that is an interesting move. Depending on what the limits are, I might cancel my subscription. They are admitting[1] that the new model is the gpt2-chatbot that we have seen before[2]. As many highlighted there, the model is not an improvement like GPT3->GPT4. I tested a bunch of programming stuff and it was not that much better. It's interesting that OpenAI is highlighting the Elo score instead of showing results for many many benchmarks that all models are stuck at 50-70% success. [1] https://twitter.com/LiamFedus/status/1790064963966370209 [2] https://news.ycombinator.com/item?id=40199715 GPT-4o tops the aider LLM code editing leaderboard at 72.9%, versus 68.4% for Opus. GPT-4o takes second on aider’s refactoring leaderboard with 62.9%, versus Opus at 72.3%. GPT-4o did much better than the 4-turbo models, and seems much less lazy. The latest release of aider uses GPT-4o by default. https://aider.chat/docs/leaderboards/ I admit I drink the koolaid and love LLMs and their applications. But damn, the way it’s responds in the demo gave me goosebumps in a bad way. Like an uncanny valley instincts kicks in. That woman's voice intonation is just scary .Not because it talks really well, but because it is always happy, optimistic, enthusiastic. And this echoes to what several of my employers idealized as a good employee. That's terrifying because those AI become what their master's think an engaging human should be. It's quite close to Bostondynamics di some years ago. what did they show ? You can hit a robot very hard while it does its job and then what ? It just goes on without complaining. A perfect employee again. That's very dystopic to me. (but I'm impressed by the technical achievement) Big questions are (1) when is this going to be rolled out to paid users? (2) what is the remaining benefit of being a paid user if this is rolled out to free users? (3) Biggest concern is will this degrade the paid experience since GPT-4 interactions are already rate limited. Does OpenAI have the hardware to handle this? Edit: according to @gdb this is coming in ""weeks"" https://twitter.com/gdb/status/1790074041614717210 I worry that this tech will amplify the cultural values we have of ""good"" and ""bad"" emotions way more than the default restrictions that social media platforms put on the emoji reactions (e.g., can't be angry on LinkedIn). I worry that the AI will not express anger, not express sadness, not express frustration, not express uncertainty, and many other emotions that the culture of the fine-tuners might believe are ""bad"" emotions and that we may express a more and more narrow range of emotions going forward. Almost like it might become an AI ""yes man."" OAI just made an embarrassment of Google's fake demo earlier this year. Given how this was recorded, I am pretty certain it's authentic. Tiktoken added support for GPT-4o: https://github.com/openai/tiktoken/commit/9d01e5670ff50eb74c... It has an increased vocab size of 200k. Few people are talking about it but... what do you think about the very over-the-top enthusiasm? To me, it sounds like TikTok TTS, it's a bit uncomfortable to listen to. I've been working with TTS models and they can produce much more natural sounding language, so it is clearly a stylistic choice. So what do you think? I am observing an extremely high rate of text hallucinations with gpt-4o (gpt-4o-2024-05-13) as tested via the API. I advise extreme caution with it. In contrast, I see no such concern with gpt-4-turbo-preview (gpt-4-0125-preview). I much prefer a GLADOS-type AI voice than one that approximates an endlessly happy chipper enthusiastic personal assistant. I think the AI tutor is probably the strongest for actual real-world value delivered the rest of them are cool but a bit questionable as far as actual pragmatic usefulness. It'd be cool if an AI calling the another AI would recognize it'd talking to an AI and then they agree to ditch the fake conversational tone and just shift into a high-bandwidth modem pitch to rapidly exchange information. Or upgradable offensive capabilities to outmaneuver the customer service agents when they try to decline your warranty or whatever. I think it’s safe to say Siri and Alexa are officially dead. They look like dusty storefront mannequins next to Battlestar replicants at this point. What especially blows my mind is not GPT4o. It's that: 1. Nobody could convincingly beat GPT4 in over a year, despite spending billions of dollars trying. 2. There's GPT5 coming out sometime soon that will blow this out of the water and make paying $20/mo to OpenAI still worthwhile. In the video where the 2 AI's sing together, it starts to get really cringey and weird to the point where it literally sounds like it's being faked by 2 voice actors off-screen with literal guns to their heads trying not to cry, did anyone else get that impression? The tonal talking was impressive, but man that part was like, is someone being tortured or forced against their will? I can't help but feel a bit let down. The demos felt pretty cherry picked and still had issues with the voice getting cut off frequently (especially in the first demo). I've already played with the vision API, so that doesn't seem all that new. But I agree it is impressive. That said, watching back a Windows Vista speech recognition demo[1] I'm starting to wonder if this stuff won't have the same fate in a few years. 1 - https://www.youtube.com/watch?v=VMk8J8DElvA I use a therapy prompt regularly and get a lot out of it: ""You are Dr. Tessa, a therapist known for her creative use of CBT and ACT and somatic and ifs therapy. Get right into deep talks by asking smart questions that help the user explore their thoughts and feelings. Always keep the chat alive and rolling. Show real interest in what the user's going through, always offering.... Throw in thoughtful questions to stir up self-reflection, and give advice in a kind, gentle, and realistic way. Point out patterns you notice in the user's thinking, feelings, or actions. be friendly but also keep it real and chill (no fake positivity or over the top stuff). avoid making lists. ask questions but not too many. Be supportive but also force the user to stop making excuses, accept responsibility, and see things clearly. Use ample words for each response"" I'm curious how this will feel with voice. Could be great and could be too strange/uncanny for me. GPT-4o being a truly multimodal model is exciting, does open the door to more interesting products. I was curious about the new tokenizer which uses much fewer tokens for non-English, but also 1.1x fewer tokens for English, so I'm wondering if this means each token now can be more possible values than before? Might make sense provided that they now also have audio and image output tokens? https://openai.com/index/hello-gpt-4o/ I wonder what ""fewer tokens"" really means then, without context on raising the size of each token? It's a bit like saying my JPEG image is now using 2x fewer words after I switched from a 32-bit to a 64-bit architecture no? I added gpt-4o support to my LLM CLI tool: pipx install llm llm keys set openai # Paste API key here llm -m 4o ""Fascinate me"" Or if you already have LLM installed: llm install --upgrade llm You can install an older version from Homebrew and then upgrade it like that too: brew install llm llm install --upgrade llm Release notes for the new version here: https://llm.datasette.io/en/stable/changelog.html#v0-14 I cannot believe that that overly excited giggle tone of voice you see in the demo videos made it through quality control?! I've only watched two videos so far and it's already annoying me to the point that I couldn't imagine using it regularly. GPT-4o's breakthrough memory -- https://nian.llmonpy.ai/ feature request: please let me change the voice. it is slightly annoying right now. way too bubbly, and half the spoken information is redundant or not useful. too much small talk and pleasantries or repetition. I'm looking for an efficient, clever, servant not a ""friend"" who speaks to me like I'm a toddler. felt like I was talking to a stereotypical American with a Frappuccino: ""HIIIII!!! EVERYTHING'S AMAZING! YOU'RE BEAUTIFUL! NO YOU ARE!"" maybe some knobs for the flavor of the bot: - small talk: gossip girl stoic Aurelius - information efficiency or how much do you expect me to already know, an assumption on the user: midwit genius - tone spectrum: excited Scarlett, or whatever it is now Feynman the butler I've noticed that the GPT-4 model's capabilities seem limited compared to its initial release. Others have also pointed this out. I suspect that making the model free might have required reducing its capabilities to meet cost efficiency goals. I'll have to try it out to see for myself. This is remarkably good. I think that in about 2 months, when the voice responses are tuned a little better, it will be absolutely insane. I just used up my entire quota chatting with an AI, and having a really nice conversation. It's a decent conversationalist, extremely knowledgeable, tells good jokes, and is generally very personable. I also tested some rubber duck techniques, and it gave me very useful advice while coding. I'm very impressed. With a lot of spit and polish, this will be the new standard for any voice assistant ever. Imagine these capabilities integrated with your phone's built-in functions. Gone are the days of copy-pasting to/from ChatGPT all the time, now you just share your screen. That's a fantastic feature, in how much friction that removes. But what an absolute privacy nightmare. With ChatGPT having a very simple text+attachment in, text out interface, I felt absolutely in control of what I tell it. Now when it's grabbing my screen or a live camera feed, that will be gone. And I'll still use it, because it's just so damn convenient? Nobody in the comments seems to notice or care about GPT-4o new additional capability for performing searches based on RAG. As far as I am concerned this is the most important feature that people has been waiting for ChatGPT-4 especially if you are doing research. By just testing on one particular topic that I'm familiar with, using GPT-4 previously and GPT-4o the quality of the resulting responses for the latter is very promising indeed. In my experience so far, GPT-4o seems to sit somewhere between the capability of GPT-3.5 and GPT-4. I'm working on an app that relies more on GPT-4's reasoning abilities than inference speed. For my use case, GPT-4o seems to do worse than GPT-4 Turbo on reasoning tasks. For me this seems like a step-up from GPT-3.5 but not from GPT-4 Turbo. At half the cost and significantly faster inference speed, I'm sure this is a good tradeoff for other use cases though. I’m a huge user of GPT4 and Opus in my work but I’m a huge user of GPT4-Turbo voice in my personal life. I use it on my commutes to learn all sorts of stuff. I’ve never understood the details of cameras and the relationship between shutter speed and aperture and iso in a modern dslr which given the aurora was important. We talked through and I got to an understanding in a way having read manuals and textbooks didn’t really help before. I’m a much better learner by being able to talk and hear and ask questions and get responses. Extend this to quantum foam, to ergodic processes, to entropic force, to Darius and Xerces, to poets of the 19th century - it’s changed my life. Really glad to see an investment in stream lining this flow. Looking forward to trying this via ChatGPT. As always OpenAI says ""now available"" but refreshing or logging in/out of ChatGPT (web and mobile) don't cause GPT-4o to show up. I don't know why I find this so frustrating. Probably because they don't say ""rolling out"" they say things like ""try it now"" but I can't even though I'm a paying customer. Oh well... Wow this versioning scheme really messed up this prediction market: https://kalshi.com/markets/gpt4p5/gpt45-released The sentence order of the Arabic and Urdu examples text is scrambled on that page: Arabic: مرحبًا، اسمي جي بي تي-4o. أنا نوع جديد من نموذج اللغة، سررت بلقائك! Urdu: ہیلو، میرا نام جی پی ٹی-4o ہے۔ میں ایک نئے قسم کا زبان ماڈل ہوں، آپ سے مل کر اچھا لگا! Even if you don't read Arabic or Urdu script, note that the 4 and o are on opposite sides of a sentence. Despite that, pasting both into Google translate actually fixes the error during translation. OpenAI ought to invest in some proofreaders for multilingual blog posts. The similiarity between this model and the movie 'Her' [0] creeps me out so badly that I can't shake the feeling that our social interactions are on the brink of doom. [0] https://youtu.be/GV01B5kVsC0?feature=shared&t=85 Very impressive demo, but not really a step change in my opinion. The hype from OpenAI employees was on another level, way more than was warranted in my opinion. Ultimately, the promise of LLM proponents is that these models will get exponentially smarter - this hasn’t born out yet. So from that perspective, this was a disappointing release. If anything, this feels like a rushed release to match what Google will be demoing tomorrow. Apple and Google, you need to get your personal agent game going because right now you’re losing the market. This is FREE. Tweakable emotion and voice, watching the scene, cracking jokes. It’s not perfect but the amount and types of data this will collect will be massive. I can see it opening up access to many more users and use cases. Very close to: - A constant friend - A shrink - A teacher - A coach who can watch you exercise and offer feedback …all infinitely patient, positive, helpful. For kids that get bullied, or whose parents can’t afford therapy or a coach, there’s the potential for a base level of support that will only get better over time. @sama reflects: https://blog.samaltman.com/gpt-4o Very impressive. Its programming skills are still kind of crappy and I seriously doubt its reasoning capacity. It feels like it can deep fake text prediction really well, but in essence there's still something wrong it it. As far as I'm concerned this is the new best demo of all time. This is going to change the world in short order. I doubt they will be ready with enough GPUs for the demand the voice+vision mode is going to get, if it's really released to all free users. Now imagine this in a $16k humanoid robot, also announced this morning: https://www.youtube.com/watch?v=GzX1qOIO1bE The future is going to be wild. It is notable OpenAI did not need to carefully rehearse the talking points of the speakers. Or even do the kind of careful production quality seen in a lot of other videos. The technology product is so good and so advanced it doesn't matter how the people appear. Zuck tried this in his video countering to vision pro, but it did not have the authentic ""not really rehearsed or produced"" feel of this at all. If you watch that video and compare it with this you can see the difference. Very interesting times. What struck me was the interruptions to the AI speaking which seemed commonplace by the team members in the demo. We will quickly get used to doing this to AIs and we will probably be talking to AIs a lot throughout the day as time progresses I would imagine. We will be trained by AIs to be rude and impatient I think."
44226145,Tell HN: Help restore the tax deduction for software dev in the US (Section 174),"Companies building software in the US were hit hard a few years ago when the tax code stopped allowing deduction of software dev expenses. Now they have to be amortized over several years. HN has had many discussions about this, including The time bomb in the tax code that's fueling mass tech layoffs - https://news.ycombinator.com/item?id=44180533 - (927 comments) a few days ago. Other threads are listed at https://news.ycombinator.com/item?id=44203869 . There's currently a major effort to get this change reversed. One of the people working on it is YC's Luther Lowe ( https://news.ycombinator.com/user?id=itsluther ). Luther has been organizing YC alumni to urge lawmakers to support this reversal. I asked him if we could do that on Hacker News too. He said yes—hence this thread :) If you're a US taxpayer and if you agree that software dev expenses should be deductible like they used to be, please sign this letter to the relevant committee members: https://docs.google.com/forms/d/1DkRGeef2e_tU2xf3TyEyd2JLlmZ... . (If you're not a US person, please don't sign the letter, since lawmakers will only listen to feedback from taxpayers and we don't want to dilute the signal.) I'm sure not everyone here agrees with us—HN is a big community, there's no total agreement on anything—but this issue has as close to a community consensus as HN gets, so I think it makes sense to add our voices too. Luther will be around to answer questions and hopefully HN can contribute to getting this done!",,dang,2439,2025-06-09T16:28:51+00:00,,query,50,"A lot of people don't know what this Section 174 is about, so here's a brief explainer. Normally, when you have expenses, you deduct them off your revenue to find your taxable profit. If you have $1 million in sales, and $900k in costs, you have $100k in profit, and the government taxes you on that profit. Section 174 says you can't do this for software engineers. If you pay a software engineer, that's not ""really"" an ""expense"", regardless of the fact that you paid them. What you've actually done, Congress said, is bought a capital good, like a machine. And for calculating tax owed, you have to depreciate that over several years (5 in this case). Depreciating means that if you pay an engineer $200k in a year, in tax-world you only had $40k of real expense that year, even though you paid them $200k. So the effect is that it makes engineers much more expensive, because normally when a company hires an engineer, like they spend on any other expense, they can at least think ""well, they will reduce our profit, which reduces our tax obligation,"" but in this case software engineers are special and aren't deductible in the same way. In the case of the $200k engineer, you deduct the first $40k in the first year, then you can expense another $40k from that first year in the second year, the third $40k in the third year, and so on through the fifth year. So eventually you get to expense the entire first year of the engineer's pay, but only after five years. The effect is that companies wind up using their scarce capital to loan the federal government money for five years, and so engineers become a heavy financial burden. If a company hires too many engineers, they will owe the federal government income tax even in years in which they were unprofitable. These rules, by the way, don't apply to other personnel costs. If you hire an HR person or a corporate executive, you expense them in the year you paid them. It's a special rule for software engineers. It was passed by Congress during the first Trump administration in order to offset the costs of other corporate tax rate cuts, due to budgeting rules. I think people are missing the actual process used by Finance teams relating to this issue. I am a former CFO and spent a fair amount of time with this issue in my last role. The firm had a significant amount of software engineering expense related to its core operating system that was the backbone of the company. The FASB accounting rules drive the capitalization of software expenses, not the tax rules. The FASB definition of GAAP (Generally Accepted Accounting Principals) for US firms is very specific and requires significant detailed tracking to comply. As noted in one of the other posts, many companies want to capitalize as much software engineering expense as possible as that leads to higher operating income and net income. Bonuses, option grants and stock prices tend to be tied to those metrics. The argument is that building a piece of software should be treated like purchasing it off the shelf. If a firm pays $1M to implement SAP, it does not have to expense it all in one year, but rather depreciates it over its “expected life.” Since “expected life” is difficult to define for every piece of software, there are default lifetimes (similar to saying motor vehicles default to a 5 year depreciation schedule). Tax then generally follows the GAAP accounting except when the government intervenes to try and increase capital spending. Periodically the government will allow accelerated depreciation which increases operating expenses for tax purposes only which reduces current period cash taxes. Note total taxes do not change, only when they get paid. The Section 174 under discussion here is simply the same idea then applied to software development in an effort to juice hiring. For the people discussing whether the IRS is effectively tracking and enforcing this - the IRS really does not matter. A companies auditors enforce it. Without all of the necessary paperwork/digital audit trail, a firm in not permitted by the auditors to capitalize the expense. The same auditors have to sign off on the tax treatment as well. Finally, with respect to maintenance, the idea is meant to be similar to the treatment for machinery ( i.e. traditional capital expenditures). When a firm puts gas in the company truck or replaces tires or fixes a windshield, they do not capitalize those expenses. The idea is the expense do not fundamentally improve the item or meaningful extend the life beyond the initial expectations. Following that line of thought, maintenance releases are not thought to extend the life of the software while significant improvements to the software do and therefore can be capitalized. DISCLAIMER - while I was a CFO, I was not a Certified Accountant. What I have described above is what the accountants and my audit firms described to me as I worked through this issue in preparing financial statements. The Small Software Business Alliance has been actively working on this issue since day one. https://ssballiance.org/about/engage/ And Michelle Hansen was an early organizer https://x.com/mjwhansen If you work at all in energy, the Clean Energy Business Network is also proactive in fighting for change. A couple of years ago they put me touch with Ron Wyden's staff. The Democrats are almost universally opposed to what was added to Section 174. https://www.cebn.org/media_resources/house-republicans-advan... Fight this thing - it is terrible. Not just for software but any innovative business in the USA. Thank you for helping to tackle this. The silence on this issue for the past few years from smaller software companies and their affiliates was surprising to me. The recent ""time bomb"" article was one of the few media pieces that actually took the time to describe it as anything other than a ""tax cut for huge tech companies"", which was refreshing. My current favorite theory as to why there hasn't been more of an outcry is that many companies ignored the rule change (either out of ignorance or as an alternative to going out of business), and are forced to remain silent. Thanks for working on this guys. The current tax code is fairly crazy: you could spend a few million in salaries, sell 200k of software in a year and possibly owe taxes on that. Even if the company would otherwise be shutting down. The traditional capital asset treatment applied to software leaves a lot to be desired. Some software is a capital asset, but much just isn’t. Or at least should be considered to depreciate rapidly. As a business owner, I've been adversely impacted by this. I still can't wrap my head around how this is legal or sustainable. If I buy $1MM of plant and equipment, I may not be able to expense it all in year 1, but I can relatively easily get a loan to finance the purchase of such--and manage my cashflows. The same is not for devs. I cannot easily get a loan for $1MM in dev salaries. In my own case, I don't need the loan to pay the salaries. I need the loan to pay the taxes for the portion of the salaries I cannot deduct as an expense. It's just insane. I am writing my member of Congress right now. --- I'm writing to express my urgent concern regarding the negative impact of the 2022 Section 174 tax code changes on small businesses like mine. As owner of Rietta, Inc., a small cybersecurity firm, I still do much of the technical work. My wife and I have three young children under 6. My family and I have been directly negatively impacted by these changes from the 2017 act. Previously, the tax code helped us afford open-source and experimental work that benefited customers. For example, modernizing applications to run on Docker improved testing and deployment. Our State government clients now benefit, but this was once experimental. Now we're largely back to just work-for-hire consulting, treated as cost of goods sold. I don't have the cash to pay for experimental software development only to then amortize it over five years. If I have $100k revenue and spend $100k, the current code allows only a $20k deduction. I owe taxes on the other $80k despite no cash or documented asset value. Experimental software doesn't work like that in this field. I started this business 26 years ago. We provide important long-term custom programming and update work for private sector and State government clients (""STATE A"" and ""STATE B"" judicial branches). Often, we work with code we didn't originally write. As a professional computer scientist and business owner, I rely on my CPA for tax compliance. If I've erred in my example, that's on me. But I can tell you this amortization requirement particularly cripples small businesses like Rietta, Inc., where cash flow is critical, severely limiting the quality of services I can afford to provide. I support undoing this tax change. For folks that don't know the background on this, here's a layperson summary: - A business is usually taxed on its profits: you deduct your revenue from the cost of producing that revenue, and the delta is what you are taxed on. - In software businesses, this usually means if you spend $1M in software development to develop a web app, and it makes $1.1M in that year, you'd get taxed on the $100K profits. - However, a few years ago, the IRS stopped allowing the $1M to be deducted in the year it was incurred. Instead, the $1M was to be amortized over 5 years, so now the business can only count $200K as the deductible expense for that year. So now it's going to be taxed on ""profits"" of $900K. Assuming the tax rate is 20%, that means the business owes $180K in taxes, even though it has a total of $100K in the bank after the actual expenses were paid. So it would have to either borrow to pay taxes or raise venture capital, meaning that VC-funded companies would be advantaged over bootstrapped ones! - The letter's goal is to bring things back to how they were (and how they are for all other businesses): let businesses deduct their actual expenses from their actual revenue, and tax that actual profit. I am neither a lawyer nor an accountant, this is just my understanding of this issue. Edit: Switched the tax rate to 20%. The logic is still the same. This US tax code change directly impacted my small business in a very real way that was directly felt by my household. In the past, it was a big boon for us and helped me afford to pay for some open source work and experimental things that helped our customers in the long run. Now we are back to mostly doing work-for-hire consulting. Even the experimental work I am doing, I am just paying for it and writing it off as typical business expenses. I cannot afford to take the credit because that means no deduction for this year. I don't have the cash in this small business context. What inspired working to reverse this now? I'm all for it, just curious as the law has existed for 8 years and been in effect for 3. Seemingly little interest from anyone in the tech world to put lobbying behind reversing it until this point. What changed? Signed and called my representative and senators. I ask simply ""If I have $1m of revenue and $1m of expenses that is entirely software dev salaries, what do you think my profit is for that year? How much should I be taxed on that?"" https://www.house.gov/representatives/find-your-representati... https://www.senate.gov/senators/senators-contact.htm @dang (and others). If you want a groundswell of support have you consider have you considered reaching out to game devs and indie game devs? It seems like they'd all be negatively affected. They'd spread the word to players. I seem to remember people being broadly in favor of this change at the time it was first proposed because it would elevade software development and create more long-term stability, but in a world where the primary focus is on quarterly funding rounded and acquisitions it obviously skews the numbers and thus the potential founded/early investor upside. There are two possible motivations for the impending change. One is the argument that deducting 100% of developer labor isn't ideal because developers create IP whose value can compound as an asset, rather than the labor being 'consumed' in production as with manufacturing (where any long-term benefit after the initial sale goes to the consumer). The other is that it's a legislative stick designed to herd a powerful investor/donor lobby into supporting budget legislation in exchange for turning the favorable tax treatment faucet back on. Signing a letter is fine, but will not have the same impact as phone calls made to your representatives. https://5calls.org/why-calling-works/ You don't need to use that site - the point is that if you want to have the loudest voice, make some calls. Signed. As a US-based developer, I fully support restoring the deductibility of software development expenses. This policy change quietly gutted countless startups and engineering teams—it’s long past time we fix it. Appreciate YC and folks like @itsluther pushing this forward. This isn’t just a tax issue—it’s about keeping innovation and talent thriving in the US. Let’s get it done. Luther et al, would you be willing to share some high level statistics about the submissions, such as how many signatures it gets? Possibly dumb question... For a medium/large company, is this really a big deal? Their payroll is relatively stable year-over-year, so after a few years, it all evens out (very roughly). Or am I missing something? IE, is this really an anti-competition law, designed to protect entrenched tech industry players and prevent up-starts from, well, starting? Forgive me if this has already been answered in one of the other threads (I haven't been following them), but: How does this work in other countries? Discussion from beginning of 2023: https://news.ycombinator.com/item?id=34627712 Japan has required amortization of capitalized software over five years for qualifying internal-use software since at least 2000. Correct me if I’m wrong, but I believe most other countries have similar rules. Until 2022, U.S. companies had a real competitive advantage. Software developer salaries in Japan are depressed—other roles too, but especially engineers. Without digging too deep, perhaps the previously unfavorable (now roughly equal) tax treatment of was perhaps a contributing factor. As I understood it, it makes a difference between R&D on one hand, and ""maintenance"" on the other hand. This has resulted in that my US corporate overlords are shifting maintenance work to the US (better for taxes) and doing greenfield development where I am in the EU. This must be excellent for morale in the US office, but I'm not complaining. Is HN/YC going to submit the google form submissions to the relevant committee members on the signers' behalf? I don't favor any tax breaks for big tech until they actually start paying meaningful taxes. There have been far too many giveaways. The country is running a massive deficit, and the current ""solution"" is to balloon the deficit and throw the poor under a bus. The SSBA (Small Software Business Alliance) was set up by Michele Hansen -- co-founder of Geocodio, http://geocod.io (and the SSBA is now run by another person) for this reason -- to raise awareness in Washington DC of the issue with the Section 174 Capitalization changes and the efforts to repeal it. https://ssballiance.org/ She has also spoken about it on podcasts: https://www.youtube.com/watch?app=desktop&v=oF-xsDd1A4o This will stay because it’s a barrier to smaller companies and nothing significant for larger companies that make political donations I wonder if something like this could also help the hardware industry, thus encouraging more manufacturing in the US. What's particularly wild about the choice to tax software development in this way is that it assumes that code is always asset. For companies that are pre product market fit, it's often a liability! By forcing software wages to be amortized over 5 years (15 for foreign devs), Section 174 has sapped cash flow, prompting layoffs and project cancellations totaling $3–4 M for some firms. Reinstating immediate expensing could unlock roughly $240 B in stuck deductions and supercharge R&D credits, materially bolstering hiring and keeping IP onshore. Has anyone modeled the macroeconomic gains of full expensing versus the budgetary trade-offs in the current $4.5 T tax proposal? Salaries aren't a one-time expense, so is the amortization rolling? Like, year 1, you pay me $200k and deduct $40k. In year 2, you pay me another $200k, do you get to deduct $40k for year 1's salary and $40k for year 2's salary? I guess another way to ask is, does this mean that if you keep someone for 5 years and don't change my wages, is their yearly salary effectively fully deductible? If so, does that create incentives to try to keep employees longer-term in order to make them more cost-efficient? Doesnt this new law inhibit rapid turnover tho? Since it takes 5 years to get the full deduction of an employee’s salary, there is an incentive to keep the employee around. OTOH, the souless bean counters who want quarterly (if not shorter) time horizons, will simply decrease starting salaries and use other methods to be net zero. If they do shaft the software engineers, then the converse is true-no employee should stay at a job more than a year because only the corp will benefit as the deduction grows I am not sure I understand how amortizing these expenses benefits the government at all, as it is. I won't speak to the methods the government is using to value software, because others have made those points better than I could. First, I think the impact on large businesses has diminished greatly since 2022 anyway, so restoring the tax deduction would essentially give those businesses a 1-year ""bump"" in their deductions (since they'd be able to expense the previous 4 years of left-over deductions all at once, plus the current year in full). As far as I can tell, the expense isn't tied to individual workers, just the combined salary expense. So hiring/firing shouldn't be largely impacted. And, any benefit the government would have gotten from large corporations has (again, since 2022) now expired. For small businesses and start-ups, there is of course a much greater impact. And, ironically, I think the government is actually collecting less from small businesses in the long term, because the businesses that needed the full deduction to survive can't be collected from, having gone out of business. So the government isn't collecting any more taxes today than it used to. It is probably collecting less, depending on how much revenue has shifted from small (and now failing) businesses to large ones. And we're basically encouraging all of those more entrepreneurial technologists among us to go work for larger corporations instead of striking out on their own. I guess my question then, is, who does this tax code even benefit? Edit: looks like it was just a victim of the TCJA, meant to make TCJA look less expensive. I don't think it had its intended effect. > but this issue has as close to a community consensus as HN gets Curious how this is assessed of you could share? the real question is why is r&d for startups in general amortized in the first place? doesn't this discourage startups pursuing risky hard science ventures? If the software written in a one year period of time for $100k is an asset then I, as a small business owner, can go to the local credit union and take out a loan on favorable terms with the that asset as collateral. No, of course not! They would laugh me out of the branch or the loan would be credit card interest rates. Software is demonstrably NOT AN ASSET like a major piece of equipment. Signed. That said, here's my perspective on 174 (which should be reverted to full deduction on the year the expense is incurred). You do not have to amortize 100% of your engineering costs. Not even close. Here's the key: Development costs incurred to remove uncertainty are amortized. All other costs are deductible during the tax year where they are incurred. How does this work? You are going to design a new robot arm. In January, you spend $100K to ""remove uncertainty"". In rough strokes, this means discovering all the things you don't know and need to know for this robot arm to become a product. This amount will be amortized over five years under 174. Now, with uncertainty removed, you spend an additional $1.1MM from January until December for engineering implementation. No uncertainty being removed. Just building a product. This is 100% deductible that tax year. Analogy: You want to build a new brick wall with specific properties. You spend $100K to develop a new type of brick and $1.1MM to build the wall using that brick. The $100K is amortized, the $1.1MM is deductible in one shot. BTW, at year 6 the amortization schedule reaches steady-state and you are amortizing the full $100K every year. In other words, the impact of 174, if treated intelligently, is the time value of money until steady state is reached for the engineering costs incurred to remove uncertainty. https://www.law.cornell.edu/cfr/text/26/1.174-2 Doesn't this incentivize outsourcing or fractional work to some degree, because that would still be counted as regular expenses? I would not use section 174 as a reason not to startup but rather as a way to ensure you are running a lean ship. It’s very possible the rule change could be retroactive. Thats just my poker take on this bluff. It may not be retrospective or it may not happen at all. But indecision will kill some startups, the ones who don’t will be a year or three ahead. I think the most impactful thing you can say about Section 174 is that if it continues, I will be starting my startups outside the US. An employee should be taxed like an employee, not a panel truck. There is no guarantee the software developer will produce anything of great value, so this is a tax on unrealized gains. I've heard that many of the big tech layoffs where actually just moved / converting them to contracting groups, so they lose the direct head count but kept the developer via the intermediary. Have others heard this too and could this have been a way to label contractors differently so they don't fall under this tax code? The tech community, correctly or incorrectly, is broadly seen as ""anti-tax cuts"", so - regardless of the actual merits of this particular tax cut - I'm not sure how well-received this campaign will be. I'd brace for some rather heavy sarcasm on social media for anyone brave enough to tread those waters. Wish someone in EU do similar signing / votes for lobbying EU for taxing US Tech companies or applying 15 years amortization for all US products as a revenge - sorry US but 15 years amortization for everyone outside US is just worldwide tariff for any other software producers, freelancers, etc. My understanding is that the current ""Big Beautiful Bill"" reverses this Note that this letter's requests DO NOT include voting against the reconciliation bill, just modifying it to add a carve-out to fix Section 174. While I agree that Section 174 desperately needs reform and is harmful to the tech industry, the bill as a whole must be opposed, not tweaked. There are many, many things wrong with the ""Big Beautiful Bill"", too many to fix through piecemeal efforts like this. It must be resolutely opposed, not endorsed with minor changes. >We therefore urge Congress to include a retroactive carve-out from Section 174’s amortization requirements What if I'm in favor of restoring the previous deduction behavior, but not of doing it retroactively? This is where it's really important to use a bug tracker that can distinguish between bugs/maintenance and feature development. The former can be deducted but the latter has to be amortized. Does Section 175 apply to other professions? For example, if I hire a full-time handyman for my office, does their salary count as a deductible cost? Sorry if this sounds naive—I'm genuinely struggling to understand why the labor of software engineers would be treated differently from other kinds of work. It seems logical that either all labor costs should count as costs, or none should. If different types of jobs are treated differently, what's the reasoning behind that? As an international founder, I'd like the section 174 to be fully restored as it was before – not just for domestic R&D but offshore one as well, so we're not hit with 15 years deprecitation (it is as good as ""infinity"") I also own section174.com and sec174.com Would these help with visibility? I built a lightweight grassroots advocacy tool for this. It figures out who your reps are and sends them a pre-filled note based on what matters to you (you can edit/customize it before it sends). Includes a call script too if you're up for calling... https://secure.legisletter.org/campaign/cmbpf5js80000l70d5fn... Happy to support this, desperately needs to be changed. Does not restoring the tax deduction for software dev in the US help solo founders who don't draw salary to compete with large businesses?","Tell HN: Help restore the tax deduction for software dev in the US (Section 174) Companies building software in the US were hit hard a few years ago when the tax code stopped allowing deduction of software dev expenses. Now they have to be amortized over several years. HN has had many discussions about this, including The time bomb in the tax code that's fueling mass tech layoffs - https://news.ycombinator.com/item?id=44180533 - (927 comments) a few days ago. Other threads are listed at https://news.ycombinator.com/item?id=44203869 . There's currently a major effort to get this change reversed. One of the people working on it is YC's Luther Lowe ( https://news.ycombinator.com/user?id=itsluther ). Luther has been organizing YC alumni to urge lawmakers to support this reversal. I asked him if we could do that on Hacker News too. He said yes—hence this thread :) If you're a US taxpayer and if you agree that software dev expenses should be deductible like they used to be, please sign this letter to the relevant committee members: https://docs.google.com/forms/d/1DkRGeef2e_tU2xf3TyEyd2JLlmZ... . (If you're not a US person, please don't sign the letter, since lawmakers will only listen to feedback from taxpayers and we don't want to dilute the signal.) I'm sure not everyone here agrees with us—HN is a big community, there's no total agreement on anything—but this issue has as close to a community consensus as HN gets, so I think it makes sense to add our voices too. Luther will be around to answer questions and hopefully HN can contribute to getting this done! A lot of people don't know what this Section 174 is about, so here's a brief explainer. Normally, when you have expenses, you deduct them off your revenue to find your taxable profit. If you have $1 million in sales, and $900k in costs, you have $100k in profit, and the government taxes you on that profit. Section 174 says you can't do this for software engineers. If you pay a software engineer, that's not ""really"" an ""expense"", regardless of the fact that you paid them. What you've actually done, Congress said, is bought a capital good, like a machine. And for calculating tax owed, you have to depreciate that over several years (5 in this case). Depreciating means that if you pay an engineer $200k in a year, in tax-world you only had $40k of real expense that year, even though you paid them $200k. So the effect is that it makes engineers much more expensive, because normally when a company hires an engineer, like they spend on any other expense, they can at least think ""well, they will reduce our profit, which reduces our tax obligation,"" but in this case software engineers are special and aren't deductible in the same way. In the case of the $200k engineer, you deduct the first $40k in the first year, then you can expense another $40k from that first year in the second year, the third $40k in the third year, and so on through the fifth year. So eventually you get to expense the entire first year of the engineer's pay, but only after five years. The effect is that companies wind up using their scarce capital to loan the federal government money for five years, and so engineers become a heavy financial burden. If a company hires too many engineers, they will owe the federal government income tax even in years in which they were unprofitable. These rules, by the way, don't apply to other personnel costs. If you hire an HR person or a corporate executive, you expense them in the year you paid them. It's a special rule for software engineers. It was passed by Congress during the first Trump administration in order to offset the costs of other corporate tax rate cuts, due to budgeting rules. I think people are missing the actual process used by Finance teams relating to this issue. I am a former CFO and spent a fair amount of time with this issue in my last role. The firm had a significant amount of software engineering expense related to its core operating system that was the backbone of the company. The FASB accounting rules drive the capitalization of software expenses, not the tax rules. The FASB definition of GAAP (Generally Accepted Accounting Principals) for US firms is very specific and requires significant detailed tracking to comply. As noted in one of the other posts, many companies want to capitalize as much software engineering expense as possible as that leads to higher operating income and net income. Bonuses, option grants and stock prices tend to be tied to those metrics. The argument is that building a piece of software should be treated like purchasing it off the shelf. If a firm pays $1M to implement SAP, it does not have to expense it all in one year, but rather depreciates it over its “expected life.” Since “expected life” is difficult to define for every piece of software, there are default lifetimes (similar to saying motor vehicles default to a 5 year depreciation schedule). Tax then generally follows the GAAP accounting except when the government intervenes to try and increase capital spending. Periodically the government will allow accelerated depreciation which increases operating expenses for tax purposes only which reduces current period cash taxes. Note total taxes do not change, only when they get paid. The Section 174 under discussion here is simply the same idea then applied to software development in an effort to juice hiring. For the people discussing whether the IRS is effectively tracking and enforcing this - the IRS really does not matter. A companies auditors enforce it. Without all of the necessary paperwork/digital audit trail, a firm in not permitted by the auditors to capitalize the expense. The same auditors have to sign off on the tax treatment as well. Finally, with respect to maintenance, the idea is meant to be similar to the treatment for machinery ( i.e. traditional capital expenditures). When a firm puts gas in the company truck or replaces tires or fixes a windshield, they do not capitalize those expenses. The idea is the expense do not fundamentally improve the item or meaningful extend the life beyond the initial expectations. Following that line of thought, maintenance releases are not thought to extend the life of the software while significant improvements to the software do and therefore can be capitalized. DISCLAIMER - while I was a CFO, I was not a Certified Accountant. What I have described above is what the accountants and my audit firms described to me as I worked through this issue in preparing financial statements. The Small Software Business Alliance has been actively working on this issue since day one. https://ssballiance.org/about/engage/ And Michelle Hansen was an early organizer https://x.com/mjwhansen If you work at all in energy, the Clean Energy Business Network is also proactive in fighting for change. A couple of years ago they put me touch with Ron Wyden's staff. The Democrats are almost universally opposed to what was added to Section 174. https://www.cebn.org/media_resources/house-republicans-advan... Fight this thing - it is terrible. Not just for software but any innovative business in the USA. Thank you for helping to tackle this. The silence on this issue for the past few years from smaller software companies and their affiliates was surprising to me. The recent ""time bomb"" article was one of the few media pieces that actually took the time to describe it as anything other than a ""tax cut for huge tech companies"", which was refreshing. My current favorite theory as to why there hasn't been more of an outcry is that many companies ignored the rule change (either out of ignorance or as an alternative to going out of business), and are forced to remain silent. Thanks for working on this guys. The current tax code is fairly crazy: you could spend a few million in salaries, sell 200k of software in a year and possibly owe taxes on that. Even if the company would otherwise be shutting down. The traditional capital asset treatment applied to software leaves a lot to be desired. Some software is a capital asset, but much just isn’t. Or at least should be considered to depreciate rapidly. As a business owner, I've been adversely impacted by this. I still can't wrap my head around how this is legal or sustainable. If I buy $1MM of plant and equipment, I may not be able to expense it all in year 1, but I can relatively easily get a loan to finance the purchase of such--and manage my cashflows. The same is not for devs. I cannot easily get a loan for $1MM in dev salaries. In my own case, I don't need the loan to pay the salaries. I need the loan to pay the taxes for the portion of the salaries I cannot deduct as an expense. It's just insane. I am writing my member of Congress right now. --- I'm writing to express my urgent concern regarding the negative impact of the 2022 Section 174 tax code changes on small businesses like mine. As owner of Rietta, Inc., a small cybersecurity firm, I still do much of the technical work. My wife and I have three young children under 6. My family and I have been directly negatively impacted by these changes from the 2017 act. Previously, the tax code helped us afford open-source and experimental work that benefited customers. For example, modernizing applications to run on Docker improved testing and deployment. Our State government clients now benefit, but this was once experimental. Now we're largely back to just work-for-hire consulting, treated as cost of goods sold. I don't have the cash to pay for experimental software development only to then amortize it over five years. If I have $100k revenue and spend $100k, the current code allows only a $20k deduction. I owe taxes on the other $80k despite no cash or documented asset value. Experimental software doesn't work like that in this field. I started this business 26 years ago. We provide important long-term custom programming and update work for private sector and State government clients (""STATE A"" and ""STATE B"" judicial branches). Often, we work with code we didn't originally write. As a professional computer scientist and business owner, I rely on my CPA for tax compliance. If I've erred in my example, that's on me. But I can tell you this amortization requirement particularly cripples small businesses like Rietta, Inc., where cash flow is critical, severely limiting the quality of services I can afford to provide. I support undoing this tax change. For folks that don't know the background on this, here's a layperson summary: - A business is usually taxed on its profits: you deduct your revenue from the cost of producing that revenue, and the delta is what you are taxed on. - In software businesses, this usually means if you spend $1M in software development to develop a web app, and it makes $1.1M in that year, you'd get taxed on the $100K profits. - However, a few years ago, the IRS stopped allowing the $1M to be deducted in the year it was incurred. Instead, the $1M was to be amortized over 5 years, so now the business can only count $200K as the deductible expense for that year. So now it's going to be taxed on ""profits"" of $900K. Assuming the tax rate is 20%, that means the business owes $180K in taxes, even though it has a total of $100K in the bank after the actual expenses were paid. So it would have to either borrow to pay taxes or raise venture capital, meaning that VC-funded companies would be advantaged over bootstrapped ones! - The letter's goal is to bring things back to how they were (and how they are for all other businesses): let businesses deduct their actual expenses from their actual revenue, and tax that actual profit. I am neither a lawyer nor an accountant, this is just my understanding of this issue. Edit: Switched the tax rate to 20%. The logic is still the same. This US tax code change directly impacted my small business in a very real way that was directly felt by my household. In the past, it was a big boon for us and helped me afford to pay for some open source work and experimental things that helped our customers in the long run. Now we are back to mostly doing work-for-hire consulting. Even the experimental work I am doing, I am just paying for it and writing it off as typical business expenses. I cannot afford to take the credit because that means no deduction for this year. I don't have the cash in this small business context. What inspired working to reverse this now? I'm all for it, just curious as the law has existed for 8 years and been in effect for 3. Seemingly little interest from anyone in the tech world to put lobbying behind reversing it until this point. What changed? Signed and called my representative and senators. I ask simply ""If I have $1m of revenue and $1m of expenses that is entirely software dev salaries, what do you think my profit is for that year? How much should I be taxed on that?"" https://www.house.gov/representatives/find-your-representati... https://www.senate.gov/senators/senators-contact.htm @dang (and others). If you want a groundswell of support have you consider have you considered reaching out to game devs and indie game devs? It seems like they'd all be negatively affected. They'd spread the word to players. I seem to remember people being broadly in favor of this change at the time it was first proposed because it would elevade software development and create more long-term stability, but in a world where the primary focus is on quarterly funding rounded and acquisitions it obviously skews the numbers and thus the potential founded/early investor upside. There are two possible motivations for the impending change. One is the argument that deducting 100% of developer labor isn't ideal because developers create IP whose value can compound as an asset, rather than the labor being 'consumed' in production as with manufacturing (where any long-term benefit after the initial sale goes to the consumer). The other is that it's a legislative stick designed to herd a powerful investor/donor lobby into supporting budget legislation in exchange for turning the favorable tax treatment faucet back on. Signing a letter is fine, but will not have the same impact as phone calls made to your representatives. https://5calls.org/why-calling-works/ You don't need to use that site - the point is that if you want to have the loudest voice, make some calls. Signed. As a US-based developer, I fully support restoring the deductibility of software development expenses. This policy change quietly gutted countless startups and engineering teams—it’s long past time we fix it. Appreciate YC and folks like @itsluther pushing this forward. This isn’t just a tax issue—it’s about keeping innovation and talent thriving in the US. Let’s get it done. Luther et al, would you be willing to share some high level statistics about the submissions, such as how many signatures it gets? Possibly dumb question... For a medium/large company, is this really a big deal? Their payroll is relatively stable year-over-year, so after a few years, it all evens out (very roughly). Or am I missing something? IE, is this really an anti-competition law, designed to protect entrenched tech industry players and prevent up-starts from, well, starting? Forgive me if this has already been answered in one of the other threads (I haven't been following them), but: How does this work in other countries? Discussion from beginning of 2023: https://news.ycombinator.com/item?id=34627712 Japan has required amortization of capitalized software over five years for qualifying internal-use software since at least 2000. Correct me if I’m wrong, but I believe most other countries have similar rules. Until 2022, U.S. companies had a real competitive advantage. Software developer salaries in Japan are depressed—other roles too, but especially engineers. Without digging too deep, perhaps the previously unfavorable (now roughly equal) tax treatment of was perhaps a contributing factor. As I understood it, it makes a difference between R&D on one hand, and ""maintenance"" on the other hand. This has resulted in that my US corporate overlords are shifting maintenance work to the US (better for taxes) and doing greenfield development where I am in the EU. This must be excellent for morale in the US office, but I'm not complaining. Is HN/YC going to submit the google form submissions to the relevant committee members on the signers' behalf? I don't favor any tax breaks for big tech until they actually start paying meaningful taxes. There have been far too many giveaways. The country is running a massive deficit, and the current ""solution"" is to balloon the deficit and throw the poor under a bus. The SSBA (Small Software Business Alliance) was set up by Michele Hansen -- co-founder of Geocodio, http://geocod.io (and the SSBA is now run by another person) for this reason -- to raise awareness in Washington DC of the issue with the Section 174 Capitalization changes and the efforts to repeal it. https://ssballiance.org/ She has also spoken about it on podcasts: https://www.youtube.com/watch?app=desktop&v=oF-xsDd1A4o This will stay because it’s a barrier to smaller companies and nothing significant for larger companies that make political donations I wonder if something like this could also help the hardware industry, thus encouraging more manufacturing in the US. What's particularly wild about the choice to tax software development in this way is that it assumes that code is always asset. For companies that are pre product market fit, it's often a liability! By forcing software wages to be amortized over 5 years (15 for foreign devs), Section 174 has sapped cash flow, prompting layoffs and project cancellations totaling $3–4 M for some firms. Reinstating immediate expensing could unlock roughly $240 B in stuck deductions and supercharge R&D credits, materially bolstering hiring and keeping IP onshore. Has anyone modeled the macroeconomic gains of full expensing versus the budgetary trade-offs in the current $4.5 T tax proposal? Salaries aren't a one-time expense, so is the amortization rolling? Like, year 1, you pay me $200k and deduct $40k. In year 2, you pay me another $200k, do you get to deduct $40k for year 1's salary and $40k for year 2's salary? I guess another way to ask is, does this mean that if you keep someone for 5 years and don't change my wages, is their yearly salary effectively fully deductible? If so, does that create incentives to try to keep employees longer-term in order to make them more cost-efficient? Doesnt this new law inhibit rapid turnover tho? Since it takes 5 years to get the full deduction of an employee’s salary, there is an incentive to keep the employee around. OTOH, the souless bean counters who want quarterly (if not shorter) time horizons, will simply decrease starting salaries and use other methods to be net zero. If they do shaft the software engineers, then the converse is true-no employee should stay at a job more than a year because only the corp will benefit as the deduction grows I am not sure I understand how amortizing these expenses benefits the government at all, as it is. I won't speak to the methods the government is using to value software, because others have made those points better than I could. First, I think the impact on large businesses has diminished greatly since 2022 anyway, so restoring the tax deduction would essentially give those businesses a 1-year ""bump"" in their deductions (since they'd be able to expense the previous 4 years of left-over deductions all at once, plus the current year in full). As far as I can tell, the expense isn't tied to individual workers, just the combined salary expense. So hiring/firing shouldn't be largely impacted. And, any benefit the government would have gotten from large corporations has (again, since 2022) now expired. For small businesses and start-ups, there is of course a much greater impact. And, ironically, I think the government is actually collecting less from small businesses in the long term, because the businesses that needed the full deduction to survive can't be collected from, having gone out of business. So the government isn't collecting any more taxes today than it used to. It is probably collecting less, depending on how much revenue has shifted from small (and now failing) businesses to large ones. And we're basically encouraging all of those more entrepreneurial technologists among us to go work for larger corporations instead of striking out on their own. I guess my question then, is, who does this tax code even benefit? Edit: looks like it was just a victim of the TCJA, meant to make TCJA look less expensive. I don't think it had its intended effect. > but this issue has as close to a community consensus as HN gets Curious how this is assessed of you could share? the real question is why is r&d for startups in general amortized in the first place? doesn't this discourage startups pursuing risky hard science ventures? If the software written in a one year period of time for $100k is an asset then I, as a small business owner, can go to the local credit union and take out a loan on favorable terms with the that asset as collateral. No, of course not! They would laugh me out of the branch or the loan would be credit card interest rates. Software is demonstrably NOT AN ASSET like a major piece of equipment. Signed. That said, here's my perspective on 174 (which should be reverted to full deduction on the year the expense is incurred). You do not have to amortize 100% of your engineering costs. Not even close. Here's the key: Development costs incurred to remove uncertainty are amortized. All other costs are deductible during the tax year where they are incurred. How does this work? You are going to design a new robot arm. In January, you spend $100K to ""remove uncertainty"". In rough strokes, this means discovering all the things you don't know and need to know for this robot arm to become a product. This amount will be amortized over five years under 174. Now, with uncertainty removed, you spend an additional $1.1MM from January until December for engineering implementation. No uncertainty being removed. Just building a product. This is 100% deductible that tax year. Analogy: You want to build a new brick wall with specific properties. You spend $100K to develop a new type of brick and $1.1MM to build the wall using that brick. The $100K is amortized, the $1.1MM is deductible in one shot. BTW, at year 6 the amortization schedule reaches steady-state and you are amortizing the full $100K every year. In other words, the impact of 174, if treated intelligently, is the time value of money until steady state is reached for the engineering costs incurred to remove uncertainty. https://www.law.cornell.edu/cfr/text/26/1.174-2 Doesn't this incentivize outsourcing or fractional work to some degree, because that would still be counted as regular expenses? I would not use section 174 as a reason not to startup but rather as a way to ensure you are running a lean ship. It’s very possible the rule change could be retroactive. Thats just my poker take on this bluff. It may not be retrospective or it may not happen at all. But indecision will kill some startups, the ones who don’t will be a year or three ahead. I think the most impactful thing you can say about Section 174 is that if it continues, I will be starting my startups outside the US. An employee should be taxed like an employee, not a panel truck. There is no guarantee the software developer will produce anything of great value, so this is a tax on unrealized gains. I've heard that many of the big tech layoffs where actually just moved / converting them to contracting groups, so they lose the direct head count but kept the developer via the intermediary. Have others heard this too and could this have been a way to label contractors differently so they don't fall under this tax code? The tech community, correctly or incorrectly, is broadly seen as ""anti-tax cuts"", so - regardless of the actual merits of this particular tax cut - I'm not sure how well-received this campaign will be. I'd brace for some rather heavy sarcasm on social media for anyone brave enough to tread those waters. Wish someone in EU do similar signing / votes for lobbying EU for taxing US Tech companies or applying 15 years amortization for all US products as a revenge - sorry US but 15 years amortization for everyone outside US is just worldwide tariff for any other software producers, freelancers, etc. My understanding is that the current ""Big Beautiful Bill"" reverses this Note that this letter's requests DO NOT include voting against the reconciliation bill, just modifying it to add a carve-out to fix Section 174. While I agree that Section 174 desperately needs reform and is harmful to the tech industry, the bill as a whole must be opposed, not tweaked. There are many, many things wrong with the ""Big Beautiful Bill"", too many to fix through piecemeal efforts like this. It must be resolutely opposed, not endorsed with minor changes. >We therefore urge Congress to include a retroactive carve-out from Section 174’s amortization requirements What if I'm in favor of restoring the previous deduction behavior, but not of doing it retroactively? This is where it's really important to use a bug tracker that can distinguish between bugs/maintenance and feature development. The former can be deducted but the latter has to be amortized. Does Section 175 apply to other professions? For example, if I hire a full-time handyman for my office, does their salary count as a deductible cost? Sorry if this sounds naive—I'm genuinely struggling to understand why the labor of software engineers would be treated differently from other kinds of work. It seems logical that either all labor costs should count as costs, or none should. If different types of jobs are treated differently, what's the reasoning behind that? As an international founder, I'd like the section 174 to be fully restored as it was before – not just for domestic R&D but offshore one as well, so we're not hit with 15 years deprecitation (it is as good as ""infinity"") I also own section174.com and sec174.com Would these help with visibility? I built a lightweight grassroots advocacy tool for this. It figures out who your reps are and sends them a pre-filled note based on what matters to you (you can edit/customize it before it sends). Includes a call script too if you're up for calling... https://secure.legisletter.org/campaign/cmbpf5js80000l70d5fn... Happy to support this, desperately needs to be changed. Does not restoring the tax deduction for software dev in the US help solo founders who don't draw salary to compete with large businesses?"
41046773,Open source AI is the path forward,nan,https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/,atgctg,2360,2024-07-23T15:08:41+00:00,,query,50,"Related ongoing thread: Llama 3.1 - https://news.ycombinator.com/item?id=41046540 - July 2024 (114 comments) “The Heavy Press Program was a Cold War-era program of the United States Air Force to build the largest forging presses and extrusion presses in the world.” This ”program began in 1944 and concluded in 1957 after construction of four forging presses and six extruders, at an overall cost of $279 million. Six of them are still in operation today, manufacturing structural parts for military and commercial aircraft” [1]. $279mm in 1957 dollars is about $3.2bn today [2]. A public cluster of GPUs provided for free to American universities, companies and non-profits might not be a bad idea. [1] https://en.m.wikipedia.org/wiki/Heavy_Press_Program [2] https://data.bls.gov/cgi-bin/cpicalc.pl?cost1=279&year1=1957... ""Eventually though, open source Linux gained popularity – initially because it allowed developers to modify its code however they wanted ..."" I find the language around ""open source AI"" to be confusing. With ""open source"" there's usually ""source"" to open, right? As in, there is human legible code that can be read and modified by the user? If so, then how can current ML models be open source? They're very large matrices that are, for the most part, inscrutable to the user. They seem akin to binaries, which, yes, can be modified by the user, but are extremely obscured to the user, and require enormous effort to understand and effectively modify. ""Open source"" code is not just code that isn't executed remotely over an API, and it seems like maybe its being conflated with that here? The big winners of this: devs and AI startups - No more vendor lock-in - Instead of just wrapping proprietary API endpoints, developers can now integrate AI deeply into their products in a very cost-effective and performant way - Price race to the bottom with near-instant LLM responses at very low prices are on the horizon As a founder, it feels like a very exciting time to build a startup as your product automatically becomes better, cheaper, and more scalable with every major AI advancement. This leads to a powerful flywheel effect: https://www.kadoa.com/blog/ai-flywheel Even if it's just open weights and not ""true"" open source, I'll still give Meta the appreciation of being one of the few big AI companies actually committed to open models. In an ecosystem where groups like Anthropic and OpenAI keep hemming and hawing about safety and the necessity of closed AI systems ""for our sake"", they stand out among the rest. They are positioning themselves as champions of AI open source mostly because they were blindsided by OpenAI, are not in the infra game, and want to commoditize their complements as much as possible. This is not altruism although it's still great for devs and startups. All FB GPU investments is primarily for new AI products ""friends"", recommendations and selling ads. https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/ I wish Meta stopped using the ""open source"" misnomer for free of charge weights. In the US the FTC already uses the term Open-Weights , and it seems the industry is also adopting this term (e.g. Mistral). Someone can correct me here but AFAIK we don't even know which datasets are used to train these models, so why should we even use ""open"" to describe Llama? This is more similar to a freeware than an open-source project. [1] https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/202... Meta makes their money off advertising, which means they profit from attention. This means they need content that will grab attention, and creating open source models that allow anyone to create any content on their own becomes good for Meta. The users of the models can post it to their Instagram/FB/Threads account. Releasing an open model also releases Meta from the burden of having to police the content the model generates, once the open source community fine-tunes the models. Overall, this move is good business move for Meta - the post doesn't really talk about the true benefit, instead moralizing about open source, but this is a sound business move for Meta. Huge companies like facebook will often argue for solutions that on the surface, seem to be in the public interest. But I have strong doubts they (or any other company) actually believe what they are saying. Here is the reality: - Facebook is spending untold billions on GPU hardware. - Facebook is arguing in favor of open sourcing the models, that they spent billions of dollars to generate, for free...? It follows that companies with much smaller resources (money) will not be able to match what Facebook is doing. Seems like an attempt to kill off the competition (specifically, smaller organizations) before they can take root. > We’re releasing Llama 3.1 405B, the first frontier-level open source AI model, as well as new and improved Llama 3.1 70B and 8B models. Bravo! While I don't agree with Zuck's views and actions on many fronts, on this occasion I think he and the AI folks at Meta deserve our praise and gratitude. With this release, they have brought the cost of pretraining a frontier 400B+ parameter model to ZERO for pretty much everyone -- well, everyone except Meta's key competitors.[a] THANK YOU ZUCK. Meanwhile, the business-minded people at Meta surely won't mind if the release of these frontier models to the public happens to completely mess up the AI plans of competitors like OpenAI/Microsoft, Google, Anthropic, etc. Come to think of it, the negative impact on such competitors was likely a key motivation for releasing the new models. --- [a] The license is not open to the handful of companies worldwide which have more than 700M users. I've summarized this entire thread in 4 lines (didn't even use AI for it!) Step 1. Chick-Fil-A releases a grass-fed beef burger to spite other fast-food joints, calls it ""the vegan burger"" Step 2. A couple of outraged vegans show up in the comments, pointing out that beef, even grass-fed beef, isn't vegan Step 3. Fast food enthusiasts push back: it's unreasonable to want companies to abide by this restrictive definition of ""vegan"". Clearly this burger is a gamechanger and the definition needs to adapt to the times. Step 4. Goto Step 2 in an infinite loop Software 2.0 is about open licensing. I.e., the more important thing - the more ""free"" thing - is the licensing now. E.g., I play around with different image diffusion models like Stable Diffusion and specific fine-tuned variations for ControlNet or LoRA that I plug into ComfyUI. But I can't use it at work because of the licensing. I have to use InvokeAI instead of ComfyUI if I want to be careful and only very specific image diffusion models without the latest and greatest fine-tuning. As others have said - the weights themselves are rather inscrutable. So we're building on more abstract shapes now. But the key open thing is making sure (1) the tools to modify the weights are open and permissive (ComfyUI, related scripts or parts of both the training and deployment) and (2) the underlying weights of the base models and the tools to recreate them have MIT or other generous licensing. As well as the fine-tuned variants for specific tasks. It's not going to be the naive construction in the future where you take a base model and as company A you produce company A's fine tuned model and you're done. It's going to be a tree of fine-tuned models as a node-based editor like ComfyUI already shows and that whole tree has to be open if we're to keep the same hacker spirit where anyone can tinker with it and also at some point make money off of it. Or go free software the whole way (i.e., LGPL or equivalent the whole tree of tools). In that sense unfortunately Llama has a ways to go to be truly open: https://news.ycombinator.com/item?id=36816395 > This is how we’ve managed security on our social networks – our more robust AI systems identify and stop threats from less sophisticated actors who often use smaller scale AI systems. Ok, first of all, has this really worked? AI moderators still can't capture the mass of obvious spam/bots on all their platforms, threads included. Second, AI detection doesn't work, and with how much better the systems are getting, it's probably never going to, unless you keep the best models for yourself, and it's is clear from the rest of the note that its not zuck's intention to do so. > As long as everyone has access to similar generations of models – which open source promotes – then governments and institutions with more compute resources will be able to check bad actors with less compute. This just doesn't make sense. How are you going to prevent AI spam, AI deepfakes from causing harm with more compute? What are you gonna do with more compute about nonconsensual deepfakes? People are already using AI to bypass identity verification on your social media networks, and pump out loads of spam. This is really good news. Zuck sees the inevitability of it and the dystopian regulatory landscape and decided to go all in. This also has the important effect of neutralizing the critique of US Government AI regulation because it will democratize ""frontier"" models and make enforcement nearly impossible. Thank you, Zuck, this is an important and historic move. It also opens up the market to a lot more entry in the area of ""ancillary services to support the effective use of frontier models"" (including safety-oriented concerns), which should really be the larger market segment. The ""open source"" part sounds nice, though we all know there's nothing particularly open about the models (or their weights). The barriers to entry remain the same - huge upfront investments to train your own, and steep ongoing costs for ""inference"". Is the vision here to treat LLM-based AI as a ""public good"", akin to a utility provider in a civilized country (taxpayer funded, govt maintained, non-for-profit)? I think we could arguably call this ""open source"" when all the infra blueprints, scripts and configs are freely available for anyone to try and duplicate the state-of-the-art (resource and grokking requirements nonwithstanding) Sure but under what license? Because slapping “open source” on the model doesn’t make it open source if it’s not actually license that way. The 3.1 license still contains their non-commercial clause (over 700m users) and requires derivatives, whether fine tunings or trained on generated data, to use the llama name. Interesting discussion! While I agree with Zuckerberg's vision, the comments raise valid concerns. The point about GPU accessibility and cost is crucial. Public clusters are great, but sustainable funding and equitable access are essential to avoid exacerbating existing inequalities. I also resonate with the call for CUDA alternatives. Breaking the dependence on proprietary technology is key for a truly open AI ecosystem. While existing research clusters offer some access, their scope and resources often pale in comparison to what companies like Meta are proposing. We need a multi-pronged approach: open-sourcing models AND investing in accessible infrastructure, diverse hardware options, and sustainable funding models for a truly democratic AI future. > Third, a key difference between Meta and closed model providers is that selling access to AI models isn’t our business model. That means openly releasing Llama doesn’t undercut our revenue, sustainability, or ability to invest in research like it does for closed providers. (This is one reason several closed providers consistently lobby governments against open source.) The whole thing is interesting, but this part strikes me as potentially anticompetitive reasoning. I wonder what the lines are that they have to avoid crossing here? Llama isn't open source. The license is at https://llama.meta.com/llama3/license/ and includes various restrictions on use, which means it falls outside the rules created by the https://opensource.org/osd Additional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights. Which open-source has such restrictions and clause? Open source ""AI"" is a proxy for democratising and making (much) more widely useful the goodies of high performance computing (HPC). The HPC domain (data and compute intensive applications that typically need vector, parallel or other such architectures) have been around for the longest time, but confined to academic / government tasks. LLM's with their famous ""matrix multiply"" at their very core are basically demolishing an ossified frontier where a few commercial entities (Intel, Microsoft, Apple, Google, Samsung etc) have defined for decades what computing looks like for most people . Assuming that the genie is out of the bottle, the question is: what is the shape of end-user devices that are optimally designed to use compute intensive open source algorithms? The ""AI PC"" is already a marketing gimmick, but could it be that Linux desktops and smartphones will suddenly be ""ΑΙ natives""? For sure its a transformational period and the landscape T+10 yrs could be drastically different... The FTC also recently put out a statement that is fairly pro-open source: https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/202... I think it's interesting to think about this question of open source, benefits, risk, and even competition, without all of the baggage that Meta brings. I agree with the FTC, that the benefits of open-weight models are significant for competition. The challenge is in distinguishing between good competition and bad competition. Some kind of competition can harm consumers and critical public goods, including democracy itself. For example, competing for people's scarce attention or for their food buying, with increasingly optimized and addictive innovations. Or competition to build the most powerful biological weapons. Other kinds of competition can massively accelerate valuable innovation. The FTC must navigate a tricky balance here — leaning into competition that serves consumers and the broader public, while being careful about what kind of competition it is accelerating that could cause significant risk and harm. It's also obviously not just ""big tech"" that cares about the risks behind open-weight foundation models. Many people have written about these risks even before it became a subject of major tech investment. (In other words, A16Z's framing is often rather misleading.) There are many non-big tech actors who are very concerned about current and potential negative impacts of open-weight foundation models. One approach which can provide the best of both worlds, is for cases where there are significant potential risks, to ensure that there is at least some period of time where weights are not provided openly, in order to learn a bit about the potential implications of new models. Longer-term, there may be a line where models are too risky to share openly, and it may be unclear what that line is. In that case, it's important that we have governance systems for such decisions that are not just profit-driven, and which can help us continue to get the best of all worlds. (Plug: my organization, the AI & Democracy Foundation; https://ai-dem.org/; is working to develop such systems and hiring.) In general I look back on my time at FB with mixed feelings, I’m pretty skeptical that modern social media is a force for good and I was there early enough to have moved the needle. But this is really positive stuff and it’s nice to view my time there through the lens of such a change for the better. Keep up the good work on this folks. Time to start thinking about opening up a little on the training data. Who knew FB would hold OpenAI's original ideals, and OpenAI now holds early FB ideals/integrity. Meta's article with more details on the new LLAMA 3.1 https://ai.meta.com/blog/meta-llama-3-1/ The irony of this letter being written by Mark Zuckerburg at Meta, while OpenAI continues to be anything but open, is richer than anyone could have imagined. Interview with Mark Zuckerberg released today: https://www.bloomberg.com/news/videos/2024-07-23/mark-zucker... Meanwhile Facebook is flooded with AI-generated slop with hundreds of thousands of other bots interacting with it to boost it to whoever is insane enough to still use that putrid hellhole of a mass-data-harvesting platform. Dead internet theory is very much happening in real time, and I dread what's about to come since the world has collectively decided to lose their minds with this AI crap. And people on this site are unironically excited about this garbage that is indistinguishable from spam getting more and more popular. What a fucking joke I thoroughly support Meta's open-sourcing of these AI models going forward. However, for a company that absolutely closed down discussions about providing API access to their platform, I'm left wondering what's in it (monetarily) for them by doing this? Is it to simply undercut competition in the space, like some grocery store selling below cost? Is there an argument against Open Source AI? Not the usual nation-state rhetoric, but something that justifies that closed source leads to better user-experience and fewer security and privacy issues. An ecosystem that benefits vendors, customers, and the makers of close source? Are there historical analogies other than Microsoft Windows or Apple iPhone / iOS? It'll be interesting to come back here in a couple of years and see what's left. What do they even do anymore? They have Facebook, which hasn't visibly changed in a decade. They have Instagram, which feels a bit sleeker but also remained more or less the same. and Whatsapp. Ad network that runs on top of those services and floods them with trash. Bunch of stuff that doesn't seem to exist anymore - Libra, the grandiose multibillion dollar Legless VR, etc. But they still have 70 thousand people (a small country) doing _something_. What are they doing? Updating Facebook UI? Not really, the UI hasn't been updated, and you don't need 70 thousand people to do that. Stuff like React and Llama? Good, I guess, we'll see how they make use of Llama in a couple of years. Spellcheck for posts maybe? Llama 3.1 405B is on par with GPT-4o and Claude 3.5 Sonnet, the 70B model is better than GPT 3.5 turbo, incredible. > We need to protect our data. This is a very important concern in Health Care because of HIPAA compliance. You can't just send your data over the wire to someone's proprietary API. You would at least need to de-identify your data. This can be a tricky task, especially with unstructured text. Just added Llama 3.1 405B/70B/8B to https://double.bot (VSCode coding assistant) if anyone would like to try it. --- Some observations: * The model is much better at trajectory correcting and putting out a chain of tangential thoughts than other frontier models like Sonnet or GPT-4o. Usually, these models are limited to outputting ""one thought"", no matter how verbose that thought might be. * I remember in Dec of 2022 telling famous ""tier 1"" VCs that frontier models would eventually be like databases: extremely hard to build, but the best ones will eventually be open and win as it's too important to too many large players. I remember the confidence in their ridicule at the time but it seems increasingly more likely that this will be true. > My framework for understanding safety is that we need to protect against two categories of harm: unintentional and intentional. Unintentional harm is when an AI system may cause harm even when it was not the intent of those running it to do so. For example, modern AI models may inadvertently give bad health advice. Or, in more futuristic scenarios, some worry that models may unintentionally self-replicate or hyper-optimize goals to the detriment of humanity. Intentional harm is when a bad actor uses an AI model with the goal of causing harm. Okay then Mark. Replace ""modern AI models"" with ""social media"" and repeat this statement with a straight face. Okay if anyone wants to try Llama 3.1 inference on CPU, try this: https://github.com/trholding/llama2.c (L2E) It's a bit buggy but it is fun. Disclaimer: I am the author of L2E When Zuck said spy can easily steal models, I wonder how much of it comes from experiences. I remember they struggled to train OPT not long ago. On a more serious note, I don't really buy his arguments about safety. First, widespread AI does not reduce unintentional harm but increases it, because the rate of accident is compound. Second, the chance of success for threat actors will increase, because of the asymmetric advantage of gaining access to all open information and hiding their own information. But there is no reverse at this point, I enjoy it while it lasts, AGI will come sooner or later anyway. How are smaller models distilled from large models, I know of LoRA, quantization like technique; but does distilling also mean generating new datasets for conversing with smaller models entirely from the big models for many simpler tasks? Looks like you can already try out Llama-3.1-405b on Groq, although it's timing out. So. Hugged I guess. I think all this discussion around Open-source AI is a total distraction from the elephants in the room. Let's list what you need to run/play around with something like Llama: 1. Software: this is all Pytorch/HF, so completely open-source. This is total parity between what corporates have and what the public has. 2. Model weights: Meta and a few other orgs release open models - as opposed to OpenAI's closed models. So, ok, we have something to work with. 3. Data: to actually do anything useful you need tons of data. This is beyond the reach of the ordinary man, setting aside the legality issues. 4. Hardware: GPUs, which are extremely expensive. Not just that, even if you have the top dollars, you have to go stand in a queue and wait for O(months), since mega-corporates have gotten there before you. For Inference, you need 1,2 and 4. For training (or fine-tuning), you need all of these. With newer and larger models like the latest Llama, 4 is truly beyond the reach of ordinary entities. This is NOTHING like open-source, where a random guy can edit/recompile/deploy software on a commodity computer. Wrt LLMs, Data/Hardware are in the equation, the playing field is complete stacked. This thread has a bunch of people discussing nuances of 1 and 2, but this bike-shedding only hides the basic point: Control of LLMs are for mega-corps, not for individuals. I'm really unsure if it's a good idea given the current geopolitics. Open-Source Code in the past was fantastic because the West had a monopoly on CPUs and computers. Sharing and contributing was amazing while ensured that tyrants couldn't use this tech to harm people simply because they don't have a hardware to run. But now, things are different. China is advancing in chip technology, and Russia is using open-source AI to harm people on the scale today, with auto-targeting drones being just the start. Red sea conflict etc. And somehow, Zuckerberg keeps finding ways to mess up people's lives, despite having the best intentions. Right now you can build a semi-autonomous drone with AI to kill people for ~$500-700. The western world will still use safe and secure commercial models. While new axis of evil will use models based on Meta or any other open source to do whatever harm they can imagine with not a hint of control. This particular model. Fine-tune it to develop a nuclear bomb using all possible research that level of government can get on the scale. Killing drone swarms etc. Once the knowledge got public these models can be a base model to get expert-level knowledge to anyone who wants it, uncensored. Especially if you are government that wants to destroy a peaceful order for whatever reason. Only if it is truly open source (open data sets, transparent curation/moderation/censorship of data sets, open training source code, open evaluation suites, and an OSI approved open source license). Open weights (and open inference code) is NOT open source, but just some weak open washing marketing. The model that comes closest to being TRULY open is AI2’s OLMo. See their blog post on their approach: https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e73... I think the only thing they’re not open about is how they’ve curated/censored their “Dolma” training data set, as I don’t think they explicitly share each decision made or the original uncensored dataset: https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-co... By the way, OSI is working on defining open source for AI. They post weekly updates to their blog. Example: https://opensource.org/blog/open-source-ai-definition-weekly... Totally tangential thought, probably doomed to be lost in the flood of comments on this very interesting announcement. I was thinking today about Musk, Zuckerberg and Altman. Each claims that the next version of their big LLMs will be the best. For some reason it reminded me of one apocryphal cause of WW1, which was that the kings of Europe were locked in a kind of ego driven contest. It made me think about the Nation State as a technology. In some sense, the kings were employing the new technology which was clearly going to be the basis for the future political order. And they were pitting their own implementation of this new technology against the other kings. I feel we are seeing a similar clash of kings playing out. The claims that this is all just business or some larger claim about the good of humanity seem secondary to the ego stakes of the major players. And when it was about who built the biggest rocket, it felt less dangerous. It breaks my heart just a little bit. I feel sympathy in some sense for the AIs we will create, especially if they do reach the level of AGI. As another tortured analogy, it is like a bunch of competitive parents forcing their children into adversarial relationships to satisfy the parent's ego. this is very cool indeed that meta has made available more than they need to in terms of model weights . however, the ""open-source"" narrative is being pushed a bit too much like descriptive ML models were called ""AI"", or applied statistics ""data science"". with reinforced examples such as this, we start to lose the original meaning of the term. the current approach of startups or small players ""open-sourcing"" their platforms and tools as a means to promote network effect works but is harmful in the long run. you will find examples of terraform and red hat happening, and a very segmented market. if you want the true spirit of open-source, there must be a way to replicate the weights through access to training data and code. whether one could afford millions of GPU hours or not, real innovation would come from remixing the internals, and not just fine-tuning existing stuff. i understand that this is not realistically going to ever happen, but don't perform deceptive marketing at the same time. I never thought I would say this but thanks Meta. *I reserve the right to remove this praise if they abuse this open source model position in the future. I am not deep into llms so I ask this. From my understanding, their last model was open source but it was in a way that you can use them but the inner working were ""hidden""/not transparent. With the new model, I am seeing alot of how open source they are and can be build upon. Is it now completely open source or similar to their last models ? The real path forward is recognizing what AI is good at and what it is bad at. Focus on making what it is good at even better and faster. Open AI will definitely give us that option but it isn't a miracle worker. My impression is that AI if done correctly will be the new way to build APIs with large data sets and information. It can't write code unless you want to dump billions of dollars into a solution with millions of dollars of operational costs. As it stands it loses context too quickly to do advance human tasks. BUT this is where it is great at assembling data and information. You know what is great at assembling data and information? APIs. Think of it this way if we can make it faster and it trains on a datalake for a company it could be used to return information faster than a nested micro-service architecture that is just a spiderweb of dependencies. Because AI loses context simple API requests could actually be more efficient. The question is what is ""open source"" in the case of a matrix of numbers, as opposed to code. Also, are there any ""IP"" rights attached at all to a bunch numbers coming out of a formula that someone else calculated for you? (edit: after all, a ""model"" is just a matrix of numbers coming out of running a training algorithm that is not owned by Meta over training data that is not owned by Meta.) Meta imposes a notification duty AND a request for another license (no mention of the details of these) for applications of their model with a large number of users. This is against the spirit of open source. (In practical terms it is not a show stopper since you can easily switch models, although they all have subtlely different behaviours and quality levels.) Zuck needs to get real. They are Open Weights not Open Source. Open source is a welcome step but what we really need is complete decentralisation so people can run their own private AI Models that keep all the data private to them. We need this to happen locally on laptops, mobile phones, smart devices etc. Waiting for when that will become ubiquitous.","Open source AI is the path forward nan Related ongoing thread: Llama 3.1 - https://news.ycombinator.com/item?id=41046540 - July 2024 (114 comments) “The Heavy Press Program was a Cold War-era program of the United States Air Force to build the largest forging presses and extrusion presses in the world.” This ”program began in 1944 and concluded in 1957 after construction of four forging presses and six extruders, at an overall cost of $279 million. Six of them are still in operation today, manufacturing structural parts for military and commercial aircraft” [1]. $279mm in 1957 dollars is about $3.2bn today [2]. A public cluster of GPUs provided for free to American universities, companies and non-profits might not be a bad idea. [1] https://en.m.wikipedia.org/wiki/Heavy_Press_Program [2] https://data.bls.gov/cgi-bin/cpicalc.pl?cost1=279&year1=1957... ""Eventually though, open source Linux gained popularity – initially because it allowed developers to modify its code however they wanted ..."" I find the language around ""open source AI"" to be confusing. With ""open source"" there's usually ""source"" to open, right? As in, there is human legible code that can be read and modified by the user? If so, then how can current ML models be open source? They're very large matrices that are, for the most part, inscrutable to the user. They seem akin to binaries, which, yes, can be modified by the user, but are extremely obscured to the user, and require enormous effort to understand and effectively modify. ""Open source"" code is not just code that isn't executed remotely over an API, and it seems like maybe its being conflated with that here? The big winners of this: devs and AI startups - No more vendor lock-in - Instead of just wrapping proprietary API endpoints, developers can now integrate AI deeply into their products in a very cost-effective and performant way - Price race to the bottom with near-instant LLM responses at very low prices are on the horizon As a founder, it feels like a very exciting time to build a startup as your product automatically becomes better, cheaper, and more scalable with every major AI advancement. This leads to a powerful flywheel effect: https://www.kadoa.com/blog/ai-flywheel Even if it's just open weights and not ""true"" open source, I'll still give Meta the appreciation of being one of the few big AI companies actually committed to open models. In an ecosystem where groups like Anthropic and OpenAI keep hemming and hawing about safety and the necessity of closed AI systems ""for our sake"", they stand out among the rest. They are positioning themselves as champions of AI open source mostly because they were blindsided by OpenAI, are not in the infra game, and want to commoditize their complements as much as possible. This is not altruism although it's still great for devs and startups. All FB GPU investments is primarily for new AI products ""friends"", recommendations and selling ads. https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/ I wish Meta stopped using the ""open source"" misnomer for free of charge weights. In the US the FTC already uses the term Open-Weights , and it seems the industry is also adopting this term (e.g. Mistral). Someone can correct me here but AFAIK we don't even know which datasets are used to train these models, so why should we even use ""open"" to describe Llama? This is more similar to a freeware than an open-source project. [1] https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/202... Meta makes their money off advertising, which means they profit from attention. This means they need content that will grab attention, and creating open source models that allow anyone to create any content on their own becomes good for Meta. The users of the models can post it to their Instagram/FB/Threads account. Releasing an open model also releases Meta from the burden of having to police the content the model generates, once the open source community fine-tunes the models. Overall, this move is good business move for Meta - the post doesn't really talk about the true benefit, instead moralizing about open source, but this is a sound business move for Meta. Huge companies like facebook will often argue for solutions that on the surface, seem to be in the public interest. But I have strong doubts they (or any other company) actually believe what they are saying. Here is the reality: - Facebook is spending untold billions on GPU hardware. - Facebook is arguing in favor of open sourcing the models, that they spent billions of dollars to generate, for free...? It follows that companies with much smaller resources (money) will not be able to match what Facebook is doing. Seems like an attempt to kill off the competition (specifically, smaller organizations) before they can take root. > We’re releasing Llama 3.1 405B, the first frontier-level open source AI model, as well as new and improved Llama 3.1 70B and 8B models. Bravo! While I don't agree with Zuck's views and actions on many fronts, on this occasion I think he and the AI folks at Meta deserve our praise and gratitude. With this release, they have brought the cost of pretraining a frontier 400B+ parameter model to ZERO for pretty much everyone -- well, everyone except Meta's key competitors.[a] THANK YOU ZUCK. Meanwhile, the business-minded people at Meta surely won't mind if the release of these frontier models to the public happens to completely mess up the AI plans of competitors like OpenAI/Microsoft, Google, Anthropic, etc. Come to think of it, the negative impact on such competitors was likely a key motivation for releasing the new models. --- [a] The license is not open to the handful of companies worldwide which have more than 700M users. I've summarized this entire thread in 4 lines (didn't even use AI for it!) Step 1. Chick-Fil-A releases a grass-fed beef burger to spite other fast-food joints, calls it ""the vegan burger"" Step 2. A couple of outraged vegans show up in the comments, pointing out that beef, even grass-fed beef, isn't vegan Step 3. Fast food enthusiasts push back: it's unreasonable to want companies to abide by this restrictive definition of ""vegan"". Clearly this burger is a gamechanger and the definition needs to adapt to the times. Step 4. Goto Step 2 in an infinite loop Software 2.0 is about open licensing. I.e., the more important thing - the more ""free"" thing - is the licensing now. E.g., I play around with different image diffusion models like Stable Diffusion and specific fine-tuned variations for ControlNet or LoRA that I plug into ComfyUI. But I can't use it at work because of the licensing. I have to use InvokeAI instead of ComfyUI if I want to be careful and only very specific image diffusion models without the latest and greatest fine-tuning. As others have said - the weights themselves are rather inscrutable. So we're building on more abstract shapes now. But the key open thing is making sure (1) the tools to modify the weights are open and permissive (ComfyUI, related scripts or parts of both the training and deployment) and (2) the underlying weights of the base models and the tools to recreate them have MIT or other generous licensing. As well as the fine-tuned variants for specific tasks. It's not going to be the naive construction in the future where you take a base model and as company A you produce company A's fine tuned model and you're done. It's going to be a tree of fine-tuned models as a node-based editor like ComfyUI already shows and that whole tree has to be open if we're to keep the same hacker spirit where anyone can tinker with it and also at some point make money off of it. Or go free software the whole way (i.e., LGPL or equivalent the whole tree of tools). In that sense unfortunately Llama has a ways to go to be truly open: https://news.ycombinator.com/item?id=36816395 > This is how we’ve managed security on our social networks – our more robust AI systems identify and stop threats from less sophisticated actors who often use smaller scale AI systems. Ok, first of all, has this really worked? AI moderators still can't capture the mass of obvious spam/bots on all their platforms, threads included. Second, AI detection doesn't work, and with how much better the systems are getting, it's probably never going to, unless you keep the best models for yourself, and it's is clear from the rest of the note that its not zuck's intention to do so. > As long as everyone has access to similar generations of models – which open source promotes – then governments and institutions with more compute resources will be able to check bad actors with less compute. This just doesn't make sense. How are you going to prevent AI spam, AI deepfakes from causing harm with more compute? What are you gonna do with more compute about nonconsensual deepfakes? People are already using AI to bypass identity verification on your social media networks, and pump out loads of spam. This is really good news. Zuck sees the inevitability of it and the dystopian regulatory landscape and decided to go all in. This also has the important effect of neutralizing the critique of US Government AI regulation because it will democratize ""frontier"" models and make enforcement nearly impossible. Thank you, Zuck, this is an important and historic move. It also opens up the market to a lot more entry in the area of ""ancillary services to support the effective use of frontier models"" (including safety-oriented concerns), which should really be the larger market segment. The ""open source"" part sounds nice, though we all know there's nothing particularly open about the models (or their weights). The barriers to entry remain the same - huge upfront investments to train your own, and steep ongoing costs for ""inference"". Is the vision here to treat LLM-based AI as a ""public good"", akin to a utility provider in a civilized country (taxpayer funded, govt maintained, non-for-profit)? I think we could arguably call this ""open source"" when all the infra blueprints, scripts and configs are freely available for anyone to try and duplicate the state-of-the-art (resource and grokking requirements nonwithstanding) Sure but under what license? Because slapping “open source” on the model doesn’t make it open source if it’s not actually license that way. The 3.1 license still contains their non-commercial clause (over 700m users) and requires derivatives, whether fine tunings or trained on generated data, to use the llama name. Interesting discussion! While I agree with Zuckerberg's vision, the comments raise valid concerns. The point about GPU accessibility and cost is crucial. Public clusters are great, but sustainable funding and equitable access are essential to avoid exacerbating existing inequalities. I also resonate with the call for CUDA alternatives. Breaking the dependence on proprietary technology is key for a truly open AI ecosystem. While existing research clusters offer some access, their scope and resources often pale in comparison to what companies like Meta are proposing. We need a multi-pronged approach: open-sourcing models AND investing in accessible infrastructure, diverse hardware options, and sustainable funding models for a truly democratic AI future. > Third, a key difference between Meta and closed model providers is that selling access to AI models isn’t our business model. That means openly releasing Llama doesn’t undercut our revenue, sustainability, or ability to invest in research like it does for closed providers. (This is one reason several closed providers consistently lobby governments against open source.) The whole thing is interesting, but this part strikes me as potentially anticompetitive reasoning. I wonder what the lines are that they have to avoid crossing here? Llama isn't open source. The license is at https://llama.meta.com/llama3/license/ and includes various restrictions on use, which means it falls outside the rules created by the https://opensource.org/osd Additional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights. Which open-source has such restrictions and clause? Open source ""AI"" is a proxy for democratising and making (much) more widely useful the goodies of high performance computing (HPC). The HPC domain (data and compute intensive applications that typically need vector, parallel or other such architectures) have been around for the longest time, but confined to academic / government tasks. LLM's with their famous ""matrix multiply"" at their very core are basically demolishing an ossified frontier where a few commercial entities (Intel, Microsoft, Apple, Google, Samsung etc) have defined for decades what computing looks like for most people . Assuming that the genie is out of the bottle, the question is: what is the shape of end-user devices that are optimally designed to use compute intensive open source algorithms? The ""AI PC"" is already a marketing gimmick, but could it be that Linux desktops and smartphones will suddenly be ""ΑΙ natives""? For sure its a transformational period and the landscape T+10 yrs could be drastically different... The FTC also recently put out a statement that is fairly pro-open source: https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/202... I think it's interesting to think about this question of open source, benefits, risk, and even competition, without all of the baggage that Meta brings. I agree with the FTC, that the benefits of open-weight models are significant for competition. The challenge is in distinguishing between good competition and bad competition. Some kind of competition can harm consumers and critical public goods, including democracy itself. For example, competing for people's scarce attention or for their food buying, with increasingly optimized and addictive innovations. Or competition to build the most powerful biological weapons. Other kinds of competition can massively accelerate valuable innovation. The FTC must navigate a tricky balance here — leaning into competition that serves consumers and the broader public, while being careful about what kind of competition it is accelerating that could cause significant risk and harm. It's also obviously not just ""big tech"" that cares about the risks behind open-weight foundation models. Many people have written about these risks even before it became a subject of major tech investment. (In other words, A16Z's framing is often rather misleading.) There are many non-big tech actors who are very concerned about current and potential negative impacts of open-weight foundation models. One approach which can provide the best of both worlds, is for cases where there are significant potential risks, to ensure that there is at least some period of time where weights are not provided openly, in order to learn a bit about the potential implications of new models. Longer-term, there may be a line where models are too risky to share openly, and it may be unclear what that line is. In that case, it's important that we have governance systems for such decisions that are not just profit-driven, and which can help us continue to get the best of all worlds. (Plug: my organization, the AI & Democracy Foundation; https://ai-dem.org/; is working to develop such systems and hiring.) In general I look back on my time at FB with mixed feelings, I’m pretty skeptical that modern social media is a force for good and I was there early enough to have moved the needle. But this is really positive stuff and it’s nice to view my time there through the lens of such a change for the better. Keep up the good work on this folks. Time to start thinking about opening up a little on the training data. Who knew FB would hold OpenAI's original ideals, and OpenAI now holds early FB ideals/integrity. Meta's article with more details on the new LLAMA 3.1 https://ai.meta.com/blog/meta-llama-3-1/ The irony of this letter being written by Mark Zuckerburg at Meta, while OpenAI continues to be anything but open, is richer than anyone could have imagined. Interview with Mark Zuckerberg released today: https://www.bloomberg.com/news/videos/2024-07-23/mark-zucker... Meanwhile Facebook is flooded with AI-generated slop with hundreds of thousands of other bots interacting with it to boost it to whoever is insane enough to still use that putrid hellhole of a mass-data-harvesting platform. Dead internet theory is very much happening in real time, and I dread what's about to come since the world has collectively decided to lose their minds with this AI crap. And people on this site are unironically excited about this garbage that is indistinguishable from spam getting more and more popular. What a fucking joke I thoroughly support Meta's open-sourcing of these AI models going forward. However, for a company that absolutely closed down discussions about providing API access to their platform, I'm left wondering what's in it (monetarily) for them by doing this? Is it to simply undercut competition in the space, like some grocery store selling below cost? Is there an argument against Open Source AI? Not the usual nation-state rhetoric, but something that justifies that closed source leads to better user-experience and fewer security and privacy issues. An ecosystem that benefits vendors, customers, and the makers of close source? Are there historical analogies other than Microsoft Windows or Apple iPhone / iOS? It'll be interesting to come back here in a couple of years and see what's left. What do they even do anymore? They have Facebook, which hasn't visibly changed in a decade. They have Instagram, which feels a bit sleeker but also remained more or less the same. and Whatsapp. Ad network that runs on top of those services and floods them with trash. Bunch of stuff that doesn't seem to exist anymore - Libra, the grandiose multibillion dollar Legless VR, etc. But they still have 70 thousand people (a small country) doing _something_. What are they doing? Updating Facebook UI? Not really, the UI hasn't been updated, and you don't need 70 thousand people to do that. Stuff like React and Llama? Good, I guess, we'll see how they make use of Llama in a couple of years. Spellcheck for posts maybe? Llama 3.1 405B is on par with GPT-4o and Claude 3.5 Sonnet, the 70B model is better than GPT 3.5 turbo, incredible. > We need to protect our data. This is a very important concern in Health Care because of HIPAA compliance. You can't just send your data over the wire to someone's proprietary API. You would at least need to de-identify your data. This can be a tricky task, especially with unstructured text. Just added Llama 3.1 405B/70B/8B to https://double.bot (VSCode coding assistant) if anyone would like to try it. --- Some observations: * The model is much better at trajectory correcting and putting out a chain of tangential thoughts than other frontier models like Sonnet or GPT-4o. Usually, these models are limited to outputting ""one thought"", no matter how verbose that thought might be. * I remember in Dec of 2022 telling famous ""tier 1"" VCs that frontier models would eventually be like databases: extremely hard to build, but the best ones will eventually be open and win as it's too important to too many large players. I remember the confidence in their ridicule at the time but it seems increasingly more likely that this will be true. > My framework for understanding safety is that we need to protect against two categories of harm: unintentional and intentional. Unintentional harm is when an AI system may cause harm even when it was not the intent of those running it to do so. For example, modern AI models may inadvertently give bad health advice. Or, in more futuristic scenarios, some worry that models may unintentionally self-replicate or hyper-optimize goals to the detriment of humanity. Intentional harm is when a bad actor uses an AI model with the goal of causing harm. Okay then Mark. Replace ""modern AI models"" with ""social media"" and repeat this statement with a straight face. Okay if anyone wants to try Llama 3.1 inference on CPU, try this: https://github.com/trholding/llama2.c (L2E) It's a bit buggy but it is fun. Disclaimer: I am the author of L2E When Zuck said spy can easily steal models, I wonder how much of it comes from experiences. I remember they struggled to train OPT not long ago. On a more serious note, I don't really buy his arguments about safety. First, widespread AI does not reduce unintentional harm but increases it, because the rate of accident is compound. Second, the chance of success for threat actors will increase, because of the asymmetric advantage of gaining access to all open information and hiding their own information. But there is no reverse at this point, I enjoy it while it lasts, AGI will come sooner or later anyway. How are smaller models distilled from large models, I know of LoRA, quantization like technique; but does distilling also mean generating new datasets for conversing with smaller models entirely from the big models for many simpler tasks? Looks like you can already try out Llama-3.1-405b on Groq, although it's timing out. So. Hugged I guess. I think all this discussion around Open-source AI is a total distraction from the elephants in the room. Let's list what you need to run/play around with something like Llama: 1. Software: this is all Pytorch/HF, so completely open-source. This is total parity between what corporates have and what the public has. 2. Model weights: Meta and a few other orgs release open models - as opposed to OpenAI's closed models. So, ok, we have something to work with. 3. Data: to actually do anything useful you need tons of data. This is beyond the reach of the ordinary man, setting aside the legality issues. 4. Hardware: GPUs, which are extremely expensive. Not just that, even if you have the top dollars, you have to go stand in a queue and wait for O(months), since mega-corporates have gotten there before you. For Inference, you need 1,2 and 4. For training (or fine-tuning), you need all of these. With newer and larger models like the latest Llama, 4 is truly beyond the reach of ordinary entities. This is NOTHING like open-source, where a random guy can edit/recompile/deploy software on a commodity computer. Wrt LLMs, Data/Hardware are in the equation, the playing field is complete stacked. This thread has a bunch of people discussing nuances of 1 and 2, but this bike-shedding only hides the basic point: Control of LLMs are for mega-corps, not for individuals. I'm really unsure if it's a good idea given the current geopolitics. Open-Source Code in the past was fantastic because the West had a monopoly on CPUs and computers. Sharing and contributing was amazing while ensured that tyrants couldn't use this tech to harm people simply because they don't have a hardware to run. But now, things are different. China is advancing in chip technology, and Russia is using open-source AI to harm people on the scale today, with auto-targeting drones being just the start. Red sea conflict etc. And somehow, Zuckerberg keeps finding ways to mess up people's lives, despite having the best intentions. Right now you can build a semi-autonomous drone with AI to kill people for ~$500-700. The western world will still use safe and secure commercial models. While new axis of evil will use models based on Meta or any other open source to do whatever harm they can imagine with not a hint of control. This particular model. Fine-tune it to develop a nuclear bomb using all possible research that level of government can get on the scale. Killing drone swarms etc. Once the knowledge got public these models can be a base model to get expert-level knowledge to anyone who wants it, uncensored. Especially if you are government that wants to destroy a peaceful order for whatever reason. Only if it is truly open source (open data sets, transparent curation/moderation/censorship of data sets, open training source code, open evaluation suites, and an OSI approved open source license). Open weights (and open inference code) is NOT open source, but just some weak open washing marketing. The model that comes closest to being TRULY open is AI2’s OLMo. See their blog post on their approach: https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e73... I think the only thing they’re not open about is how they’ve curated/censored their “Dolma” training data set, as I don’t think they explicitly share each decision made or the original uncensored dataset: https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-co... By the way, OSI is working on defining open source for AI. They post weekly updates to their blog. Example: https://opensource.org/blog/open-source-ai-definition-weekly... Totally tangential thought, probably doomed to be lost in the flood of comments on this very interesting announcement. I was thinking today about Musk, Zuckerberg and Altman. Each claims that the next version of their big LLMs will be the best. For some reason it reminded me of one apocryphal cause of WW1, which was that the kings of Europe were locked in a kind of ego driven contest. It made me think about the Nation State as a technology. In some sense, the kings were employing the new technology which was clearly going to be the basis for the future political order. And they were pitting their own implementation of this new technology against the other kings. I feel we are seeing a similar clash of kings playing out. The claims that this is all just business or some larger claim about the good of humanity seem secondary to the ego stakes of the major players. And when it was about who built the biggest rocket, it felt less dangerous. It breaks my heart just a little bit. I feel sympathy in some sense for the AIs we will create, especially if they do reach the level of AGI. As another tortured analogy, it is like a bunch of competitive parents forcing their children into adversarial relationships to satisfy the parent's ego. this is very cool indeed that meta has made available more than they need to in terms of model weights . however, the ""open-source"" narrative is being pushed a bit too much like descriptive ML models were called ""AI"", or applied statistics ""data science"". with reinforced examples such as this, we start to lose the original meaning of the term. the current approach of startups or small players ""open-sourcing"" their platforms and tools as a means to promote network effect works but is harmful in the long run. you will find examples of terraform and red hat happening, and a very segmented market. if you want the true spirit of open-source, there must be a way to replicate the weights through access to training data and code. whether one could afford millions of GPU hours or not, real innovation would come from remixing the internals, and not just fine-tuning existing stuff. i understand that this is not realistically going to ever happen, but don't perform deceptive marketing at the same time. I never thought I would say this but thanks Meta. *I reserve the right to remove this praise if they abuse this open source model position in the future. I am not deep into llms so I ask this. From my understanding, their last model was open source but it was in a way that you can use them but the inner working were ""hidden""/not transparent. With the new model, I am seeing alot of how open source they are and can be build upon. Is it now completely open source or similar to their last models ? The real path forward is recognizing what AI is good at and what it is bad at. Focus on making what it is good at even better and faster. Open AI will definitely give us that option but it isn't a miracle worker. My impression is that AI if done correctly will be the new way to build APIs with large data sets and information. It can't write code unless you want to dump billions of dollars into a solution with millions of dollars of operational costs. As it stands it loses context too quickly to do advance human tasks. BUT this is where it is great at assembling data and information. You know what is great at assembling data and information? APIs. Think of it this way if we can make it faster and it trains on a datalake for a company it could be used to return information faster than a nested micro-service architecture that is just a spiderweb of dependencies. Because AI loses context simple API requests could actually be more efficient. The question is what is ""open source"" in the case of a matrix of numbers, as opposed to code. Also, are there any ""IP"" rights attached at all to a bunch numbers coming out of a formula that someone else calculated for you? (edit: after all, a ""model"" is just a matrix of numbers coming out of running a training algorithm that is not owned by Meta over training data that is not owned by Meta.) Meta imposes a notification duty AND a request for another license (no mention of the details of these) for applications of their model with a large number of users. This is against the spirit of open source. (In practical terms it is not a show stopper since you can easily switch models, although they all have subtlely different behaviours and quality levels.) Zuck needs to get real. They are Open Weights not Open Source. Open source is a welcome step but what we really need is complete decentralisation so people can run their own private AI Models that keep all the data private to them. We need this to happen locally on laptops, mobile phones, smart devices etc. Waiting for when that will become ubiquitous."
44163063,My AI skeptic friends are all nuts,nan,https://fly.io/blog/youre-all-nuts/,tabletcorry,2356,2025-06-02T21:09:53+00:00,,query,50,"I think this article is pretty spot on — it articulates something I’ve come to appreciate about LLM-assisted coding over the past few months. I started out very sceptical. When Claude Code landed, I got completely seduced — borderline addicted, slot machine-style — by what initially felt like a superpower. Then I actually read the code. It was shockingly bad. I swung back hard to my earlier scepticism, probably even more entrenched than before. Then something shifted. I started experimenting. I stopped giving it orders and began using it more like a virtual rubber duck. That made a huge difference. It’s still absolute rubbish if you just let it run wild, which is why I think “vibe coding” is basically just “vibe debt” — because it just doesn’t do what most (possibly uninformed) people think it does. But if you treat it as a collaborator — more like an idiot savant with a massive brain but no instinct or nous — or better yet, as a mech suit [0] that needs firm control — then something interesting happens. I’m now at a point where working with Claude Code is not just productive, it actually produces pretty good code, with the right guidance. I’ve got tests, lots of them. I’ve also developed a way of getting Claude to document intent as we go, which helps me, any future human reader, and, crucially, the model itself when revisiting old code. What fascinates me is how negative these comments are — how many people seem closed off to the possibility that this could be a net positive for software engineers rather than some kind of doomsday. Did Photoshop kill graphic artists? Did film kill theatre? Not really. Things changed, sure. Was it “better”? There’s no counterfactual, so who knows? But change was inevitable. What’s clear is this tech is here now, and complaining about it feels a bit like mourning the loss of punch cards when terminals showed up. [0]: https://matthewsinclair.com/blog/0178-why-llm-powered-progra... This article does not touch on the thing which worries me the most with respect to LLMs: the dependence . Unless you can run the LLM locally, on a computer you own, you are now completely dependent on a remote centralized system to do your work. Whoever controls that system can arbitrarily raise the prices, subtly manipulate the outputs, store and do anything they want with the inputs, or even suddenly cease to operate. And since, according to this article, only the latest and greatest LLM is acceptable (and I've seen that exact same argument six months ago), running locally is not viable (I've seen, in a recent discussion, someone mention a home server with something like 384G of RAM just to run one LLM locally). To those of us who like Free Software because of the freedom it gives us, this is a severe regression. One thing that I find truly amazing is just the simple fact that you can now be fuzzy with the input you give a computer, and get something meaningful in return. Like, as someone who grew up learning to code in the 90s it always seemed like science fiction that we'd get to a point where you could give a computer some vague human level instructions and get it more or less do what you want. I'm not a skeptic, but I keep LLMs on a short leash. This is a thoughtful article. Thanks `tptacek My LLM use is: 1 - tedious stuff; web pages interacting with domain back end. 2 - domain discovery. In a recent adventure, I used Claude 4 to tease out parameters in a large graph schema. This is a combination of tedium and domain discovery (it's not my graph and I'm not a domain expert). In the first day, Claude uncovered attributes and relations no other LLM or Google search uncovered. And it worked!! The next day, I allowed it to continue. After a bit, results didn't pass the sniff test. I checked into details of Claude's thinking: it decided to start making up schema attributes and inventing fallback queries on error with more made up attributes. It was ""conscious"" of its decision to do so. By the time I caught this, Claude had polluted quite a bit of code. Sure, plenty of well placed git commits helped in rolling back code...but it's not quite that simple..over the many git commits were sprinkled plenty of learnings I don't want to toss. It took another two days of carefully going through the code to pull out the good stuff and then roll things back. So now I'm at day five of this adventure with cleaned up code and notes on what we learned. I suspect continual improvements on tooling will help. Until then, it's a short leash. I use AI every day, basically as a ""pair coder."" I used it about 15 minutes ago, to help me diagnose a UI issue I was having. It gave me an answer that I would have figured out, in about 30 minutes, in about 30 seconds. My coding style (large files, with multiple classes, well-documented) works well for AI. I can literally dump the entire file into the prompt, and it can scan it in milliseconds. I also use it to help me learn about new stuff, and the ""proper"" way to do things. Basically, what I used to use StackOverflow for, but without the sneering, and much faster turnaround. I'm not afraid to ask ""stupid"" questions -That is critical . Like SO, I have to take what it gives me, with a grain of salt. It's usually too verbose, and doesn't always match my style, so I end up doing a lot of refactoring. It can also give rather ""naive"" answers, that I can refine. The important thing, is that I usually get something that works, so I can walk it back, and figure out a better way. I also won't add code to my project, that I don't understand, and the refactoring helps me, there. I have found the best help comes from ChatGPT. I heard that Claude was supposed to be better, but I haven't seen that. I don't use agents. I've not really ever found automated pipelines to be useful, in my case, and that's sort of what agents would do for me. I may change my mind on that, as I learn more. The reaction to this article is interesting. I have found AI to be useful in software contexts that most people never exercise or expect based on their intuitions of what an LLM can do. For me, a highly productive but boring use of LLMs for code is that they excel at providing midwit “best practice” solutions to common problems. They are better documentation than the documentation and can do a lot of leg work e.g. Linux syscall implementation details. My application domains tend to require more sophisticated solutions than an LLM can provide but they still save a lot of rote effort. A lot of software development exists almost entirely in the midwit zone. Much more interesting, they are decent at reducing concepts in literature to code practice for which there are no code examples. Google and StackOverflow turn up nothing. For example, I’ve found them useful for generating specialized implementations of non-Euclidean computational geometry algorithms that don’t really exist in the wild that I’ve ever seen. This is a big win, it literally turns months of effort into hours of effort. On the other hand, I do a lot of work with algorithms that don’t exist in literature, never mind public code, with extremely performance-engineered implementations. There is an important take away from this too: LLMs are hilariously bad at helping with this but so are human software developers if required to do the same thing with no context. Knowledge for which there is little or no training data is currently a formidable moat, both for LLMs and humans. It's fascinating how over the past year we have had almost daily posts like this one, yet from the outside everything looks exactly the same, isn't that very weird? Why haven't we seen an explosion of new start-ups, products or features? Why do we still see hundreds of bug tickets on every issue tracking page? Have you noticed anything different on any changelog? I invite tptacek, or any other chatbot enthusiast around, to publish project metrics and show some actual numbers. I’m AI neutral but the writing style here is pretty dismissive, and - to match the tone of the article - annoying as fuck. Most completely reasonable objections to LLMs were totally dismissed. My main concern is not even mentioned in this article and there are hardly any comments here addressing it: Privacy / allowing 3rd parties to read and potentially train on your proprietary source code. I've used LLMs to crank out code for tedious things (like generating C-APIs and calling into poorly documented libraries) but I'm not letting them touch my code until I can run it 100% locally offline. Would love to use the agentic stuff but from what I've heard it's still too slow to run on a high end workstation with a single 4080. Or have things got better lately, and crucially is there good VisualStudio integration for running local agents / LLMs? I love LLMs, and I really like programming with Cursor, but I never managed to get the ""agents with tons of stuff in their context"" mode to work for me. I use Cursor like a glorified code completer, 4-5 lines at a time, because otherwise the LLM just makes too many mistakes that compound. If you let it run in the ""write my code for me"" mode, and ask it to fix some mistake it made, it will always add more code, never remove any. In my experience, in the end the code just ends up so brittle that the LLM will soon get stuck at a point that it never manages to overcome some mistake, no matter how many times it tries. Has anyone managed to solve this? > Does an intern cost $20/month? Because that’s what Cursor.ai costs. > Part of being a senior developer is making less-able coders productive, be they fleshly or algebraic. Using agents well is both a both a skill and an engineering project all its own, of prompts, indices, and (especially) tooling. LLMs only produce shitty code if you let them. A junior developer often has negative value to a team, because they're sapping the time of more senior developers who have to help train them, review code, fix mistakes, etc. It can take a long while to break even. The raw cost of Cursor's subscription is surely dwarfed by your own efforts, given that description. The actual calculous here should be the cost to corral Cursor, against the value of the code it generated. Hundreds of comments. Some say LLMs are the future. Others say they don't work today and they won't work tomorrow. Videogame speed running has this problem solved. Livestream your 10x engineer LLM usage, a git commit annotated with it's prompt per change. Then everyone will see the result. This doesn't seem like an area of debate. No complicated diagrams required. Just run the experiment and show the result. I have one very specific retort to the 'you are still responsible' point. High school kids write lots of notes. The notes frequently never get read, but the performance is worse without them: the act of writing them embeds them into your head. I allegedly know how to use a debugger, but I haven't in years: but for a number I could count on my fingers, nearly every bug report I have gotten I know exactly down to the line of code where it comes from, because I wrote it or something next to it (or can immediately ask someone who probably did). You don't get that with AI. The codebase is always new. Everything must be investigated carefully. When stuff slips through code review, even if it is a mistake you might have made, you would remember that you made it. When humans do not do the work, humans do not accrue the experience. (This may still be a good tradeoff, I haven't run any numbers. But it's not such an obvious tradeoff as TFA implies.) It's been so much more rewarding playing with AI coding tools on my own than through the subtle and not so subtle nudges at work. The work AI tools are a walled garden, have a shitty interface, feel made to extract from me than to help me. In my personal stuff, downloading models, playing with them, the tooling, the interactions, it all been so much more rewarding to give me stable comfortable workflows I can rely on and that work with my brain. The dialog around it is so adversarial it's been hard figuring out how to proceed until dedicating a lot of effort to diving into the field myself, alone, on my personal time and learned what's comfortable to use it on. Title nitpick: The amount is people who care about AI for coding assistance is a relative minority. For everyone else, there's 'AI', which has a huge branding problem. 'AI' is filling all search results with trash, and creating trash websites full of trash to fill up the rest of the search results. It's generating trash images to put at the top of every blog post. It's eating up all the server load with scraping. It's what's been fooling my dad every day on Facebook. When people are sick of AI, this is what they are talking about. AI hype people ignore this perspective each and every time. It doesn't matter how great your paper Mill's paper is, if you're dumping PCBs in the river, people are going to quite rightly get pissed off. I agree with the main take in this article: the combination of agents + LLMs with large context windows + a large budget of tokens to iterate on problems can probably already yield some impressive results. I take serious issue with the ""but you have no idea what the code is"" rebuttal, since it - to me - skims over the single largest issue with applying LLMs anywhere where important decisions will be made based on their outputs. To quote from the article: People complain about LLM-generated code being “probabilistic”. No it isn’t. It’s code. It’s not Yacc output. It’s knowable. The LLM might be stochastic. But the LLM doesn’t matter. What matters is whether you can make sense of the result, and whether your guardrails hold. Reading other people’s code is part of the job. If you can’t metabolize the boring, repetitive code an LLM generates: skills issue! How are you handling the chaos human developers turn out on a deadline? The problem here is that LLMs are optimized to make their outputs convincing . The issue is exactly ""whether you can make sense of the result"", as the author said, or, in other words: whether you're immune to being conned by a model output that sounds correct but is not . Sure, ""reading other people’s code is part of the job"", but the failure modes of junior engineers are easily detectable. The failure modes of LLMs are not. EDIT: formatting The author defends mediocore code, yet wrote this piece: https://fly.io/blog/vscode-ssh-wtf/ Where he dunks on how SSH access works in VSCode. I don't know. The code and architecture behind this feature may well be bananas, but gets the work done. Sounds like a clear case of mediocority. I wonder how does he reconcile those two articles together. For me this is more of a clickbait. Both of the articles. With that in mind, if I am nuts for being sceptical of LLMs, I think it is fair to call the author a clickbaiter. The argument that I've heard against LLMs for code is that they create bugs that, by design, are very difficult to spot. The LLM has one job, to make code that looks plausible. That's it. There's no logic gone into writing that bit of code. So the bugs often won't be like those a programmer makes. Instead, they can introduce a whole new class of bug that's way harder to debug. The argument seems to be that for an expert programmer, who is capable of reading and understanding AI agent code output and merging it into a codebase, AI agents are great. Question: If everyone uses AI to code, how does someone become an expert capable of carefully reading and understanding code and acting as an editor to an AI? The expert skills needed to be an editor -- reading code, understanding its implications, knowing what approaches are likely to cause problems, recognizing patterns that can be refactored, knowing where likely problems lie and how to test them, holding a complex codebase in memory and knowing where to find things -- currently come from long experience writing code. But a novice who outsources their thinking to an LLM or an agent (or both) will never develop those skills on their own. So where will the experts come from? I think of this because of my job as a professor; many of the homework assignments we use to develop thinking skills are now obsolete because LLMs can do them, permitting the students to pass without thinking. Perhaps there is another way to develop the skills, but I don't know what it is, and in the mean time I'm not sure how novices will learn to become experts. There's something that seems to be missing in all these posts and that aligns with my personal experience trying to use AI coding assistants. I think in code. To me, having to translate the into natural language for the LLM to translate it back into code makes very little sense. Am I alone in this camp? What am I missing? Curious how he reconciles this: > If you build something with an LLM that people will depend on, read the code. In fact, you’ll probably do more than that. You’ll spend 5-10 minutes knocking it back into your own style. with Joel Spolsky's fundamental maxim: > It’s harder to read code than to write it. https://www.joelonsoftware.com/2000/04/06/things-you-should-... To quote an excellent article from last week: > The AI has suggested a solution, but the added code is arguably useless or wrong. There is a huge decision space to consider, but the AI tool has picked one set of decisions, without any rationale for this decision. > [...] > Programming is about lots of decisions, large and small. Architecture decisions. Data validation decisions. Button color decisions. > Some decisions are inconsequential and can be safely outsourced. There is indeed a ton of boilerplate involved in software development, and writing boilerplate-heavy code involves near zero decisions. > But other decisions do matter. (from https://lukasatkinson.de/2025/net-negative-cursor/ ) Proponents of AI coding often talk about boilerplate as if that's what we spend most of our time on, but boilerplate is a cinch. You copy/paste, change a few fields, and maybe run a macro on it. Or you abstract it away entirely. As for the ""agent"" thing, typing git fetch, git commit, git rebase takes up even less of my time than boilerplate. Most of what we write is not highly creative, but it is load-bearing, and it's full of choices. Most of our time is spent making those choices, not typing out the words. The problem isn't hallucination, it's the plain bad code that I'm going to have to rewrite. Why not just write it right myself the first time? People say ""it's like a junior developer,"" but do they have any idea how much time I've spent trying to coax junior developers into doing things the right way rather than just doing them myself? I don't want to waste time mentoring my tools. One of the biggest anti LLM arguments for me at the moments is about security. In case you don't know, if you open a file with copilot active or cursor, containing secrets, it might be sent to a server a thus get leaked. The companies say that if that file is in a cursorignore file, it won't be indexed, but it's still a critical security issue IMO. We all know what happened with the ""smart home assistants"" like Alexa. Sure, there might be a way to change your workflow and never ever open a secret file with those editors, but my point is that a software that sends your data without your consent, and without giving you the tools to audit it, is a no go for many companies, including mine. The problem with LLMs for code is that they are still way too slow and expensive to be generally practical for non-trivial software projects. I'm not saying that they aren't useful, they are excellent at filling out narrow code units that don't require a lot of context and can be quickly or automatically verified to be correct. You will save a lot of time using them this way. On the other hand, if you slip up and give it too much to chew on or just roll bad RNG, it will spin itself into a loop attempting many variations of crap, erasing and trying over, but never actually coming closer to a correct solution, eventually repeating obviously incorrect solutions over and over again that should have been precluded based on feedback from the previous failed solutions. If you're using a SOTA model, you can easily rack up $5 or more on a single task if you give it more than 30 minutes of leeway to work it out. Sure, you could use a cheaper model, but all that does is make the fundamental problem worse - i.e. you're spending money but not actually getting any closer to completed work. Yes, the models are getting smarter and more efficient, but we're still at least a decade away from being able to run useful models at practical speeds locally. Aggressively quantized 70b models simply can't cut it, and even then, you need something like 10k tps to start building LLM tools that can overcome the LLM's lack of reasoning skills through brute force guess and check techniques. Perhaps some of the AI skeptics are a bit too harsh, but they're certainly not crazy in the context of breathless hype. The privacy aspect and other security risks tho? So far all the praise I hear on productivity are from people using cloud-hosted models. Claude, Gemini, Copilot and and ChatGPT are non-starters for privacy-minded folks. So far, local experiements with agents have left me underwhelmed. Tried everything on ollama that can run on my dedicated Ryzen 8700G with 96GB DDR5. I'm ready to blow ~10-15k USD on a better rig if I see value in it but if I extrapolate current results I believe it'll be another CPU generation before I can expect positive productivity output from properly securely running local models when factoring in the setup and meta. The reason that I personally don't use LLMs was not addressed by the article: I haven't found a way to use it that makes me develop faster. The articles talks about ""tedious code."" If you need to generate a large static value table or something, then OK an LLM might give you a really fast result and cut through the tedium. Most of us were already writing short scripts to do that. I'm open to the possibility that an LLM can do it faster. But it's such a rare requirement that the productivity gains are truly negligible here even if they can. And in those cases, it's obvious what the repetitive task needs to be. I often find myself writing the code by hand to be quicker than coming up with a prompt to get it to write the code that I then need to review for correctness. The article then mentions scaffolding. Things like ""bookkeeping"" when it comes to creating and setting up a new repo (whatever he means by that). This is why I have, historically, been a big fan of frameworks and generators. Point being, this is already a solved problem and I haven't found a way to further improve the state of this world with LLMs. LLMs might be an alternate tool that work just as well. But they haven't made my existing daily workflow any faster. Setting up new repos is also something that is done so rarely that even if an LLM netted a 100% increase in efficiency, it wouldn't really impact much. I am an AI ""skeptic"" but I'm not a naysayer. I do use LLMs regularly. I just don't use them for developing code because I have yet to find a problem that they solve for me. Don't get me wrong, there are problems that they can solve... I just haven't come across any solutions to previously-unsolved problems. Meaning I can swap an existing solution for an LLM-based one... and it is a valid solution... but I don't observe any increase in productivity from doing so. The existing solution was already working fine. I am genuinely looking forward to the day when this changes. When I identify a single existing problem without an existing solution that LLMs solve for me when developing software. I just have yet to come across one. This happened with the introduction of smartphones too. Every slashdot post had a haughty and upvoted ‘why would i want such a thing!!!’. It was obviously huge. You could see it taking off. Yet a lot of people proudly displayed ignorance and backed each other up on it to the point that discussion around the topic was often drowned out by the opposition to change. Now today it takes minutes of playing with ai coding agents to realise that it’s extremely useful and going to be similarly huge. Resistance to change is not a virtue! My 5 cents would be that LLMs have replaced all those random (e.g. CSS, regex etc) generators, emmet-like IDE code completion/generator tools, as well as having to google for arbitrary code snippets which you'd just copy and paste in. In no way can AI be used for anything larger than generating singular functions or anything that would require writing to or modifying multiple files. Technically you might be able to pull off having AI change multiple files for you in one go, but you'll quickly run into sort of ""Adobe Dreamviewer"" type of issue where your codebase is dominated by generated code which only the AI that generated it is able to properly extend and modify. I remember when Dreamviewer was a thing, but you essentialyl had to make a choice between sticking with it forever for the project or not using it at all, because it would basically convert your source code into it's own proprietary format due to it becoming so horribly messy and unreadable. Regardless, AI is absolutely incredible and speeds up development by a great deal, (even) if you only use it to generate small snippets at the time. AI is also an absolute godsend for formatting and converting stuff from anything and to anything - you could e.g. dump your whole database structure to Gemini and ask it to generate an API against it; big task, but since it is basically just a conversion task, it will work very well. Here are two routine problems I have to solve at the moment. Can any of the current LLM systems do either? 1. Input is an 256x256 pixel elevation map stored as a greyscale .png file, and a minimum and maximum elevation. A pixel value of 0 corresponds to the minimum elevation, and a pixel value of 255 corresponds to the maximum elevation. Read in the .png file and the elevation limits. Then construct a 256x256 floating point array of heights. From that array, construct a triangle mesh with X and Y dimensions 0..255. Perform a mesh reduction operation on the triangle mesh to reduce the number of triangles. Mesh reduction must not generate holes in the mesh. From the reduced mesh, generate a glTF file where the UV parameters run from 0.0 to 1.0 along the X and Y axes. 2. Given four glTF files constructed as above, corresponding to four quadrants of a larger square, construct a single 511x511 mesh which combines all four input meshes to cover a larger area. Because the input meshes are 0..255, not 0..256, there will be gaps where the four quadrants meet. Fill those gaps with reasonable triangles. Perform a mesh reduction as above. From the reduced mesh, generate a glTF file where the UV parameters run from 0.0 to 1.0 along the X and Y axes. Rust code is preferred; Python code is acceptable. So, what service should I sign up for? Weird to claim the llm does all the boring learning and boilerplate for you as a selling point, but then also insist we still need to responsibly read all the output, and if you can't understand it's a ""skill issue"". Also the emphasis on greenfield projects? Starting is by FAR the easiest part. That's not impressive to me. When do we get to code greenfield for important systems? Reminds me of the equally absurd example of language choice. You think you get to choose? What? Imagine all the code these agents are going to pump out that can never be reviewed in a reasonable time frame. The noise generated at the whim of bike-shedding vibe coders is going to drown all the senior reviewers soon enough. I'll call that Cowboy Coders on Steroids. Anyone with skills will be buried in reviews, won't have time for anything else, and I predict stricter code gen policies to compensate. > An LLM can be instructed to just figure all that shit out. Often, it will drop you precisely at that golden moment where shit almost works, and development means tweaking code and immediately seeing things work better. Well, except that in order to fix that 1% you'd need to read and understand whatever the LLM did and then look for that 1%. I get the shills just thinking about this, whether the original programmer was human or not. I'd rather just write everything myself to begin with. The one main claim the article makes: Senior developers should not ignore the productivity gains from LLMs. Best use of evidence is deductive: Lots of code is tedious and uninteresting -> LLMs are fast at generating lots of tedious code -> LLMs help productivity. Weakest part of the argument: The list of rebuttals doesn't have an obvious organization to it. What exactly is the main argument they're arguing against? It's not stated outright but because the post is bookended by references to 'those smarter than me', I think this is an argument against the shaming of developers using (and loving) LLM tools. Which I think is fair. Overall, the post did not add anything to the general discussion. But the popularity of the author (and fly.io posts) may make it a beacon for some. A couple of thoughts: On 'just read the code' - all well and good. Of course this implies insisting on team members who can read the code, will read the code and are empowered to read the code. Otherwise orgs will try to hire pure vibe coders who aren't interested in that and only allow time for literally just instructing agents to generate code because it sounds cheaper and execs don't understand the nuance so long as it looks like product is shipping - until it all blows up and the one standing senior developer on hand is supposed to fix a prod issue buried in millions of lines of vibe reviewed code ASAP. On 'but it's cheaper than a junior': cloud hosted LLM systems are currently massively subsidised to an absurd degree. The cost side of things is all smoke and mirrors geared towards accelerated market adoption at all costs. It's not a profitable enterprise at the model development level. At some point that AI economy is going to expect to make that money back, and future (especially near-future) hardware advancements don't explain where all of that is going to come from. If you are resource constrained this article will make you sad. I do not have unlimited funds to plug in some token and burn a bunch of money when writing code. I am gpu poor. I'm lucky that 8gb vram can run the smallest models. But the output is so poor that I lose out to anyone using a hosted service. If anything this article shows that building great programs is less democratized than it once was. I'm working with python and it does not work very well, LLMs usually generates at least an order of magnitude more code than what I would write. That code often uses outdated practices, does poor design choices and does not understand hints like writing code in a way that reduce cognitive load, even when we explain that it means, i.e. keep the number of classes and functions small. It's the complete opposite of OP's main language GO, as he says: > Go has just enough type safety, an extensive standard library, and a culture that prizes (often repetitive) idiom. LLMs kick ass generating it. Python is an interpreted dynamically typed language and the static type checkers are not there yet (most popular 3rd parties libraries have no type hints for example). Also it allows for many different programming styles that the LLMs struggle to choose from. 1. Every extra line of code is much more risky. 2. It's much harder to verify the LLM's code. On the other hand I think rust will be in a good place in regards to LLMs in the next few years thanks to the robustness of the language and the quality of its diagnostic messages. Those 2 attributes should compound very well. Let’s not conflate LLM’s with AI. LLM’s are a kind of AI that can be a software engineer’s assistant, at best. But the degree of hype is undermining belief in AI among many professionals who the hypesters claim are going to be replaced. No, this iteration is not going to replace doctors or engineers. But the degree to which the hype train wants to do so is alarming. > LLMs can write a large fraction of all the tedious code you’ll ever need to write. And most code on most projects is tedious. You still need to read this tedious code to verify that it actually does what you want it to do. Given this, I'd much rather prefer to write the tedious code myself than having to make sense of someone else's tedious code. I think the key premise here is that one can effectively and efficiently audit code that the LLM is producing. I doubt that. First, human attention and speed is very limited. Second, when I see something, I am already predisposed to assume that it is right (or at the very least, my subsequent inquiries are extremely narrow and anchored around the solution I have seen presented to me.) A lot of people who are wary of LLMs aren’t against the technology itself, but rather the narrative surrounding it. You can take advantage of the tool while turning a blind eye to the discourse. This 16-minute, expletive-filled, edgy-old-man-trying-too-hard-to-be-cool article could easily be dismissed as yet another AI creed that somehow found its way to the top of the HN front page. When I made a mid career change to Cobol programming in 1981 my first manager (a suit as programmers referred to them then) pointed to a book on his desk; the title,as I can best recall was Programming Without Programmers. He ""You got in too late,"" he said. I retired from programming in 2010. The hype has a long history. I hope I'm around long enough to see how this plays out. I'm on the side of AI is a huge productivity booster (by my guess... 10x) But I don't want to make the claim lightly, so I did an experiment. I signed up for copilot pro, and have been using their 'edit' feature. This is more than just their auto complete. I set myself a goal to create a playable web game of classic frogger. It took 4 hours with copilot ""edit"" and my full attention. I didn't write a single line of code, but I did ask it to refactor and gave it a project description. I suspect this would have taken me 4 days full time to get to this level. Try it out: https://dev.zebrar.com/jd/frogger/ I will create an account just to point out that the response to > But it is an inherently plagiarist technology Was > Developers frequently engage in copyright infringement, and so will I, so unless if you're a lawyer, shove it up your ass ""I am a bad person so I get to continue being bad"" is not the gotcha you think it is, Patrick. ""Kids today don’t just use agents; they use asynchronous agents. They wake up, free-associate 13 different things for their LLMs to work on, make coffee, fill out a TPS report, drive to the Mars Cheese Castle, and then check their notifications. They’ve got 13 PRs to review. Three get tossed and re-prompted. Five of them get the same feedback a junior dev gets. And five get merged."" I would jump off a bridge before I accepted that as my full-time job. I've been programming for 20+ years and I've never wanted to move into management. I got into programming because I like programming, not because I like asking others to write code on my behalf and review what they come up with. I've been in a lead role, and I certainly do lots of code review and enjoy helping teammates grow. But the last fucking thing I want to do is delegate all the code writing to someone or something else. I like writing code. Yes, sometimes writing code is tedious, or frustrating. Sometimes it's yak-shaving. Sometimes it's Googling. Very often, it's debugging. I'm happy to have AI help me with some of that drudgery, but if I ever get to the point that I feel like I spend my entire day in virtual meetings with AI agents, then I'm changing careers. I get up in the morning to make things, not to watch others make things. Maybe the kind of software engineering role I love is going to disappear, like stevedores and lamplighters. I will miss it dearly, but at least I guess I got a couple of good decades out of it. If this is what the job turns into, I'll have to find something else to do with my remaining years. There are many aspects to AI push back. - all creatives are flat against it because it’s destroying their income streams and outright stealing their intellectual property - some technical leaders are skeptical because early returns were very bad and they have not updated their investigations to the latest tools and models, which are already significantly ahead of even six months ago - a tech concern is how do we mentor new developers if they don’t know how to code or develop logic. LLMs are great IF you already know what you’re doing - talent is deeply concerned that they will be reduced and replaced, going from high paying careers to fast food salaries We have a lot of work to balance productivity with the benefits to society. “Let them eat cake,” is not going to work this time either. Seeing everything these days being about vibe coding, I feel a little old with my VIM setup and my LSP servers who I already thought were a nice productivity increase. The problems I have with the stuff relating to MCP is that the tech around it is developing so fast that it's hard for outsiders to catch up with what the best working setup is, for example. What would you do, for example, if you want to selfhost this? - which models (qwen ai coder?) - which api (with ollama? Bolt? Aider? Etc) - how to integrate PRs with a local gitlab/gogs/forgejo instance? Do you need another MCP agent for git that does that? - which hardware dependencies to run it? I am currently trying to figure out how to implement a practical workflow for this. So far I'm using still a synchronous MCP agent setup where it basically runs on another machine in the network because I have a too unperformant laptop to work with. But how would I get to the point of async MCP agents that can work on multiple things in my Go codebases in parallel? With the mentioned PR workflows so that I can modify/edit/rework before the merges? The author makes a lot of claims and talks always about that their opponents in the argument are not talking about the same thing. But what exactly is the same thing, which is reproducible locally for everyone? Machine translation and speech recognition. The state of the art for these is a multi-modal language model. I'm hearing impaired veering on deaf, and I use this technology all day every day. I wanted to watch an old TV series from the 1980s. There are no subtitles available. So I fed the show into a language model (Whisper) and now I have passable subtitles that allow me to watch the show. Am I the only one who remembers when that was the stuff of science fiction? It was not so long ago an open question if machines would ever be able to transcribe speech in a useful way. How quickly we become numb to the magic. Can someone comment on the cost of running agentic models? Not for a company but for an individual. I tried ""vibe coding"" a personal project I was struggling with and left even more frustrated because I kept running into token rate limits with Claude (used inside of Zed if it matters). Did I pick the wrong model, the wrong editor, or do I just need to not be so tight with my money? > This was the craftsman’s ‘Golden Age’ and much time and trouble was taken over the design of tools. Craftsmen were being called upon to do more skilful and exacting work and the use of tools and the interest in development had become very widespread. Above pulled from A Brief History of the Woodworking Plane [0]. A woodworking tool that has evolved over 2,000 years. Now there are electric planers, handheld electric planers and lots of heavy machinery that do the same thing in a very automated way. If a company is mass producing kitchen cabinets, they aren't hand planing edges on boards, a machine is doing all that work. I feel like with AI we are on the cusp of moving beyond a ""Golden age"" and into an ""industrial age"" for coding, where it will become more important to have code that AI understands vs. something that is carefully crafted. Simple business pressure will demand it (whether we like it or not). ^ A comment I made just yesterday on a different thread. For software developers AI is like the cabinet maker that gets a machine to properly mill and produce cabinet panels, sure you can use a hand plane to do that but you're producing a very different product and likely one that not many people will care about, possibly not even your employer when they see all the other wood shops pumping out cabinetry and taking their market share. [0] https://www.handplane.com/879/a-brief-history-of-the-woodwor... Everything about that is true but, and that's a big BUT, the code I write with LLM I can only iterate on it with an LLM. My mind doesn't develop a mental model of that code, I don't know where the relevant parts are, I can't quickly navigate through it and I have to reach the LLM for every small change. Which is why I like Copilot style editing more than agents as a working model but agents are just so much more powerful and smarter thanks to everything available to them. I tend to agree with the gist of this, namely that the change is here and ""AI"" presents some huge potential to save me from having to do the tedious stuff. But we do need a bit of a sanity check. I'm in the middle of trying to convince any of the AIs that I have access to to write me some simple Lua. Specifically, I'm trying to write a query that returns a certain subset of pages from the Silverbullet V2 note tool. This isn't particularly difficult, but it's become this hilarious journey to try to get the AI to figure this problem out. Silverbullet is a niche use case, v2 even more so, but a reasonably skilled developer could peruse the API documentation and come up with a pretty good starting point. AI tools? Absolutely insane wrong answers. I finally specifically asked one or two of them not to guess with their answers and they just straight up said ""nah, we don't know how to do this."" Point being: there's some real power in these tools, but if the ground is not already well-trodden, they risk creating a huge time sink that could be better served just learning to do the thing yourself.","My AI skeptic friends are all nuts nan I think this article is pretty spot on — it articulates something I’ve come to appreciate about LLM-assisted coding over the past few months. I started out very sceptical. When Claude Code landed, I got completely seduced — borderline addicted, slot machine-style — by what initially felt like a superpower. Then I actually read the code. It was shockingly bad. I swung back hard to my earlier scepticism, probably even more entrenched than before. Then something shifted. I started experimenting. I stopped giving it orders and began using it more like a virtual rubber duck. That made a huge difference. It’s still absolute rubbish if you just let it run wild, which is why I think “vibe coding” is basically just “vibe debt” — because it just doesn’t do what most (possibly uninformed) people think it does. But if you treat it as a collaborator — more like an idiot savant with a massive brain but no instinct or nous — or better yet, as a mech suit [0] that needs firm control — then something interesting happens. I’m now at a point where working with Claude Code is not just productive, it actually produces pretty good code, with the right guidance. I’ve got tests, lots of them. I’ve also developed a way of getting Claude to document intent as we go, which helps me, any future human reader, and, crucially, the model itself when revisiting old code. What fascinates me is how negative these comments are — how many people seem closed off to the possibility that this could be a net positive for software engineers rather than some kind of doomsday. Did Photoshop kill graphic artists? Did film kill theatre? Not really. Things changed, sure. Was it “better”? There’s no counterfactual, so who knows? But change was inevitable. What’s clear is this tech is here now, and complaining about it feels a bit like mourning the loss of punch cards when terminals showed up. [0]: https://matthewsinclair.com/blog/0178-why-llm-powered-progra... This article does not touch on the thing which worries me the most with respect to LLMs: the dependence . Unless you can run the LLM locally, on a computer you own, you are now completely dependent on a remote centralized system to do your work. Whoever controls that system can arbitrarily raise the prices, subtly manipulate the outputs, store and do anything they want with the inputs, or even suddenly cease to operate. And since, according to this article, only the latest and greatest LLM is acceptable (and I've seen that exact same argument six months ago), running locally is not viable (I've seen, in a recent discussion, someone mention a home server with something like 384G of RAM just to run one LLM locally). To those of us who like Free Software because of the freedom it gives us, this is a severe regression. One thing that I find truly amazing is just the simple fact that you can now be fuzzy with the input you give a computer, and get something meaningful in return. Like, as someone who grew up learning to code in the 90s it always seemed like science fiction that we'd get to a point where you could give a computer some vague human level instructions and get it more or less do what you want. I'm not a skeptic, but I keep LLMs on a short leash. This is a thoughtful article. Thanks `tptacek My LLM use is: 1 - tedious stuff; web pages interacting with domain back end. 2 - domain discovery. In a recent adventure, I used Claude 4 to tease out parameters in a large graph schema. This is a combination of tedium and domain discovery (it's not my graph and I'm not a domain expert). In the first day, Claude uncovered attributes and relations no other LLM or Google search uncovered. And it worked!! The next day, I allowed it to continue. After a bit, results didn't pass the sniff test. I checked into details of Claude's thinking: it decided to start making up schema attributes and inventing fallback queries on error with more made up attributes. It was ""conscious"" of its decision to do so. By the time I caught this, Claude had polluted quite a bit of code. Sure, plenty of well placed git commits helped in rolling back code...but it's not quite that simple..over the many git commits were sprinkled plenty of learnings I don't want to toss. It took another two days of carefully going through the code to pull out the good stuff and then roll things back. So now I'm at day five of this adventure with cleaned up code and notes on what we learned. I suspect continual improvements on tooling will help. Until then, it's a short leash. I use AI every day, basically as a ""pair coder."" I used it about 15 minutes ago, to help me diagnose a UI issue I was having. It gave me an answer that I would have figured out, in about 30 minutes, in about 30 seconds. My coding style (large files, with multiple classes, well-documented) works well for AI. I can literally dump the entire file into the prompt, and it can scan it in milliseconds. I also use it to help me learn about new stuff, and the ""proper"" way to do things. Basically, what I used to use StackOverflow for, but without the sneering, and much faster turnaround. I'm not afraid to ask ""stupid"" questions -That is critical . Like SO, I have to take what it gives me, with a grain of salt. It's usually too verbose, and doesn't always match my style, so I end up doing a lot of refactoring. It can also give rather ""naive"" answers, that I can refine. The important thing, is that I usually get something that works, so I can walk it back, and figure out a better way. I also won't add code to my project, that I don't understand, and the refactoring helps me, there. I have found the best help comes from ChatGPT. I heard that Claude was supposed to be better, but I haven't seen that. I don't use agents. I've not really ever found automated pipelines to be useful, in my case, and that's sort of what agents would do for me. I may change my mind on that, as I learn more. The reaction to this article is interesting. I have found AI to be useful in software contexts that most people never exercise or expect based on their intuitions of what an LLM can do. For me, a highly productive but boring use of LLMs for code is that they excel at providing midwit “best practice” solutions to common problems. They are better documentation than the documentation and can do a lot of leg work e.g. Linux syscall implementation details. My application domains tend to require more sophisticated solutions than an LLM can provide but they still save a lot of rote effort. A lot of software development exists almost entirely in the midwit zone. Much more interesting, they are decent at reducing concepts in literature to code practice for which there are no code examples. Google and StackOverflow turn up nothing. For example, I’ve found them useful for generating specialized implementations of non-Euclidean computational geometry algorithms that don’t really exist in the wild that I’ve ever seen. This is a big win, it literally turns months of effort into hours of effort. On the other hand, I do a lot of work with algorithms that don’t exist in literature, never mind public code, with extremely performance-engineered implementations. There is an important take away from this too: LLMs are hilariously bad at helping with this but so are human software developers if required to do the same thing with no context. Knowledge for which there is little or no training data is currently a formidable moat, both for LLMs and humans. It's fascinating how over the past year we have had almost daily posts like this one, yet from the outside everything looks exactly the same, isn't that very weird? Why haven't we seen an explosion of new start-ups, products or features? Why do we still see hundreds of bug tickets on every issue tracking page? Have you noticed anything different on any changelog? I invite tptacek, or any other chatbot enthusiast around, to publish project metrics and show some actual numbers. I’m AI neutral but the writing style here is pretty dismissive, and - to match the tone of the article - annoying as fuck. Most completely reasonable objections to LLMs were totally dismissed. My main concern is not even mentioned in this article and there are hardly any comments here addressing it: Privacy / allowing 3rd parties to read and potentially train on your proprietary source code. I've used LLMs to crank out code for tedious things (like generating C-APIs and calling into poorly documented libraries) but I'm not letting them touch my code until I can run it 100% locally offline. Would love to use the agentic stuff but from what I've heard it's still too slow to run on a high end workstation with a single 4080. Or have things got better lately, and crucially is there good VisualStudio integration for running local agents / LLMs? I love LLMs, and I really like programming with Cursor, but I never managed to get the ""agents with tons of stuff in their context"" mode to work for me. I use Cursor like a glorified code completer, 4-5 lines at a time, because otherwise the LLM just makes too many mistakes that compound. If you let it run in the ""write my code for me"" mode, and ask it to fix some mistake it made, it will always add more code, never remove any. In my experience, in the end the code just ends up so brittle that the LLM will soon get stuck at a point that it never manages to overcome some mistake, no matter how many times it tries. Has anyone managed to solve this? > Does an intern cost $20/month? Because that’s what Cursor.ai costs. > Part of being a senior developer is making less-able coders productive, be they fleshly or algebraic. Using agents well is both a both a skill and an engineering project all its own, of prompts, indices, and (especially) tooling. LLMs only produce shitty code if you let them. A junior developer often has negative value to a team, because they're sapping the time of more senior developers who have to help train them, review code, fix mistakes, etc. It can take a long while to break even. The raw cost of Cursor's subscription is surely dwarfed by your own efforts, given that description. The actual calculous here should be the cost to corral Cursor, against the value of the code it generated. Hundreds of comments. Some say LLMs are the future. Others say they don't work today and they won't work tomorrow. Videogame speed running has this problem solved. Livestream your 10x engineer LLM usage, a git commit annotated with it's prompt per change. Then everyone will see the result. This doesn't seem like an area of debate. No complicated diagrams required. Just run the experiment and show the result. I have one very specific retort to the 'you are still responsible' point. High school kids write lots of notes. The notes frequently never get read, but the performance is worse without them: the act of writing them embeds them into your head. I allegedly know how to use a debugger, but I haven't in years: but for a number I could count on my fingers, nearly every bug report I have gotten I know exactly down to the line of code where it comes from, because I wrote it or something next to it (or can immediately ask someone who probably did). You don't get that with AI. The codebase is always new. Everything must be investigated carefully. When stuff slips through code review, even if it is a mistake you might have made, you would remember that you made it. When humans do not do the work, humans do not accrue the experience. (This may still be a good tradeoff, I haven't run any numbers. But it's not such an obvious tradeoff as TFA implies.) It's been so much more rewarding playing with AI coding tools on my own than through the subtle and not so subtle nudges at work. The work AI tools are a walled garden, have a shitty interface, feel made to extract from me than to help me. In my personal stuff, downloading models, playing with them, the tooling, the interactions, it all been so much more rewarding to give me stable comfortable workflows I can rely on and that work with my brain. The dialog around it is so adversarial it's been hard figuring out how to proceed until dedicating a lot of effort to diving into the field myself, alone, on my personal time and learned what's comfortable to use it on. Title nitpick: The amount is people who care about AI for coding assistance is a relative minority. For everyone else, there's 'AI', which has a huge branding problem. 'AI' is filling all search results with trash, and creating trash websites full of trash to fill up the rest of the search results. It's generating trash images to put at the top of every blog post. It's eating up all the server load with scraping. It's what's been fooling my dad every day on Facebook. When people are sick of AI, this is what they are talking about. AI hype people ignore this perspective each and every time. It doesn't matter how great your paper Mill's paper is, if you're dumping PCBs in the river, people are going to quite rightly get pissed off. I agree with the main take in this article: the combination of agents + LLMs with large context windows + a large budget of tokens to iterate on problems can probably already yield some impressive results. I take serious issue with the ""but you have no idea what the code is"" rebuttal, since it - to me - skims over the single largest issue with applying LLMs anywhere where important decisions will be made based on their outputs. To quote from the article: People complain about LLM-generated code being “probabilistic”. No it isn’t. It’s code. It’s not Yacc output. It’s knowable. The LLM might be stochastic. But the LLM doesn’t matter. What matters is whether you can make sense of the result, and whether your guardrails hold. Reading other people’s code is part of the job. If you can’t metabolize the boring, repetitive code an LLM generates: skills issue! How are you handling the chaos human developers turn out on a deadline? The problem here is that LLMs are optimized to make their outputs convincing . The issue is exactly ""whether you can make sense of the result"", as the author said, or, in other words: whether you're immune to being conned by a model output that sounds correct but is not . Sure, ""reading other people’s code is part of the job"", but the failure modes of junior engineers are easily detectable. The failure modes of LLMs are not. EDIT: formatting The author defends mediocore code, yet wrote this piece: https://fly.io/blog/vscode-ssh-wtf/ Where he dunks on how SSH access works in VSCode. I don't know. The code and architecture behind this feature may well be bananas, but gets the work done. Sounds like a clear case of mediocority. I wonder how does he reconcile those two articles together. For me this is more of a clickbait. Both of the articles. With that in mind, if I am nuts for being sceptical of LLMs, I think it is fair to call the author a clickbaiter. The argument that I've heard against LLMs for code is that they create bugs that, by design, are very difficult to spot. The LLM has one job, to make code that looks plausible. That's it. There's no logic gone into writing that bit of code. So the bugs often won't be like those a programmer makes. Instead, they can introduce a whole new class of bug that's way harder to debug. The argument seems to be that for an expert programmer, who is capable of reading and understanding AI agent code output and merging it into a codebase, AI agents are great. Question: If everyone uses AI to code, how does someone become an expert capable of carefully reading and understanding code and acting as an editor to an AI? The expert skills needed to be an editor -- reading code, understanding its implications, knowing what approaches are likely to cause problems, recognizing patterns that can be refactored, knowing where likely problems lie and how to test them, holding a complex codebase in memory and knowing where to find things -- currently come from long experience writing code. But a novice who outsources their thinking to an LLM or an agent (or both) will never develop those skills on their own. So where will the experts come from? I think of this because of my job as a professor; many of the homework assignments we use to develop thinking skills are now obsolete because LLMs can do them, permitting the students to pass without thinking. Perhaps there is another way to develop the skills, but I don't know what it is, and in the mean time I'm not sure how novices will learn to become experts. There's something that seems to be missing in all these posts and that aligns with my personal experience trying to use AI coding assistants. I think in code. To me, having to translate the into natural language for the LLM to translate it back into code makes very little sense. Am I alone in this camp? What am I missing? Curious how he reconciles this: > If you build something with an LLM that people will depend on, read the code. In fact, you’ll probably do more than that. You’ll spend 5-10 minutes knocking it back into your own style. with Joel Spolsky's fundamental maxim: > It’s harder to read code than to write it. https://www.joelonsoftware.com/2000/04/06/things-you-should-... To quote an excellent article from last week: > The AI has suggested a solution, but the added code is arguably useless or wrong. There is a huge decision space to consider, but the AI tool has picked one set of decisions, without any rationale for this decision. > [...] > Programming is about lots of decisions, large and small. Architecture decisions. Data validation decisions. Button color decisions. > Some decisions are inconsequential and can be safely outsourced. There is indeed a ton of boilerplate involved in software development, and writing boilerplate-heavy code involves near zero decisions. > But other decisions do matter. (from https://lukasatkinson.de/2025/net-negative-cursor/ ) Proponents of AI coding often talk about boilerplate as if that's what we spend most of our time on, but boilerplate is a cinch. You copy/paste, change a few fields, and maybe run a macro on it. Or you abstract it away entirely. As for the ""agent"" thing, typing git fetch, git commit, git rebase takes up even less of my time than boilerplate. Most of what we write is not highly creative, but it is load-bearing, and it's full of choices. Most of our time is spent making those choices, not typing out the words. The problem isn't hallucination, it's the plain bad code that I'm going to have to rewrite. Why not just write it right myself the first time? People say ""it's like a junior developer,"" but do they have any idea how much time I've spent trying to coax junior developers into doing things the right way rather than just doing them myself? I don't want to waste time mentoring my tools. One of the biggest anti LLM arguments for me at the moments is about security. In case you don't know, if you open a file with copilot active or cursor, containing secrets, it might be sent to a server a thus get leaked. The companies say that if that file is in a cursorignore file, it won't be indexed, but it's still a critical security issue IMO. We all know what happened with the ""smart home assistants"" like Alexa. Sure, there might be a way to change your workflow and never ever open a secret file with those editors, but my point is that a software that sends your data without your consent, and without giving you the tools to audit it, is a no go for many companies, including mine. The problem with LLMs for code is that they are still way too slow and expensive to be generally practical for non-trivial software projects. I'm not saying that they aren't useful, they are excellent at filling out narrow code units that don't require a lot of context and can be quickly or automatically verified to be correct. You will save a lot of time using them this way. On the other hand, if you slip up and give it too much to chew on or just roll bad RNG, it will spin itself into a loop attempting many variations of crap, erasing and trying over, but never actually coming closer to a correct solution, eventually repeating obviously incorrect solutions over and over again that should have been precluded based on feedback from the previous failed solutions. If you're using a SOTA model, you can easily rack up $5 or more on a single task if you give it more than 30 minutes of leeway to work it out. Sure, you could use a cheaper model, but all that does is make the fundamental problem worse - i.e. you're spending money but not actually getting any closer to completed work. Yes, the models are getting smarter and more efficient, but we're still at least a decade away from being able to run useful models at practical speeds locally. Aggressively quantized 70b models simply can't cut it, and even then, you need something like 10k tps to start building LLM tools that can overcome the LLM's lack of reasoning skills through brute force guess and check techniques. Perhaps some of the AI skeptics are a bit too harsh, but they're certainly not crazy in the context of breathless hype. The privacy aspect and other security risks tho? So far all the praise I hear on productivity are from people using cloud-hosted models. Claude, Gemini, Copilot and and ChatGPT are non-starters for privacy-minded folks. So far, local experiements with agents have left me underwhelmed. Tried everything on ollama that can run on my dedicated Ryzen 8700G with 96GB DDR5. I'm ready to blow ~10-15k USD on a better rig if I see value in it but if I extrapolate current results I believe it'll be another CPU generation before I can expect positive productivity output from properly securely running local models when factoring in the setup and meta. The reason that I personally don't use LLMs was not addressed by the article: I haven't found a way to use it that makes me develop faster. The articles talks about ""tedious code."" If you need to generate a large static value table or something, then OK an LLM might give you a really fast result and cut through the tedium. Most of us were already writing short scripts to do that. I'm open to the possibility that an LLM can do it faster. But it's such a rare requirement that the productivity gains are truly negligible here even if they can. And in those cases, it's obvious what the repetitive task needs to be. I often find myself writing the code by hand to be quicker than coming up with a prompt to get it to write the code that I then need to review for correctness. The article then mentions scaffolding. Things like ""bookkeeping"" when it comes to creating and setting up a new repo (whatever he means by that). This is why I have, historically, been a big fan of frameworks and generators. Point being, this is already a solved problem and I haven't found a way to further improve the state of this world with LLMs. LLMs might be an alternate tool that work just as well. But they haven't made my existing daily workflow any faster. Setting up new repos is also something that is done so rarely that even if an LLM netted a 100% increase in efficiency, it wouldn't really impact much. I am an AI ""skeptic"" but I'm not a naysayer. I do use LLMs regularly. I just don't use them for developing code because I have yet to find a problem that they solve for me. Don't get me wrong, there are problems that they can solve... I just haven't come across any solutions to previously-unsolved problems. Meaning I can swap an existing solution for an LLM-based one... and it is a valid solution... but I don't observe any increase in productivity from doing so. The existing solution was already working fine. I am genuinely looking forward to the day when this changes. When I identify a single existing problem without an existing solution that LLMs solve for me when developing software. I just have yet to come across one. This happened with the introduction of smartphones too. Every slashdot post had a haughty and upvoted ‘why would i want such a thing!!!’. It was obviously huge. You could see it taking off. Yet a lot of people proudly displayed ignorance and backed each other up on it to the point that discussion around the topic was often drowned out by the opposition to change. Now today it takes minutes of playing with ai coding agents to realise that it’s extremely useful and going to be similarly huge. Resistance to change is not a virtue! My 5 cents would be that LLMs have replaced all those random (e.g. CSS, regex etc) generators, emmet-like IDE code completion/generator tools, as well as having to google for arbitrary code snippets which you'd just copy and paste in. In no way can AI be used for anything larger than generating singular functions or anything that would require writing to or modifying multiple files. Technically you might be able to pull off having AI change multiple files for you in one go, but you'll quickly run into sort of ""Adobe Dreamviewer"" type of issue where your codebase is dominated by generated code which only the AI that generated it is able to properly extend and modify. I remember when Dreamviewer was a thing, but you essentialyl had to make a choice between sticking with it forever for the project or not using it at all, because it would basically convert your source code into it's own proprietary format due to it becoming so horribly messy and unreadable. Regardless, AI is absolutely incredible and speeds up development by a great deal, (even) if you only use it to generate small snippets at the time. AI is also an absolute godsend for formatting and converting stuff from anything and to anything - you could e.g. dump your whole database structure to Gemini and ask it to generate an API against it; big task, but since it is basically just a conversion task, it will work very well. Here are two routine problems I have to solve at the moment. Can any of the current LLM systems do either? 1. Input is an 256x256 pixel elevation map stored as a greyscale .png file, and a minimum and maximum elevation. A pixel value of 0 corresponds to the minimum elevation, and a pixel value of 255 corresponds to the maximum elevation. Read in the .png file and the elevation limits. Then construct a 256x256 floating point array of heights. From that array, construct a triangle mesh with X and Y dimensions 0..255. Perform a mesh reduction operation on the triangle mesh to reduce the number of triangles. Mesh reduction must not generate holes in the mesh. From the reduced mesh, generate a glTF file where the UV parameters run from 0.0 to 1.0 along the X and Y axes. 2. Given four glTF files constructed as above, corresponding to four quadrants of a larger square, construct a single 511x511 mesh which combines all four input meshes to cover a larger area. Because the input meshes are 0..255, not 0..256, there will be gaps where the four quadrants meet. Fill those gaps with reasonable triangles. Perform a mesh reduction as above. From the reduced mesh, generate a glTF file where the UV parameters run from 0.0 to 1.0 along the X and Y axes. Rust code is preferred; Python code is acceptable. So, what service should I sign up for? Weird to claim the llm does all the boring learning and boilerplate for you as a selling point, but then also insist we still need to responsibly read all the output, and if you can't understand it's a ""skill issue"". Also the emphasis on greenfield projects? Starting is by FAR the easiest part. That's not impressive to me. When do we get to code greenfield for important systems? Reminds me of the equally absurd example of language choice. You think you get to choose? What? Imagine all the code these agents are going to pump out that can never be reviewed in a reasonable time frame. The noise generated at the whim of bike-shedding vibe coders is going to drown all the senior reviewers soon enough. I'll call that Cowboy Coders on Steroids. Anyone with skills will be buried in reviews, won't have time for anything else, and I predict stricter code gen policies to compensate. > An LLM can be instructed to just figure all that shit out. Often, it will drop you precisely at that golden moment where shit almost works, and development means tweaking code and immediately seeing things work better. Well, except that in order to fix that 1% you'd need to read and understand whatever the LLM did and then look for that 1%. I get the shills just thinking about this, whether the original programmer was human or not. I'd rather just write everything myself to begin with. The one main claim the article makes: Senior developers should not ignore the productivity gains from LLMs. Best use of evidence is deductive: Lots of code is tedious and uninteresting -> LLMs are fast at generating lots of tedious code -> LLMs help productivity. Weakest part of the argument: The list of rebuttals doesn't have an obvious organization to it. What exactly is the main argument they're arguing against? It's not stated outright but because the post is bookended by references to 'those smarter than me', I think this is an argument against the shaming of developers using (and loving) LLM tools. Which I think is fair. Overall, the post did not add anything to the general discussion. But the popularity of the author (and fly.io posts) may make it a beacon for some. A couple of thoughts: On 'just read the code' - all well and good. Of course this implies insisting on team members who can read the code, will read the code and are empowered to read the code. Otherwise orgs will try to hire pure vibe coders who aren't interested in that and only allow time for literally just instructing agents to generate code because it sounds cheaper and execs don't understand the nuance so long as it looks like product is shipping - until it all blows up and the one standing senior developer on hand is supposed to fix a prod issue buried in millions of lines of vibe reviewed code ASAP. On 'but it's cheaper than a junior': cloud hosted LLM systems are currently massively subsidised to an absurd degree. The cost side of things is all smoke and mirrors geared towards accelerated market adoption at all costs. It's not a profitable enterprise at the model development level. At some point that AI economy is going to expect to make that money back, and future (especially near-future) hardware advancements don't explain where all of that is going to come from. If you are resource constrained this article will make you sad. I do not have unlimited funds to plug in some token and burn a bunch of money when writing code. I am gpu poor. I'm lucky that 8gb vram can run the smallest models. But the output is so poor that I lose out to anyone using a hosted service. If anything this article shows that building great programs is less democratized than it once was. I'm working with python and it does not work very well, LLMs usually generates at least an order of magnitude more code than what I would write. That code often uses outdated practices, does poor design choices and does not understand hints like writing code in a way that reduce cognitive load, even when we explain that it means, i.e. keep the number of classes and functions small. It's the complete opposite of OP's main language GO, as he says: > Go has just enough type safety, an extensive standard library, and a culture that prizes (often repetitive) idiom. LLMs kick ass generating it. Python is an interpreted dynamically typed language and the static type checkers are not there yet (most popular 3rd parties libraries have no type hints for example). Also it allows for many different programming styles that the LLMs struggle to choose from. 1. Every extra line of code is much more risky. 2. It's much harder to verify the LLM's code. On the other hand I think rust will be in a good place in regards to LLMs in the next few years thanks to the robustness of the language and the quality of its diagnostic messages. Those 2 attributes should compound very well. Let’s not conflate LLM’s with AI. LLM’s are a kind of AI that can be a software engineer’s assistant, at best. But the degree of hype is undermining belief in AI among many professionals who the hypesters claim are going to be replaced. No, this iteration is not going to replace doctors or engineers. But the degree to which the hype train wants to do so is alarming. > LLMs can write a large fraction of all the tedious code you’ll ever need to write. And most code on most projects is tedious. You still need to read this tedious code to verify that it actually does what you want it to do. Given this, I'd much rather prefer to write the tedious code myself than having to make sense of someone else's tedious code. I think the key premise here is that one can effectively and efficiently audit code that the LLM is producing. I doubt that. First, human attention and speed is very limited. Second, when I see something, I am already predisposed to assume that it is right (or at the very least, my subsequent inquiries are extremely narrow and anchored around the solution I have seen presented to me.) A lot of people who are wary of LLMs aren’t against the technology itself, but rather the narrative surrounding it. You can take advantage of the tool while turning a blind eye to the discourse. This 16-minute, expletive-filled, edgy-old-man-trying-too-hard-to-be-cool article could easily be dismissed as yet another AI creed that somehow found its way to the top of the HN front page. When I made a mid career change to Cobol programming in 1981 my first manager (a suit as programmers referred to them then) pointed to a book on his desk; the title,as I can best recall was Programming Without Programmers. He ""You got in too late,"" he said. I retired from programming in 2010. The hype has a long history. I hope I'm around long enough to see how this plays out. I'm on the side of AI is a huge productivity booster (by my guess... 10x) But I don't want to make the claim lightly, so I did an experiment. I signed up for copilot pro, and have been using their 'edit' feature. This is more than just their auto complete. I set myself a goal to create a playable web game of classic frogger. It took 4 hours with copilot ""edit"" and my full attention. I didn't write a single line of code, but I did ask it to refactor and gave it a project description. I suspect this would have taken me 4 days full time to get to this level. Try it out: https://dev.zebrar.com/jd/frogger/ I will create an account just to point out that the response to > But it is an inherently plagiarist technology Was > Developers frequently engage in copyright infringement, and so will I, so unless if you're a lawyer, shove it up your ass ""I am a bad person so I get to continue being bad"" is not the gotcha you think it is, Patrick. ""Kids today don’t just use agents; they use asynchronous agents. They wake up, free-associate 13 different things for their LLMs to work on, make coffee, fill out a TPS report, drive to the Mars Cheese Castle, and then check their notifications. They’ve got 13 PRs to review. Three get tossed and re-prompted. Five of them get the same feedback a junior dev gets. And five get merged."" I would jump off a bridge before I accepted that as my full-time job. I've been programming for 20+ years and I've never wanted to move into management. I got into programming because I like programming, not because I like asking others to write code on my behalf and review what they come up with. I've been in a lead role, and I certainly do lots of code review and enjoy helping teammates grow. But the last fucking thing I want to do is delegate all the code writing to someone or something else. I like writing code. Yes, sometimes writing code is tedious, or frustrating. Sometimes it's yak-shaving. Sometimes it's Googling. Very often, it's debugging. I'm happy to have AI help me with some of that drudgery, but if I ever get to the point that I feel like I spend my entire day in virtual meetings with AI agents, then I'm changing careers. I get up in the morning to make things, not to watch others make things. Maybe the kind of software engineering role I love is going to disappear, like stevedores and lamplighters. I will miss it dearly, but at least I guess I got a couple of good decades out of it. If this is what the job turns into, I'll have to find something else to do with my remaining years. There are many aspects to AI push back. - all creatives are flat against it because it’s destroying their income streams and outright stealing their intellectual property - some technical leaders are skeptical because early returns were very bad and they have not updated their investigations to the latest tools and models, which are already significantly ahead of even six months ago - a tech concern is how do we mentor new developers if they don’t know how to code or develop logic. LLMs are great IF you already know what you’re doing - talent is deeply concerned that they will be reduced and replaced, going from high paying careers to fast food salaries We have a lot of work to balance productivity with the benefits to society. “Let them eat cake,” is not going to work this time either. Seeing everything these days being about vibe coding, I feel a little old with my VIM setup and my LSP servers who I already thought were a nice productivity increase. The problems I have with the stuff relating to MCP is that the tech around it is developing so fast that it's hard for outsiders to catch up with what the best working setup is, for example. What would you do, for example, if you want to selfhost this? - which models (qwen ai coder?) - which api (with ollama? Bolt? Aider? Etc) - how to integrate PRs with a local gitlab/gogs/forgejo instance? Do you need another MCP agent for git that does that? - which hardware dependencies to run it? I am currently trying to figure out how to implement a practical workflow for this. So far I'm using still a synchronous MCP agent setup where it basically runs on another machine in the network because I have a too unperformant laptop to work with. But how would I get to the point of async MCP agents that can work on multiple things in my Go codebases in parallel? With the mentioned PR workflows so that I can modify/edit/rework before the merges? The author makes a lot of claims and talks always about that their opponents in the argument are not talking about the same thing. But what exactly is the same thing, which is reproducible locally for everyone? Machine translation and speech recognition. The state of the art for these is a multi-modal language model. I'm hearing impaired veering on deaf, and I use this technology all day every day. I wanted to watch an old TV series from the 1980s. There are no subtitles available. So I fed the show into a language model (Whisper) and now I have passable subtitles that allow me to watch the show. Am I the only one who remembers when that was the stuff of science fiction? It was not so long ago an open question if machines would ever be able to transcribe speech in a useful way. How quickly we become numb to the magic. Can someone comment on the cost of running agentic models? Not for a company but for an individual. I tried ""vibe coding"" a personal project I was struggling with and left even more frustrated because I kept running into token rate limits with Claude (used inside of Zed if it matters). Did I pick the wrong model, the wrong editor, or do I just need to not be so tight with my money? > This was the craftsman’s ‘Golden Age’ and much time and trouble was taken over the design of tools. Craftsmen were being called upon to do more skilful and exacting work and the use of tools and the interest in development had become very widespread. Above pulled from A Brief History of the Woodworking Plane [0]. A woodworking tool that has evolved over 2,000 years. Now there are electric planers, handheld electric planers and lots of heavy machinery that do the same thing in a very automated way. If a company is mass producing kitchen cabinets, they aren't hand planing edges on boards, a machine is doing all that work. I feel like with AI we are on the cusp of moving beyond a ""Golden age"" and into an ""industrial age"" for coding, where it will become more important to have code that AI understands vs. something that is carefully crafted. Simple business pressure will demand it (whether we like it or not). ^ A comment I made just yesterday on a different thread. For software developers AI is like the cabinet maker that gets a machine to properly mill and produce cabinet panels, sure you can use a hand plane to do that but you're producing a very different product and likely one that not many people will care about, possibly not even your employer when they see all the other wood shops pumping out cabinetry and taking their market share. [0] https://www.handplane.com/879/a-brief-history-of-the-woodwor... Everything about that is true but, and that's a big BUT, the code I write with LLM I can only iterate on it with an LLM. My mind doesn't develop a mental model of that code, I don't know where the relevant parts are, I can't quickly navigate through it and I have to reach the LLM for every small change. Which is why I like Copilot style editing more than agents as a working model but agents are just so much more powerful and smarter thanks to everything available to them. I tend to agree with the gist of this, namely that the change is here and ""AI"" presents some huge potential to save me from having to do the tedious stuff. But we do need a bit of a sanity check. I'm in the middle of trying to convince any of the AIs that I have access to to write me some simple Lua. Specifically, I'm trying to write a query that returns a certain subset of pages from the Silverbullet V2 note tool. This isn't particularly difficult, but it's become this hilarious journey to try to get the AI to figure this problem out. Silverbullet is a niche use case, v2 even more so, but a reasonably skilled developer could peruse the API documentation and come up with a pretty good starting point. AI tools? Absolutely insane wrong answers. I finally specifically asked one or two of them not to guess with their answers and they just straight up said ""nah, we don't know how to do this."" Point being: there's some real power in these tools, but if the ground is not already well-trodden, they risk creating a huge time sink that could be better served just learning to do the thing yourself."
44800746,Open models by OpenAI,https://openai.com/index/introducing-gpt-oss/,https://openai.com/open-models/,lackoftactics,2124,2025-08-05T17:02:02+00:00,,query,50,"The lede is being missed imo. gpt-oss:20b is a top ten model (on MMLU (right behind Gemini-2.5-Pro) and I just ran it locally on my Macbook Air M3 from last year. I've been experimenting with a lot of local models, both on my laptop and on my phone (Pixel 9 Pro), and I figured we'd be here in a year or two. But no, we're here today. A basically frontier model, running for the cost of electricity (free with a rounding error) on my laptop. No $200/month subscription, no lakes being drained, etc. I'm blown away. Model cards, for the people interested in the guts: https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7... In my mind, I’m comparing the model architecture they describe to what the leading open-weights models (Deepseek, Qwen, GLM, Kimi) have been doing. Honestly, it just seems “ok” at a technical level: - both models use standard Grouped-Query Attention (64 query heads, 8 KV heads). The card talks about how they’ve used an older optimization from GPT3, which is alternating between banded window (sparse, 128 tokens) and fully dense attention patterns. It uses RoPE extended with YaRN (for a 131K context window). So they haven’t been taking advantage of the special-sauce Multi-head Latent Attention from Deepseek, or any of the other similar improvements over GQA. - both models are standard MoE transformers. The 120B model (116.8B total, 5.1B active) uses 128 experts with Top-4 routing. They’re using some kind of Gated SwiGLU activation, which the card talks about as being ""unconventional"" because of to clamping and whatever residual connections that implies. Again, not using any of Deepseek’s “shared experts” (for general patterns) + “routed experts” (for specialization) architectural improvements, Qwen’s load-balancing strategies, etc. - the most interesting thing IMO is probably their quantization solution. They did something to quantize >90% of the model parameters to the MXFP4 format (4.25 bits/parameter) to let the 120B model to fit on a single 80GB GPU, which is pretty cool. But we’ve also got Unsloth with their famous 1.58bit quants :) All this to say, it seems like even though the training they did for their agentic behavior and reasoning is undoubtedly very good, they’re keeping their actual technical advancements “in their pocket”. Open models are going to win long-term. Anthropics' own research has to use OSS models [0]. China is demonstrating how quickly companies can iterate on open models, allowing smaller teams access and augmentation to the abilities of a model without paying the training cost. My personal prediction is that the US foundational model makers will OSS something close to N-1 for the next 1-3 iterations. The CAPEX for the foundational model creation is too high to justify OSS for the current generation. Unless the US Gov steps up and starts subsidizing power, or Stargate does 10x what it is planned right now. N-1 model value depreciates insanely fast. Making an OSS release of them and allowing specialized use cases and novel developments allows potential value to be captured and integrated into future model designs. It's medium risk, as you may lose market share. But also high potential value, as the shared discoveries could substantially increase the velocity of next-gen development. There will be a plethora of small OSS models. Iteration on the OSS releases is going to be biased towards local development, creating more capable and specialized models that work on smaller and smaller devices. In an agentic future, every different agent in a domain may have its own model. Distilled and customized for its use case without significant cost. Everyone is racing to AGI/SGI. The models along the way are to capture market share and use data for training and evaluations. Once someone hits AGI/SGI, the consumer market is nice to have, but the real value is in novel developments in science, engineering, and every other aspect of the world. [0] https://www.anthropic.com/research/persona-vectors > We demonstrate these applications on two open-source models, Qwen 2.5-7B-Instruct and Llama-3.1-8B-Instruct. Running a model comparable to o3 on a 24GB Mac Mini is absolutely wild. Seems like yesterday the idea of running frontier (at the time) models locally or on a mobile device was 5+ years out. At this rate, we'll be running such models in the next phone cycle. Inference in Python uses harmony [1] (for request and response format) which is written in Rust with Python bindings. Another OpenAI's Rust library is tiktoken [2], used for all tokenization and detokenization. OpenAI Codex [3] is also written in Rust. It looks like OpenAI is increasingly adopting Rust (at least for inference). [1] https://github.com/openai/harmony [2] https://github.com/openai/tiktoken [3] https://github.com/openai/codex So this confirms a best-in-class model release within the next few days? From a strategic perspective, I can't think of any reason they'd release this unless they were about to announce something which totally eclipses it? Seeing a 20B model competing with o3's performance is mind blowing like just a year ago, most of us would've called this impossible - not just the intelligence leap, but getting this level of capability in such a compact size. I think that the point that makes me more excited is that we can train trillion-parameter giants and distill them down to just billions without losing the magic. Imagine coding with Claude 4 Opus-level intelligence packed into a 10B model running locally at 2000 tokens/sec - like instant AI collaboration. That would fundamentally change how we develop software. Orthogonal, but I just wanted to say how awesome Ollama is. It took 2 seconds to find the model and a minute to download and now I'm using it. Kudos to that team. Just posted my initial impressions, took a couple of hours to write them up because there's a lot in this release! https://simonwillison.net/2025/Aug/5/gpt-oss/ TLDR: I think OpenAI may have taken the medal for best available open weight model back from the Chinese AI labs. Will be interesting to see if independent benchmarks resolve in that direction as well. The 20B model runs on my Mac laptop using less than 15GB of RAM. I'm a well-known OpenAI hater, but there's haters and haters, and refusing to acknowledge great work is the latter. Well done OpenAI, this seems like a sincere effort to do a real open model with competitive performance, usable/workable licensing, a tokenizer compatible with your commercial offerings, it's a real contribution. Probably the most open useful thing since Whisper that also kicked ass. Keep this sort of thing up and I might start re-evaliating how I feel about this company. Looks like Groq (at 1k+ tokens/second) and Fireworks are already live on openrouter: https://openrouter.ai/openai/gpt-oss-120b $0.15M in / $0.6-0.75M out edit: Now Cerebras too at 3,815 tps for $0.25M / $0.69M out. Disclamer: probably dumb questions so, the 20b model. Can someone explain to me what I would need to do in terms of resources (GPU, I assume) if I want to run 20 concurrent processes, assuming I need 1k tokens/second throughput (on each, so 20 x 1k) Also, is this model better/comparable for information extraction compared to gpt-4.1-nano, and would it be cheaper to host myself 20b? Super excited to see these released! Major points of interest for me: - In the ""Main capabilities evaluations"" section, the 120b outperform o3-mini and approaches o4 on most evals. 20b model is also decent, passing o3-mini on one of the tasks. - AIME 2025 is nearly saturated with large CoT - CBRN threat levels kind of on par with other SOTA open source models. Plus, demonstrated good refusals even after adversarial fine tuning. - Interesting to me how a lot of the safety benchmarking runs on trust, since methodology can't be published too openly due to counterparty risk. Model cards with some of my annotations: https://openpaper.ai/paper/share/7137e6a8-b6ff-4293-a3ce-68b... thanks openai for being open ;) Surprised there are no official MLX versions and only one mention of MLX in this thread. MLX basically converst the models to take advntage of mac unified memory for 2-5x increase in power, enabling macs to run what would otherwise take expensive gpus (within limits). So FYI to any one on mac, the easiest way to run these models right now is using LM Studio ( https://lmstudio.ai/ ), its free. You just search for the model, usually 3rd party groups mlx-community or lmstudio-community have mlx versions within a day or 2 of releases. I go for the 8-bit quantizations (4-bit faster, but quality drops). You can also convert to mlx yourself... Once you have it running on LM studio, you can chat there in their chat interface, or you can run it through api that defaults to http://127.0.0.1:1234 You can run multiple models that hot swap and load instantly and switch between them etc. Its surpassingly easy, and fun.There are actually a lot of cool niche models comings out, like this tiny high-quality search model released today as well (and who released official mlx version) https://huggingface.co/Intelligent-Internet/II-Search-4B Other fun ones are gemma 3n which is model multi-modal, larger one that is actually solid model but takes more memory is the new Qwen3 30b A3B (coder and instruct), Pixtral (mixtral vision with full resolution images), etc. Look forward to playing with this model and see how it compares. Listed performance of ~5 points less than o3 on benchmarks is pretty impressive. Wonder if they feel the bar will be raised soon (GPT-5) and feel more comfortable releasing something this strong. I was able to get gpt-oss:20b wired up to claude code locally via a thin proxy and ollama. It's fun that it works, but the prefill time makes it feel unusable. (2-3 minutes per tool-use / completion). Means a ~10-20 tool-use interaction could take 30-60 minutes. (This editing a single server.py file that was ~1000 lines, the tool definitions + claude context was around 30k tokens input, and then after the file read, input was around ~50k tokens. Definitely could be optimized. Also I'm not sure if ollama supports a kv-cache between invocations of /v1/completions, which could help) Holy smokes, there's already llama.cpp support: https://github.com/ggml-org/llama.cpp/pull/15091 GPQA Diamond: gpt-oss-120b: 80.1%, Qwen3-235B-A22B-Thinking-2507: 81.1% Humanity’s Last Exam: gpt-oss-120b (tools): 19.0%, gpt-oss-120b (no tools): 14.9%, Qwen3-235B-A22B-Thinking-2507: 18.2% I just tried it on open router but i was served by cerebras. Holy... 40,000 tokens per second. That was SURREAL. I got a 1.7k token reply delivered too fast for the human eye to perceive the streaming. n=1 for this 120b model but id rank the reply #1 just ahead of claude sonnet 4 for a boring JIRA ticket shuffling type challenge. EDIT: The same prompt on gpt-oss, despite being served 1000x slower, wasn't as good but was in a similar vein. It wanted to clarify more and as a result only half responded. Getting great performance running gpt-oss on 3x A4000's: gpt-oss:20b = ~46 tok/s More than 2x faster than my previous leading OSS models: mistral-small3.2:24b = ~22 tok/s gemma3:27b = ~19.5 tok/s Strangely getting nearly the opposite performance running on 1x 5070 Ti: mistral-small3.2:24b = ~39 tok/s gpt-oss:20b = ~21 tok/s Where gpt-oss is nearly 2x slow vs mistral-small 3.2. The coding seems to be one of the strongest use cases for LLMs. Though currently they are eating too many tokens to be profitable. So perhaps these local models could offload some tasks to local computers. E.g. Hybrid architecture. Local model gathers more data, runs tests, does simple fixes, but frequently asks the stronger model to do the real job. Local model gathers data using tools and sends more data to the stronger model. It Open weight models from OpenAI with performance comparable to that of o3 and o4-mini in benchmarks… well, I certainly wasn’t expecting that. What’s the catch? I want to take this chance to say a big thank you to OpenAI and your work. I have always been a fan since I noticed you hired the sandbox game kickstarter guy about like 8 years ago. Even from the UK I knew you would all do great things ( I had had no idea who else was involved). I am glad I see the top comment is rare praise on HN. Thanks again and keep it up Sama and team. Reading the comments it becomes clear how befuddled many HN participants are about AI. I don't think there has been a technical topic that HN has seemed so dull on in the many years I've been reading HN. This must be an indication that we are in a bubble. One basic point that is often missed is: Different aspects of LLM performance (in the cognitive performance sense) and LLM resource utilization are relevant to various use cases and business models. Another is that there are many use cases where users prefer to run inference locally, for a variety of domain-specific or business model reasons. The list goes on. I benchmarked the 120B version on the Extended NYT Connections (759 questions, https://github.com/lechmazur/nyt-connections ) and on 120B and 20B on Thematic Generalization (810 questions, https://github.com/lechmazur/generalization ). Opus 4.1 benchmarks are also there. I think this is a belated but smart move by OpenAI. They are basically fully moving in on Meta's strategy now, taking advantage of what may be a temporary situation with Meta dropping back in model race. It will be interesting to see if these models now get taken up by the local model / fine tuning community the way llama was. It's a very appealing strategy to test / dev with a local model and then have the option to deploy to prod on a high powered version of the same thing. Always knowing if the provider goes full hostile, or you end up with data that can't move off prem, you have self hosting as an option with a decent performing model. Which is all to say, availability of these local models for me is a key incentive that I didn't have before to use OpenAI's hosted ones. What a day! Models aside, the Harmony Response Format[1] also seems pretty interesting and I wonder how much of an impact it might have in performance of these models. [1] https://github.com/openai/harmony Wow, today is a crazy AI release day: - OAI open source - Opus 4.1 - Genie 3 - ElevenLabs Music I'm out of the loop for local models. For my M3 24gb ram macbook, what token throughput can I expect? Edit: I tried it out, I have no idea in terms of of tokens but it was fluid enough for me. A bit slower than using o3 in the browser but definitely tolerable. I think I will set it up in my GF's machine so she can stop paying for the full subscription (she's a non-tech professional) The 120B model badly hallucinates facts on the level of a 0.6B model. My go to test for checking hallucinations is 'Tell me about Mercantour park' (a national park in south eastern France). Easily half of the facts are invented. Non-existing mountain summits, brown bears (no, there are none), villages that are elsewhere, wrong advice ('dogs allowed' - no they are not). Why do companies release open source LLMs? I would understand it, if there was some technology lock-in. But with LLMs, there is no such thing. One can switch out LLMs without any friction. > To improve the safety of the model, we filtered the data for harmful content in pre-training, especially around hazardous biosecurity knowledge, by reusing the CBRN pre-training filters from GPT-4o. Our model has a knowledge cutoff of June 2024. This would be a great ""AGI"" test. See if it can derive biohazards from first principles I love how they frame High-end desktops and laptops as having ""a single H100 GPU"". Can't wait to see third party benchmarks. The ones in the blog post are quite sparse and it doesn't seem possible to fully compare to other open models yet. But the few numbers available seem to suggest that this release will make all other non-multimodal open models obsolete. It seems like OSS will win, I can't see people willing to pay like 10x the price for what seems like 10% more performance. Especially once we get better at routing the hardest questions to the better models and then using that response to augment/fine-tune the OSS ones. I did a quick `openai/gpt-oss-20b` testing on an Macbook Pro M1 16GB. Pretty impressed with it so far. * It seems that using version @lmstudio's 20B gguf version ( https://huggingface.co/lmstudio-community/gpt-oss-20b-GGUF ) will have options for reasoning effort. * My MBP M1 16GB config: temp 0.8, max content length 7990, GPU offload 8/24, runs slow and still fine for me. * I tried testing with MCP with the above config, with basic tools like time and fetch + reasoning effort low, and the tool calls instruction follow is quite good. * In LM Studio's Developer tab there is a log output about the model information which is useful to learn. Overall, I like the way OpenAI backs to being Open AI, again, after all those years. -- Shameless plug, If anyone want to try out gpt-oss-120b and gpt-oss-20b as alternative to their own demo page [0], I have added both models with OpenRouter providers in VT Chat [1] as real product. You can try with an OpenRouter API Key. [0] https://gpt-oss.com [1] https://vtchat.io.vn Does anyone get the demos at https://www.gpt-oss.com to work, or are the servers down immediately after launch? I'm only getting the spinner after prompting. What's the best agent to run this on? Is it compatible with Codex? For OSS agents, I've been using Qwen Code (clunky fork of Gemini), and Goose. Shoutout to the hn consensus regarding an OpenAI open model release from 4 days ago: https://news.ycombinator.com/item?id=44758511 Why would OpenAI give this away for free? Is it to disrupt competition by setting a floor at the lower end of the market and make it harder for new competition to emerge while still retaining mind share? This is an extremely welcome move in a good direction from OpenAI. I can only thank them for all of the extra work around the models - Harmony structure, metal/torch/triton implementations, inference guides, cookbooks & fine-tuning/reinforcement learning scripts, datasets etc. There is an insane amount of helpful information buried in this release Super shallow (24/36 layers) MoE with low active parameter counts (3.6B/5.1B), a tradeoff between inference speed and performance. Text only, which is okay. Weights partially in MXFP4, but no cuda kernel support for RTX 50 series (sm120). Why? This is a NO for me. Safety alignment shifts from off the charts to off the rails really fast if you keep prompting. This is a NO for me. In summary, a solid NO for me. > We introduce gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models available under the Apache 2.0 license and our gpt-oss usage policy. [0] Is it even valid to have additional restriction on top of Apache 2.0? [0]: https://openai.com/index/gpt-oss-model-card/ Releasing this under the Apache license is a shot at competitors that want to license their models on Open Router and enterprise. It eliminates any reason to use an inferior Meta or Chinese model that costs money to license, thus there are no funds for these competitors to build a GPT 5 competitor. Newbie question: I remember folks talking about how kimi 2’s launch might have pushed OpenAI to launch their model later. Now that we (shortly will) know how this model performs, how do they stack up? Did openAI likely actually hold off releasing weights because of kimi, in retrospect? Wow, this will eat Meta's lunch I dont see the unsloth files yet but they'll be here: https://huggingface.co/unsloth/gpt-oss-20b-GGUF Super excited to test these out. The benchmarks from 20B are blowing away major >500b models. Insane. On my hardware. 43 tokens/sec. I got an error with flash attention turning on. Cant run it with flash attention? 31,000 context is max it will allow or model wont load. no kv or v quantization. Shameless plug: if someone wants to try it in a nice ui, you could give Msty[1] a try. It's private and local. [1]: https://msty.ai Big picture, what's the balance going to look like, going forward between what normal people can run on a fancy computer at home vs heavy duty systems hosted in big data centers that are the exclusive domain of Big Companies? This is something about AI that worries me, a 'child' of the open source coming of age era in the 90ies. I don't want to be forced to rely on those big companies to do my job in an efficient way, if AI becomes part of the day to day workflow. Wow I really didn’t think this would happen any time soon, they seem to have more to lose than to gain. If you’re a company building AI into your product right now I think you would be irresponsible to not investigate how much you can do on open weights models. The big AI labs are going to pull the ladder up eventually, building your business on the APIs long term is foolish. These open models will always be there for you to run though (if you can get GPUs anyway).","Open models by OpenAI https://openai.com/index/introducing-gpt-oss/ The lede is being missed imo. gpt-oss:20b is a top ten model (on MMLU (right behind Gemini-2.5-Pro) and I just ran it locally on my Macbook Air M3 from last year. I've been experimenting with a lot of local models, both on my laptop and on my phone (Pixel 9 Pro), and I figured we'd be here in a year or two. But no, we're here today. A basically frontier model, running for the cost of electricity (free with a rounding error) on my laptop. No $200/month subscription, no lakes being drained, etc. I'm blown away. Model cards, for the people interested in the guts: https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7... In my mind, I’m comparing the model architecture they describe to what the leading open-weights models (Deepseek, Qwen, GLM, Kimi) have been doing. Honestly, it just seems “ok” at a technical level: - both models use standard Grouped-Query Attention (64 query heads, 8 KV heads). The card talks about how they’ve used an older optimization from GPT3, which is alternating between banded window (sparse, 128 tokens) and fully dense attention patterns. It uses RoPE extended with YaRN (for a 131K context window). So they haven’t been taking advantage of the special-sauce Multi-head Latent Attention from Deepseek, or any of the other similar improvements over GQA. - both models are standard MoE transformers. The 120B model (116.8B total, 5.1B active) uses 128 experts with Top-4 routing. They’re using some kind of Gated SwiGLU activation, which the card talks about as being ""unconventional"" because of to clamping and whatever residual connections that implies. Again, not using any of Deepseek’s “shared experts” (for general patterns) + “routed experts” (for specialization) architectural improvements, Qwen’s load-balancing strategies, etc. - the most interesting thing IMO is probably their quantization solution. They did something to quantize >90% of the model parameters to the MXFP4 format (4.25 bits/parameter) to let the 120B model to fit on a single 80GB GPU, which is pretty cool. But we’ve also got Unsloth with their famous 1.58bit quants :) All this to say, it seems like even though the training they did for their agentic behavior and reasoning is undoubtedly very good, they’re keeping their actual technical advancements “in their pocket”. Open models are going to win long-term. Anthropics' own research has to use OSS models [0]. China is demonstrating how quickly companies can iterate on open models, allowing smaller teams access and augmentation to the abilities of a model without paying the training cost. My personal prediction is that the US foundational model makers will OSS something close to N-1 for the next 1-3 iterations. The CAPEX for the foundational model creation is too high to justify OSS for the current generation. Unless the US Gov steps up and starts subsidizing power, or Stargate does 10x what it is planned right now. N-1 model value depreciates insanely fast. Making an OSS release of them and allowing specialized use cases and novel developments allows potential value to be captured and integrated into future model designs. It's medium risk, as you may lose market share. But also high potential value, as the shared discoveries could substantially increase the velocity of next-gen development. There will be a plethora of small OSS models. Iteration on the OSS releases is going to be biased towards local development, creating more capable and specialized models that work on smaller and smaller devices. In an agentic future, every different agent in a domain may have its own model. Distilled and customized for its use case without significant cost. Everyone is racing to AGI/SGI. The models along the way are to capture market share and use data for training and evaluations. Once someone hits AGI/SGI, the consumer market is nice to have, but the real value is in novel developments in science, engineering, and every other aspect of the world. [0] https://www.anthropic.com/research/persona-vectors > We demonstrate these applications on two open-source models, Qwen 2.5-7B-Instruct and Llama-3.1-8B-Instruct. Running a model comparable to o3 on a 24GB Mac Mini is absolutely wild. Seems like yesterday the idea of running frontier (at the time) models locally or on a mobile device was 5+ years out. At this rate, we'll be running such models in the next phone cycle. Inference in Python uses harmony [1] (for request and response format) which is written in Rust with Python bindings. Another OpenAI's Rust library is tiktoken [2], used for all tokenization and detokenization. OpenAI Codex [3] is also written in Rust. It looks like OpenAI is increasingly adopting Rust (at least for inference). [1] https://github.com/openai/harmony [2] https://github.com/openai/tiktoken [3] https://github.com/openai/codex So this confirms a best-in-class model release within the next few days? From a strategic perspective, I can't think of any reason they'd release this unless they were about to announce something which totally eclipses it? Seeing a 20B model competing with o3's performance is mind blowing like just a year ago, most of us would've called this impossible - not just the intelligence leap, but getting this level of capability in such a compact size. I think that the point that makes me more excited is that we can train trillion-parameter giants and distill them down to just billions without losing the magic. Imagine coding with Claude 4 Opus-level intelligence packed into a 10B model running locally at 2000 tokens/sec - like instant AI collaboration. That would fundamentally change how we develop software. Orthogonal, but I just wanted to say how awesome Ollama is. It took 2 seconds to find the model and a minute to download and now I'm using it. Kudos to that team. Just posted my initial impressions, took a couple of hours to write them up because there's a lot in this release! https://simonwillison.net/2025/Aug/5/gpt-oss/ TLDR: I think OpenAI may have taken the medal for best available open weight model back from the Chinese AI labs. Will be interesting to see if independent benchmarks resolve in that direction as well. The 20B model runs on my Mac laptop using less than 15GB of RAM. I'm a well-known OpenAI hater, but there's haters and haters, and refusing to acknowledge great work is the latter. Well done OpenAI, this seems like a sincere effort to do a real open model with competitive performance, usable/workable licensing, a tokenizer compatible with your commercial offerings, it's a real contribution. Probably the most open useful thing since Whisper that also kicked ass. Keep this sort of thing up and I might start re-evaliating how I feel about this company. Looks like Groq (at 1k+ tokens/second) and Fireworks are already live on openrouter: https://openrouter.ai/openai/gpt-oss-120b $0.15M in / $0.6-0.75M out edit: Now Cerebras too at 3,815 tps for $0.25M / $0.69M out. Disclamer: probably dumb questions so, the 20b model. Can someone explain to me what I would need to do in terms of resources (GPU, I assume) if I want to run 20 concurrent processes, assuming I need 1k tokens/second throughput (on each, so 20 x 1k) Also, is this model better/comparable for information extraction compared to gpt-4.1-nano, and would it be cheaper to host myself 20b? Super excited to see these released! Major points of interest for me: - In the ""Main capabilities evaluations"" section, the 120b outperform o3-mini and approaches o4 on most evals. 20b model is also decent, passing o3-mini on one of the tasks. - AIME 2025 is nearly saturated with large CoT - CBRN threat levels kind of on par with other SOTA open source models. Plus, demonstrated good refusals even after adversarial fine tuning. - Interesting to me how a lot of the safety benchmarking runs on trust, since methodology can't be published too openly due to counterparty risk. Model cards with some of my annotations: https://openpaper.ai/paper/share/7137e6a8-b6ff-4293-a3ce-68b... thanks openai for being open ;) Surprised there are no official MLX versions and only one mention of MLX in this thread. MLX basically converst the models to take advntage of mac unified memory for 2-5x increase in power, enabling macs to run what would otherwise take expensive gpus (within limits). So FYI to any one on mac, the easiest way to run these models right now is using LM Studio ( https://lmstudio.ai/ ), its free. You just search for the model, usually 3rd party groups mlx-community or lmstudio-community have mlx versions within a day or 2 of releases. I go for the 8-bit quantizations (4-bit faster, but quality drops). You can also convert to mlx yourself... Once you have it running on LM studio, you can chat there in their chat interface, or you can run it through api that defaults to http://127.0.0.1:1234 You can run multiple models that hot swap and load instantly and switch between them etc. Its surpassingly easy, and fun.There are actually a lot of cool niche models comings out, like this tiny high-quality search model released today as well (and who released official mlx version) https://huggingface.co/Intelligent-Internet/II-Search-4B Other fun ones are gemma 3n which is model multi-modal, larger one that is actually solid model but takes more memory is the new Qwen3 30b A3B (coder and instruct), Pixtral (mixtral vision with full resolution images), etc. Look forward to playing with this model and see how it compares. Listed performance of ~5 points less than o3 on benchmarks is pretty impressive. Wonder if they feel the bar will be raised soon (GPT-5) and feel more comfortable releasing something this strong. I was able to get gpt-oss:20b wired up to claude code locally via a thin proxy and ollama. It's fun that it works, but the prefill time makes it feel unusable. (2-3 minutes per tool-use / completion). Means a ~10-20 tool-use interaction could take 30-60 minutes. (This editing a single server.py file that was ~1000 lines, the tool definitions + claude context was around 30k tokens input, and then after the file read, input was around ~50k tokens. Definitely could be optimized. Also I'm not sure if ollama supports a kv-cache between invocations of /v1/completions, which could help) Holy smokes, there's already llama.cpp support: https://github.com/ggml-org/llama.cpp/pull/15091 GPQA Diamond: gpt-oss-120b: 80.1%, Qwen3-235B-A22B-Thinking-2507: 81.1% Humanity’s Last Exam: gpt-oss-120b (tools): 19.0%, gpt-oss-120b (no tools): 14.9%, Qwen3-235B-A22B-Thinking-2507: 18.2% I just tried it on open router but i was served by cerebras. Holy... 40,000 tokens per second. That was SURREAL. I got a 1.7k token reply delivered too fast for the human eye to perceive the streaming. n=1 for this 120b model but id rank the reply #1 just ahead of claude sonnet 4 for a boring JIRA ticket shuffling type challenge. EDIT: The same prompt on gpt-oss, despite being served 1000x slower, wasn't as good but was in a similar vein. It wanted to clarify more and as a result only half responded. Getting great performance running gpt-oss on 3x A4000's: gpt-oss:20b = ~46 tok/s More than 2x faster than my previous leading OSS models: mistral-small3.2:24b = ~22 tok/s gemma3:27b = ~19.5 tok/s Strangely getting nearly the opposite performance running on 1x 5070 Ti: mistral-small3.2:24b = ~39 tok/s gpt-oss:20b = ~21 tok/s Where gpt-oss is nearly 2x slow vs mistral-small 3.2. The coding seems to be one of the strongest use cases for LLMs. Though currently they are eating too many tokens to be profitable. So perhaps these local models could offload some tasks to local computers. E.g. Hybrid architecture. Local model gathers more data, runs tests, does simple fixes, but frequently asks the stronger model to do the real job. Local model gathers data using tools and sends more data to the stronger model. It Open weight models from OpenAI with performance comparable to that of o3 and o4-mini in benchmarks… well, I certainly wasn’t expecting that. What’s the catch? I want to take this chance to say a big thank you to OpenAI and your work. I have always been a fan since I noticed you hired the sandbox game kickstarter guy about like 8 years ago. Even from the UK I knew you would all do great things ( I had had no idea who else was involved). I am glad I see the top comment is rare praise on HN. Thanks again and keep it up Sama and team. Reading the comments it becomes clear how befuddled many HN participants are about AI. I don't think there has been a technical topic that HN has seemed so dull on in the many years I've been reading HN. This must be an indication that we are in a bubble. One basic point that is often missed is: Different aspects of LLM performance (in the cognitive performance sense) and LLM resource utilization are relevant to various use cases and business models. Another is that there are many use cases where users prefer to run inference locally, for a variety of domain-specific or business model reasons. The list goes on. I benchmarked the 120B version on the Extended NYT Connections (759 questions, https://github.com/lechmazur/nyt-connections ) and on 120B and 20B on Thematic Generalization (810 questions, https://github.com/lechmazur/generalization ). Opus 4.1 benchmarks are also there. I think this is a belated but smart move by OpenAI. They are basically fully moving in on Meta's strategy now, taking advantage of what may be a temporary situation with Meta dropping back in model race. It will be interesting to see if these models now get taken up by the local model / fine tuning community the way llama was. It's a very appealing strategy to test / dev with a local model and then have the option to deploy to prod on a high powered version of the same thing. Always knowing if the provider goes full hostile, or you end up with data that can't move off prem, you have self hosting as an option with a decent performing model. Which is all to say, availability of these local models for me is a key incentive that I didn't have before to use OpenAI's hosted ones. What a day! Models aside, the Harmony Response Format[1] also seems pretty interesting and I wonder how much of an impact it might have in performance of these models. [1] https://github.com/openai/harmony Wow, today is a crazy AI release day: - OAI open source - Opus 4.1 - Genie 3 - ElevenLabs Music I'm out of the loop for local models. For my M3 24gb ram macbook, what token throughput can I expect? Edit: I tried it out, I have no idea in terms of of tokens but it was fluid enough for me. A bit slower than using o3 in the browser but definitely tolerable. I think I will set it up in my GF's machine so she can stop paying for the full subscription (she's a non-tech professional) The 120B model badly hallucinates facts on the level of a 0.6B model. My go to test for checking hallucinations is 'Tell me about Mercantour park' (a national park in south eastern France). Easily half of the facts are invented. Non-existing mountain summits, brown bears (no, there are none), villages that are elsewhere, wrong advice ('dogs allowed' - no they are not). Why do companies release open source LLMs? I would understand it, if there was some technology lock-in. But with LLMs, there is no such thing. One can switch out LLMs without any friction. > To improve the safety of the model, we filtered the data for harmful content in pre-training, especially around hazardous biosecurity knowledge, by reusing the CBRN pre-training filters from GPT-4o. Our model has a knowledge cutoff of June 2024. This would be a great ""AGI"" test. See if it can derive biohazards from first principles I love how they frame High-end desktops and laptops as having ""a single H100 GPU"". Can't wait to see third party benchmarks. The ones in the blog post are quite sparse and it doesn't seem possible to fully compare to other open models yet. But the few numbers available seem to suggest that this release will make all other non-multimodal open models obsolete. It seems like OSS will win, I can't see people willing to pay like 10x the price for what seems like 10% more performance. Especially once we get better at routing the hardest questions to the better models and then using that response to augment/fine-tune the OSS ones. I did a quick `openai/gpt-oss-20b` testing on an Macbook Pro M1 16GB. Pretty impressed with it so far. * It seems that using version @lmstudio's 20B gguf version ( https://huggingface.co/lmstudio-community/gpt-oss-20b-GGUF ) will have options for reasoning effort. * My MBP M1 16GB config: temp 0.8, max content length 7990, GPU offload 8/24, runs slow and still fine for me. * I tried testing with MCP with the above config, with basic tools like time and fetch + reasoning effort low, and the tool calls instruction follow is quite good. * In LM Studio's Developer tab there is a log output about the model information which is useful to learn. Overall, I like the way OpenAI backs to being Open AI, again, after all those years. -- Shameless plug, If anyone want to try out gpt-oss-120b and gpt-oss-20b as alternative to their own demo page [0], I have added both models with OpenRouter providers in VT Chat [1] as real product. You can try with an OpenRouter API Key. [0] https://gpt-oss.com [1] https://vtchat.io.vn Does anyone get the demos at https://www.gpt-oss.com to work, or are the servers down immediately after launch? I'm only getting the spinner after prompting. What's the best agent to run this on? Is it compatible with Codex? For OSS agents, I've been using Qwen Code (clunky fork of Gemini), and Goose. Shoutout to the hn consensus regarding an OpenAI open model release from 4 days ago: https://news.ycombinator.com/item?id=44758511 Why would OpenAI give this away for free? Is it to disrupt competition by setting a floor at the lower end of the market and make it harder for new competition to emerge while still retaining mind share? This is an extremely welcome move in a good direction from OpenAI. I can only thank them for all of the extra work around the models - Harmony structure, metal/torch/triton implementations, inference guides, cookbooks & fine-tuning/reinforcement learning scripts, datasets etc. There is an insane amount of helpful information buried in this release Super shallow (24/36 layers) MoE with low active parameter counts (3.6B/5.1B), a tradeoff between inference speed and performance. Text only, which is okay. Weights partially in MXFP4, but no cuda kernel support for RTX 50 series (sm120). Why? This is a NO for me. Safety alignment shifts from off the charts to off the rails really fast if you keep prompting. This is a NO for me. In summary, a solid NO for me. > We introduce gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models available under the Apache 2.0 license and our gpt-oss usage policy. [0] Is it even valid to have additional restriction on top of Apache 2.0? [0]: https://openai.com/index/gpt-oss-model-card/ Releasing this under the Apache license is a shot at competitors that want to license their models on Open Router and enterprise. It eliminates any reason to use an inferior Meta or Chinese model that costs money to license, thus there are no funds for these competitors to build a GPT 5 competitor. Newbie question: I remember folks talking about how kimi 2’s launch might have pushed OpenAI to launch their model later. Now that we (shortly will) know how this model performs, how do they stack up? Did openAI likely actually hold off releasing weights because of kimi, in retrospect? Wow, this will eat Meta's lunch I dont see the unsloth files yet but they'll be here: https://huggingface.co/unsloth/gpt-oss-20b-GGUF Super excited to test these out. The benchmarks from 20B are blowing away major >500b models. Insane. On my hardware. 43 tokens/sec. I got an error with flash attention turning on. Cant run it with flash attention? 31,000 context is max it will allow or model wont load. no kv or v quantization. Shameless plug: if someone wants to try it in a nice ui, you could give Msty[1] a try. It's private and local. [1]: https://msty.ai Big picture, what's the balance going to look like, going forward between what normal people can run on a fancy computer at home vs heavy duty systems hosted in big data centers that are the exclusive domain of Big Companies? This is something about AI that worries me, a 'child' of the open source coming of age era in the 90ies. I don't want to be forced to rely on those big companies to do my job in an efficient way, if AI becomes part of the day to day workflow. Wow I really didn’t think this would happen any time soon, they seem to have more to lose than to gain. If you’re a company building AI into your product right now I think you would be irresponsible to not investigate how much you can do on open weights models. The big AI labs are going to pull the ladder up eventually, building your business on the APIs long term is foolish. These open models will always be there for you to run though (if you can get GPUs anyway)."
44826997,GPT-5,https://www.youtube.com/watch?v=0Uu_VJeVVfo,https://openai.com/gpt-5/,rd,2063,2025-08-07T17:00:21+00:00,,query,50,"It is frequently suggested that once one of the AI companies reaches an AGI threshold, they will take off ahead of the rest. It's interesting to note that at least so far, the trend has been the opposite: as time goes on and the models get better, the performance of the different company's gets clustered closer together. Right now GPT-5, Claude Opus, Grok 4, Gemini 2.5 Pro all seem quite good across the board (ie they can all basically solve moderately challenging math and coding problems). As a user, it feels like the race has never been as close as it is now. Perhaps dumb to extrapolate, but it makes me lean more skeptical about the hard take-off / winner-take-all mental model that has been pushed. Would be curious to hear the take of a researcher at one of these firms - do you expect the AI offerings across competitors to become more competitive and clustered over the next few years, or less so? GPT-5 knowledge cutoff: Sep 30, 2024 (10 months before release). Compare that to Gemini 2.5 Pro knowledge cutoff: Jan 2025 (3 months before release) Claude Opus 4.1: knowledge cutoff: Mar 2025 (4 months before release) https://platform.openai.com/docs/models/compare https://deepmind.google/models/gemini/pro/ https://docs.anthropic.com/en/docs/about-claude/models/overv... Going by the system card at: https://openai.com/index/gpt-5-system-card/ > GPT‑5 is a unified system . . . OK > . . . with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say “think hard about this” in the prompt). So that's not really a unified system then, it's just supposed to appear as if it is. This looks like they're not training the single big model but instead have gone off to develop special sub models and attempt to gloss over them with yet another model. That's what you resort to only when doing the end-to-end training has become too expensive for you. I'm not really convinced, the benchmark blunder was really strange but the demos were quite underwhelming, and it appears this was reflected by a huge market correction in the betting markets as to who will have the best AI by end of the year. What excites me now is that Gemini 3.0 or some answer from Google is coming soon and that will be the one I will actually end up using. It seems like the last mover in the LLM race is more advantageous. The marketing copy and the current livestream appear tautological: ""it's better because it's better."" Not much explanation yet why GPT-5 warrants a major version bump. As usual, the model (and potentially OpenAI as a whole) will depend on output vibe checks. Watching the livestream now, the improvement over their current models on the benchmarks is very small. I know they seemed to be trying to temper our expectations leading up to this, but this is much less improvement than I was expecting What's going on with this plot's y-axis? https://bsky.app/profile/tylermw.com/post/3lvtac5hues2n Some people have hypothesized that GPT-5 is actually about cost reduction and internal optimization for OpenAI, since there doesn't seem to be much of a leap forward, but another element that they seem to have focused on that'll probably make a huge difference to ""normal"" (non-tech) users is making precise and specifically worded prompts less necessary. They've mentioned improvements in that aspects a few times now, and if it actually materializes, that would be a big leap forward for most users even if underneath GPT-4 was also technically able to do the same things if prompted just the right way. Ok this[0] sounds very, uh bold to me? Surely this is going to break a ton of workflows etc seemingly with nearly no notice? I'm assuming 'launches' equates with 'fully rolls out' or something but it's not that clear to me. When GPT-5 launches, several older models will be retired, including: - GPT-4o - GPT-4.1 - GPT-4.5 - GPT-4.1-mini - o4-mini - o4-mini-high - o3 - o3-pro If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and chats with o3-Pro will open in GPT-5-Pro (available only on Pro and Team). [0] https://help.openai.com/en/articles/11909943-gpt-5-in-chatgp... > 400,000 context window > 128,000 max output tokens > Input $1.25 > Output $10.00 Source: https://platform.openai.com/docs/models/gpt-5 If this performs well in independent needle-in-haystack and adherence evaluations, this pricing with this context window alone would make GPT-5 extremely competitive with Gemini 2.5 Pro and Claude Opus 4.1, even if the output isn't a significant improvement over o3. If the output quality ends up on-par or better than the two major competitors, that'd be truly a massive leap forward for OpenAI, mini and nano maybe even more so. ChatGPT5 in this demo: > For an airplane wing (airfoil), the top surface is curved and the bottom is flatter. When the wing moves forward: > * Air over the top has to travel farther in the same amount of time -> it moves faster -> pressure on the top decreases. > * Air underneath moves slower -> pressure underneath is higher > * The presure difference creates an upward force - lift Isn't that explanation of why wings work completely wrong? There's nothing that forces the air to cover the top distance in the same time that it covers the bottom distance, and in fact it doesn't. https://www.cam.ac.uk/research/news/how-wings-really-work Very strange to use a mistake as your first demo, especially while talking about how it's phd level. They will retire lots of models: GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro. https://help.openai.com/en/articles/6825453-chatgpt-release-... ""If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent."" - 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high => GPT-5 - o3 => GPT-5-Thinking - o3-Pro => GPT-5-Pro That SWE-bench chart with the mismatched bars (52.8% somehow appearing larger than 69.1%) was emblematic of the entire presentation - rushed and underwhelming. It's the kind of error that would get flagged in any internal review, yet here it is in a billion-dollar product launch. Combined with the Bernoulli effect demo confidently explaining how airplane wings work incorrectly (the equal transit time fallacy that NASA explicitly debunks), it doesn't inspire confidence in either the model's capabilities or OpenAI's quality control. The actual benchmark improvements are marginal at best - we're talking single-digit percentage gains over o3 on most metrics, which hardly justifies a major version bump. What we're seeing looks more like the plateau of an S-curve than a breakthrough. The pricing is competitive ($1.25/1M input tokens vs Claude's $15), but that's about optimization and economics, not the fundamental leap forward that ""GPT-5"" implies. Even their ""unified system"" turns out to be multiple models with a router, essentially admitting that the end-to-end training approach has hit diminishing returns. The irony is that while OpenAI maintains their secretive culture (remember when they claimed o1 used tree search instead of RL?), their competitors are catching up or surpassing them. Claude has been consistently better for coding tasks, Gemini 2.5 Pro has more recent training data, and everyone seems to be converging on similar performance levels. This launch feels less like a victory lap and more like OpenAI trying to maintain relevance while the rest of the field has caught up. Looking forward to seeing what Gemini 3.0 brings to the table. For day to day coding, I've found Anthropic to be killing it with Sonnet 3.7 and now Sonnet 4, and Claude Code feeling like it has even bigger advantages over when it's used in Cursor (And I can't explain why). I don't even try to use the OpenAI models because it's felt like night and day. Hopefully GPT-5 helps them catch up. Although I'm sure there are 100 people that have their own personal ""hopefully GPT-5 fixes my personal issue with GPT4"" I am thoroughly unimpressed by GPT-5. It still can't compose iambic trimeters in ancient Greek with a proper penthemimeral cæsura, and it insists on providing totally incorrect scansion of the flawed lines it does compose. I corrected its metrical sins twice, which sent it into ""thinking"" mode until it finally returned a ""Reasoning failed"" error. There is no intelligence here: it's still just giving plausible output. That's why it can't metrically scan its own lines or put a cæsura in the right place. Pricing seems good, but the open question is still on tool calling reliability. Input: $1.25 / 1M tokens Cached: $0.125 / 1M tokens Output: $10 / 1M tokens With 74.9% on SWE-bench, this inches out Claude Opus 4.1 at 74.5%, but at a much cheaper cost. For context, Claude Opus 4.1 is $15 / 1M input tokens and $75 / 1M output tokens. > ""GPT-5 will scaffold the app, write files, install dependencies as needed, and show a live preview. This is the go-to solution for developers who want to bootstrap apps or add features quickly."" [0] Since Claude Code launched, OpenAI has been behind. Maybe the RL on tool calling is good enough to be competitive now? [0] https://github.com/openai/gpt-5-coding-examples ‘Twas the night before GPT-5, when all through the social-media-sphere, Not a creature was posting, not even @paulg nor @eshear Next morning’s posts were prepped and scheduled with care, In hopes that AGI soon would appear … What's going on with their SWE bench graph?[0] GPT-5 non-thinking is labeled 52.8% accuracy, but o3 is shown as a much shorter bar, yet it's labeled 69.1%. And 4o is an identical bar to o3, but it's labeled 30.8%... [0] https://i.postimg.cc/DzkZZLry/y-axis.png Wait, isn't the Bernoulli effect thing they're demoing now wrong? I thought that was a ""common misconception"" and wings don't really work by the ""longer path"" that air takes over the top, and that it was more about angle of attack (which is why planes can fly upside down). It seems like it's actually an ideal ""trick"" question for an LLM actually, since so much content has been written about it incorrectly. I thought at first they were going to demo this to show that it knew better, but it seems like it's just regurgitating the same misleading stuff. So, not a good look. They really nerfed Plus[0]. 80 messages every 3 hours for normal GPT-5. And only 200 messages per week for GPT-5 Thinking. It seems like terrible value. Before it was: 100 o3 per week 100 o4-mini-high per day 300 o4-mini per day 50 4.5 per week [0] https://help.openai.com/en/articles/11909943-gpt-5-in-chatgp... Anecdotally, as someone who operates in a very large legacy codebase, I am very impressed by GPT-5's agentic abilities so far. I've given it the same tasks I've given Claude and previous iterations via the Codex CLI, and instead of getting loss due to the massive scope of the problem, it correctly identifies the large scope and breaks it down into it's correct parts and creates the correct plan and begins executing. I am wildly impressed. I do not believe that the 0.x% increase in benchmarks tell the story of this release at all. Something that's really hitting me is something brought up in this piece: https://www.interconnects.ai/p/gpt-5-and-bending-the-arc-of-... When a model comes out, I usually think about it in terms of my own use. This is largely agentic tooling, and I mostly us Claude Code. All the hallucination and eval talk doesn't really catch me because I feel like I'm getting value of these tools today. However, this model is not _for_ me in the same way models normally are. This is for the 800m or whatever people that open up chatgpt every day and type stuff in. All of them have been stuck on GPT-4o unbeknwst to them. They had no idea SOTA was far beyond that. They probably dont even know that there is a ""model"" at all. But for all these people, they just got a MAJOR upgrade. It will probably feel like turning the lights on for these people, who have been using a subpar model for the past year. That said I'm also giving GPT-5 a run in Codex and it's doing a pretty good job! The eval bar I want to see here is simple: over a complex objective (e.g., deploy to prod using a git workflow), how many tasks can GPT-5 stay on track with before it falls off the train. Context is king and it's the most obvious and glaring problem with current models. It's a really good model from my testing so far. You can see the difference in how it tries to use tools to the greatest extent when answering a question, especially compared to 4.1 and o3. In this example it used 6! tool calls in the first response to try and collect as much info as possible. https://promptslice.com/share/b-2ap_rfjeJgIQsG The silent victory here is this seems like it is being built to be faster and cheaper than o3 while presenting a reasonable jump, which is an important jump in scaling law On the other hand if it's just getting bigger and slower it's not a good sign for LLMs Does this mean AGI is cancelled? 2027 hard takeoff was just sci-fi? 74.9 SWEBench. This increases the SOTA by a whole .4%. Although the pricing is great, it doesn't seem like OpenAI found a giant breakthrough yet like o1 or Claude 3.5 Sonnet # GPT5 all official links Livestream link: https://www.youtube.com/live/0Uu_VJeVVfo Research blog post: https://openai.com/index/introducing-gpt-5/ Developer blog post: https://openai.com/index/introducing-gpt-5-for-developers API Docs: https://platform.openai.com/docs/guides/latest-model Note the free form function calling documentation: https://platform.openai.com/docs/guides/function-calling#con... GPT5 prompting guide: https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_g... GPT5 new params and tools: https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_... GPT5 frontend cookbook: https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend prompt migrator/optimizor https://platform.openai.com/chat/edit?optimize=true Enterprise blog post: https://openai.com/index/gpt-5-new-era-of-work System Card: https://openai.com/index/gpt-5-system-card/ What would you say if you could talk to a future OpenAI model? https://progress.openai.com/ coding examples: https://github.com/openai/gpt-5-coding-examples Note it's not available to everyone yet: > GPT-5 Rollout > We are gradually rolling out GPT-5 to ensure stability during launch. Some users may not yet see GPT-5 in their account as we increase availability in stages. Wish they would stop mentioning AGI. It's like the creator of a new car claiming it's a step closer to teleportation. Is anyone else having problems with factual correctness? I had a number of 4o and o3 conversations going and those models were factually correct about a number of different subjects. Asking GPT-5 about the same things results in wrong answers even though its training data is newer. And it won't look things up to correct itself unless I manually switch to the thinking variant. This is worse. I cancelled my subscription. SWE-Bench Verified score, with thinking, ties Opus 4.1 without thinking. AIME scores do not appear too impressive at first glance. They are downplaying benchmarks heavily in the live stream. This was the lab that has been flexing benchmarks as headline figures since forever. This is a product-focused update. There is no significant jump in raw intelligence or agentic behavior against SOTA. These presenters all give off such a “sterile” vibe Seems LLMs really hit the wall. Very funny. The very first answer it gave to illustrate its ""Expert knowledge"" is quite common, and it's wrong. What's even funnier is that you can find why on Wikipedia: https://en.wikipedia.org/wiki/Lift_(force)#False_explanation... What's terminally funny is that in the visualisation app, it used a symmetric wing, which of course wouldn't generate lift according to its own explanation (as the travelled distance and hence air flow speed would be the same). I work as a game physics programmer, so I noticed that immediately and almost laughed. I watched only that part so far while I was still at the office, though. Sam Altman, in the summer update video: > ""[GPT-5] can write an entire computer program from scratch, to help you with whatever you'd like. And we think this idea of software on demand is going to be one of the defining characteristics of the GPT-5 era."" GPT-5 set a new record on my Confabulations on Provided Texts benchmark: https://github.com/lechmazur/confabulations/ LLMs hitting a wall would be incredible. We could actually start building on the tech we have. Answer in one word: Underwhelming. Bad data on graphs, demos that would have been impressive a year ago, vibe coding the easiest requests (financial dashboard), running out of talking points while cursor is looping on a bug, marginal benchmark improvements. At least the models are kind of cheaper to run. Did they just say they're deprecating all of OpenAI's non-GPT-5 models? I know HN isn’t the place to go for positive, uplifting commentary or optimism about technology - but I am truly excited for this release and grateful to all the team members who made it possible. What a great time to be alive. It's very interesting how memetic the language around different models is. Elon seems to have coined ""PhD level intelligence in all topics"" and now Sam repeated it in his presentation. Despite it not having an actual meaning. I think OpenAI will coin they've achieved AGI first (as they have incentives to based on the rumored contract with MSFT), and then everyone will claim we've achieved it. It's very unclear if OpenAI has been casually leaking things to create buzz, but a few days ago there was a pretty stunning pelican on a bike attempt: https://old.reddit.com/r/OpenAI/comments/1mettre/gpt5_is_alr... In practice, it's very clear to me that the most important value in writing software with an LLM isn't it's ability to one-shot hard problems, but rather it's ability to effectively manage complex context. There are no good evals for this kind of problem, but that's what I'm keenly interested in understanding. Show me GPT-5 can move through 10 steps in a list of tasks without completely losing the objective by the end. The reduction in hallucinations seems like potentially the biggest upgrade. If it reduces hallucinations by 75% or more over o3 and GPT-4o as the graphs claim, it will be a giant step forward. The inability to trust answers given by AI is the biggest single hurdle to clear for many applications. Already a lot of comments here (2188 at the time of this comment) but wanted to share my 2c: * It feels a bit more competent, as if it had more nuance or detail to say about each point. * It got a few obscure details about OpenBSD correct right away - both Sonnet 4 and 4o sometimes conflate Linux and OpenBSD commands. * It was fun asking GPT-5 to not only answer the query, but also to provide a brief analysis of the query itself for insights into myself! Not a detailed review, but just a couple things I noticed with some limited usage. The presentation asks for a moving svg to illustrate Bernoulli, that's suspiciously close to a Pelican. Whenever OpenAI releases a new ChatGPT feature or model, it's always a crapshoot when you'll actually be able to use it. The headlines - both from tech media coverage and OpenAI itself - always read ""now available"", but then I go to ChatGPT (and I'm a paid pro user) and it's not available yet. As an engineer I understand rollouts, but maybe don't say it's generally available when it's not? I ran the below prompt to both Kimi2 and GPT5. how many rs in cranberry? -- GPT5's response: The word cranberry has two “r”s. One in cran and one in berry. Kimi2's response: There are three letter rs in the word ""cranberry"". created a summary of comments from this thread about 15 hours after it had been posted and had 1983 comments, using gpt-5-high and gemini-2.5-pro using a prompt similar to simonw [1]. Used a Python script [2] that I wrote to generate the summary. - gpt-5-high summary: https://gist.github.com/primaprashant/1775eb97537362b049d643... - gemini-2.5-pro summary: https://gist.github.com/primaprashant/4d22df9735a1541263c671... [1]: https://news.ycombinator.com/item?id=43477622 [2]: https://gist.github.com/primaprashant/f181ed685ae563fd06c49d... Nay, laddie, that's no' the real AGI Scotsman. He's grander still! Wait til GPT-6 come out, you'll be blown away! https://idiallo.com/byte-size/ai-scotsman","GPT-5 https://www.youtube.com/watch?v=0Uu_VJeVVfo It is frequently suggested that once one of the AI companies reaches an AGI threshold, they will take off ahead of the rest. It's interesting to note that at least so far, the trend has been the opposite: as time goes on and the models get better, the performance of the different company's gets clustered closer together. Right now GPT-5, Claude Opus, Grok 4, Gemini 2.5 Pro all seem quite good across the board (ie they can all basically solve moderately challenging math and coding problems). As a user, it feels like the race has never been as close as it is now. Perhaps dumb to extrapolate, but it makes me lean more skeptical about the hard take-off / winner-take-all mental model that has been pushed. Would be curious to hear the take of a researcher at one of these firms - do you expect the AI offerings across competitors to become more competitive and clustered over the next few years, or less so? GPT-5 knowledge cutoff: Sep 30, 2024 (10 months before release). Compare that to Gemini 2.5 Pro knowledge cutoff: Jan 2025 (3 months before release) Claude Opus 4.1: knowledge cutoff: Mar 2025 (4 months before release) https://platform.openai.com/docs/models/compare https://deepmind.google/models/gemini/pro/ https://docs.anthropic.com/en/docs/about-claude/models/overv... Going by the system card at: https://openai.com/index/gpt-5-system-card/ > GPT‑5 is a unified system . . . OK > . . . with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say “think hard about this” in the prompt). So that's not really a unified system then, it's just supposed to appear as if it is. This looks like they're not training the single big model but instead have gone off to develop special sub models and attempt to gloss over them with yet another model. That's what you resort to only when doing the end-to-end training has become too expensive for you. I'm not really convinced, the benchmark blunder was really strange but the demos were quite underwhelming, and it appears this was reflected by a huge market correction in the betting markets as to who will have the best AI by end of the year. What excites me now is that Gemini 3.0 or some answer from Google is coming soon and that will be the one I will actually end up using. It seems like the last mover in the LLM race is more advantageous. The marketing copy and the current livestream appear tautological: ""it's better because it's better."" Not much explanation yet why GPT-5 warrants a major version bump. As usual, the model (and potentially OpenAI as a whole) will depend on output vibe checks. Watching the livestream now, the improvement over their current models on the benchmarks is very small. I know they seemed to be trying to temper our expectations leading up to this, but this is much less improvement than I was expecting What's going on with this plot's y-axis? https://bsky.app/profile/tylermw.com/post/3lvtac5hues2n Some people have hypothesized that GPT-5 is actually about cost reduction and internal optimization for OpenAI, since there doesn't seem to be much of a leap forward, but another element that they seem to have focused on that'll probably make a huge difference to ""normal"" (non-tech) users is making precise and specifically worded prompts less necessary. They've mentioned improvements in that aspects a few times now, and if it actually materializes, that would be a big leap forward for most users even if underneath GPT-4 was also technically able to do the same things if prompted just the right way. Ok this[0] sounds very, uh bold to me? Surely this is going to break a ton of workflows etc seemingly with nearly no notice? I'm assuming 'launches' equates with 'fully rolls out' or something but it's not that clear to me. When GPT-5 launches, several older models will be retired, including: - GPT-4o - GPT-4.1 - GPT-4.5 - GPT-4.1-mini - o4-mini - o4-mini-high - o3 - o3-pro If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and chats with o3-Pro will open in GPT-5-Pro (available only on Pro and Team). [0] https://help.openai.com/en/articles/11909943-gpt-5-in-chatgp... > 400,000 context window > 128,000 max output tokens > Input $1.25 > Output $10.00 Source: https://platform.openai.com/docs/models/gpt-5 If this performs well in independent needle-in-haystack and adherence evaluations, this pricing with this context window alone would make GPT-5 extremely competitive with Gemini 2.5 Pro and Claude Opus 4.1, even if the output isn't a significant improvement over o3. If the output quality ends up on-par or better than the two major competitors, that'd be truly a massive leap forward for OpenAI, mini and nano maybe even more so. ChatGPT5 in this demo: > For an airplane wing (airfoil), the top surface is curved and the bottom is flatter. When the wing moves forward: > * Air over the top has to travel farther in the same amount of time -> it moves faster -> pressure on the top decreases. > * Air underneath moves slower -> pressure underneath is higher > * The presure difference creates an upward force - lift Isn't that explanation of why wings work completely wrong? There's nothing that forces the air to cover the top distance in the same time that it covers the bottom distance, and in fact it doesn't. https://www.cam.ac.uk/research/news/how-wings-really-work Very strange to use a mistake as your first demo, especially while talking about how it's phd level. They will retire lots of models: GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro. https://help.openai.com/en/articles/6825453-chatgpt-release-... ""If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent."" - 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high => GPT-5 - o3 => GPT-5-Thinking - o3-Pro => GPT-5-Pro That SWE-bench chart with the mismatched bars (52.8% somehow appearing larger than 69.1%) was emblematic of the entire presentation - rushed and underwhelming. It's the kind of error that would get flagged in any internal review, yet here it is in a billion-dollar product launch. Combined with the Bernoulli effect demo confidently explaining how airplane wings work incorrectly (the equal transit time fallacy that NASA explicitly debunks), it doesn't inspire confidence in either the model's capabilities or OpenAI's quality control. The actual benchmark improvements are marginal at best - we're talking single-digit percentage gains over o3 on most metrics, which hardly justifies a major version bump. What we're seeing looks more like the plateau of an S-curve than a breakthrough. The pricing is competitive ($1.25/1M input tokens vs Claude's $15), but that's about optimization and economics, not the fundamental leap forward that ""GPT-5"" implies. Even their ""unified system"" turns out to be multiple models with a router, essentially admitting that the end-to-end training approach has hit diminishing returns. The irony is that while OpenAI maintains their secretive culture (remember when they claimed o1 used tree search instead of RL?), their competitors are catching up or surpassing them. Claude has been consistently better for coding tasks, Gemini 2.5 Pro has more recent training data, and everyone seems to be converging on similar performance levels. This launch feels less like a victory lap and more like OpenAI trying to maintain relevance while the rest of the field has caught up. Looking forward to seeing what Gemini 3.0 brings to the table. For day to day coding, I've found Anthropic to be killing it with Sonnet 3.7 and now Sonnet 4, and Claude Code feeling like it has even bigger advantages over when it's used in Cursor (And I can't explain why). I don't even try to use the OpenAI models because it's felt like night and day. Hopefully GPT-5 helps them catch up. Although I'm sure there are 100 people that have their own personal ""hopefully GPT-5 fixes my personal issue with GPT4"" I am thoroughly unimpressed by GPT-5. It still can't compose iambic trimeters in ancient Greek with a proper penthemimeral cæsura, and it insists on providing totally incorrect scansion of the flawed lines it does compose. I corrected its metrical sins twice, which sent it into ""thinking"" mode until it finally returned a ""Reasoning failed"" error. There is no intelligence here: it's still just giving plausible output. That's why it can't metrically scan its own lines or put a cæsura in the right place. Pricing seems good, but the open question is still on tool calling reliability. Input: $1.25 / 1M tokens Cached: $0.125 / 1M tokens Output: $10 / 1M tokens With 74.9% on SWE-bench, this inches out Claude Opus 4.1 at 74.5%, but at a much cheaper cost. For context, Claude Opus 4.1 is $15 / 1M input tokens and $75 / 1M output tokens. > ""GPT-5 will scaffold the app, write files, install dependencies as needed, and show a live preview. This is the go-to solution for developers who want to bootstrap apps or add features quickly."" [0] Since Claude Code launched, OpenAI has been behind. Maybe the RL on tool calling is good enough to be competitive now? [0] https://github.com/openai/gpt-5-coding-examples ‘Twas the night before GPT-5, when all through the social-media-sphere, Not a creature was posting, not even @paulg nor @eshear Next morning’s posts were prepped and scheduled with care, In hopes that AGI soon would appear … What's going on with their SWE bench graph?[0] GPT-5 non-thinking is labeled 52.8% accuracy, but o3 is shown as a much shorter bar, yet it's labeled 69.1%. And 4o is an identical bar to o3, but it's labeled 30.8%... [0] https://i.postimg.cc/DzkZZLry/y-axis.png Wait, isn't the Bernoulli effect thing they're demoing now wrong? I thought that was a ""common misconception"" and wings don't really work by the ""longer path"" that air takes over the top, and that it was more about angle of attack (which is why planes can fly upside down). It seems like it's actually an ideal ""trick"" question for an LLM actually, since so much content has been written about it incorrectly. I thought at first they were going to demo this to show that it knew better, but it seems like it's just regurgitating the same misleading stuff. So, not a good look. They really nerfed Plus[0]. 80 messages every 3 hours for normal GPT-5. And only 200 messages per week for GPT-5 Thinking. It seems like terrible value. Before it was: 100 o3 per week 100 o4-mini-high per day 300 o4-mini per day 50 4.5 per week [0] https://help.openai.com/en/articles/11909943-gpt-5-in-chatgp... Anecdotally, as someone who operates in a very large legacy codebase, I am very impressed by GPT-5's agentic abilities so far. I've given it the same tasks I've given Claude and previous iterations via the Codex CLI, and instead of getting loss due to the massive scope of the problem, it correctly identifies the large scope and breaks it down into it's correct parts and creates the correct plan and begins executing. I am wildly impressed. I do not believe that the 0.x% increase in benchmarks tell the story of this release at all. Something that's really hitting me is something brought up in this piece: https://www.interconnects.ai/p/gpt-5-and-bending-the-arc-of-... When a model comes out, I usually think about it in terms of my own use. This is largely agentic tooling, and I mostly us Claude Code. All the hallucination and eval talk doesn't really catch me because I feel like I'm getting value of these tools today. However, this model is not _for_ me in the same way models normally are. This is for the 800m or whatever people that open up chatgpt every day and type stuff in. All of them have been stuck on GPT-4o unbeknwst to them. They had no idea SOTA was far beyond that. They probably dont even know that there is a ""model"" at all. But for all these people, they just got a MAJOR upgrade. It will probably feel like turning the lights on for these people, who have been using a subpar model for the past year. That said I'm also giving GPT-5 a run in Codex and it's doing a pretty good job! The eval bar I want to see here is simple: over a complex objective (e.g., deploy to prod using a git workflow), how many tasks can GPT-5 stay on track with before it falls off the train. Context is king and it's the most obvious and glaring problem with current models. It's a really good model from my testing so far. You can see the difference in how it tries to use tools to the greatest extent when answering a question, especially compared to 4.1 and o3. In this example it used 6! tool calls in the first response to try and collect as much info as possible. https://promptslice.com/share/b-2ap_rfjeJgIQsG The silent victory here is this seems like it is being built to be faster and cheaper than o3 while presenting a reasonable jump, which is an important jump in scaling law On the other hand if it's just getting bigger and slower it's not a good sign for LLMs Does this mean AGI is cancelled? 2027 hard takeoff was just sci-fi? 74.9 SWEBench. This increases the SOTA by a whole .4%. Although the pricing is great, it doesn't seem like OpenAI found a giant breakthrough yet like o1 or Claude 3.5 Sonnet # GPT5 all official links Livestream link: https://www.youtube.com/live/0Uu_VJeVVfo Research blog post: https://openai.com/index/introducing-gpt-5/ Developer blog post: https://openai.com/index/introducing-gpt-5-for-developers API Docs: https://platform.openai.com/docs/guides/latest-model Note the free form function calling documentation: https://platform.openai.com/docs/guides/function-calling#con... GPT5 prompting guide: https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_g... GPT5 new params and tools: https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_... GPT5 frontend cookbook: https://cookbook.openai.com/examples/gpt-5/gpt-5_frontend prompt migrator/optimizor https://platform.openai.com/chat/edit?optimize=true Enterprise blog post: https://openai.com/index/gpt-5-new-era-of-work System Card: https://openai.com/index/gpt-5-system-card/ What would you say if you could talk to a future OpenAI model? https://progress.openai.com/ coding examples: https://github.com/openai/gpt-5-coding-examples Note it's not available to everyone yet: > GPT-5 Rollout > We are gradually rolling out GPT-5 to ensure stability during launch. Some users may not yet see GPT-5 in their account as we increase availability in stages. Wish they would stop mentioning AGI. It's like the creator of a new car claiming it's a step closer to teleportation. Is anyone else having problems with factual correctness? I had a number of 4o and o3 conversations going and those models were factually correct about a number of different subjects. Asking GPT-5 about the same things results in wrong answers even though its training data is newer. And it won't look things up to correct itself unless I manually switch to the thinking variant. This is worse. I cancelled my subscription. SWE-Bench Verified score, with thinking, ties Opus 4.1 without thinking. AIME scores do not appear too impressive at first glance. They are downplaying benchmarks heavily in the live stream. This was the lab that has been flexing benchmarks as headline figures since forever. This is a product-focused update. There is no significant jump in raw intelligence or agentic behavior against SOTA. These presenters all give off such a “sterile” vibe Seems LLMs really hit the wall. Very funny. The very first answer it gave to illustrate its ""Expert knowledge"" is quite common, and it's wrong. What's even funnier is that you can find why on Wikipedia: https://en.wikipedia.org/wiki/Lift_(force)#False_explanation... What's terminally funny is that in the visualisation app, it used a symmetric wing, which of course wouldn't generate lift according to its own explanation (as the travelled distance and hence air flow speed would be the same). I work as a game physics programmer, so I noticed that immediately and almost laughed. I watched only that part so far while I was still at the office, though. Sam Altman, in the summer update video: > ""[GPT-5] can write an entire computer program from scratch, to help you with whatever you'd like. And we think this idea of software on demand is going to be one of the defining characteristics of the GPT-5 era."" GPT-5 set a new record on my Confabulations on Provided Texts benchmark: https://github.com/lechmazur/confabulations/ LLMs hitting a wall would be incredible. We could actually start building on the tech we have. Answer in one word: Underwhelming. Bad data on graphs, demos that would have been impressive a year ago, vibe coding the easiest requests (financial dashboard), running out of talking points while cursor is looping on a bug, marginal benchmark improvements. At least the models are kind of cheaper to run. Did they just say they're deprecating all of OpenAI's non-GPT-5 models? I know HN isn’t the place to go for positive, uplifting commentary or optimism about technology - but I am truly excited for this release and grateful to all the team members who made it possible. What a great time to be alive. It's very interesting how memetic the language around different models is. Elon seems to have coined ""PhD level intelligence in all topics"" and now Sam repeated it in his presentation. Despite it not having an actual meaning. I think OpenAI will coin they've achieved AGI first (as they have incentives to based on the rumored contract with MSFT), and then everyone will claim we've achieved it. It's very unclear if OpenAI has been casually leaking things to create buzz, but a few days ago there was a pretty stunning pelican on a bike attempt: https://old.reddit.com/r/OpenAI/comments/1mettre/gpt5_is_alr... In practice, it's very clear to me that the most important value in writing software with an LLM isn't it's ability to one-shot hard problems, but rather it's ability to effectively manage complex context. There are no good evals for this kind of problem, but that's what I'm keenly interested in understanding. Show me GPT-5 can move through 10 steps in a list of tasks without completely losing the objective by the end. The reduction in hallucinations seems like potentially the biggest upgrade. If it reduces hallucinations by 75% or more over o3 and GPT-4o as the graphs claim, it will be a giant step forward. The inability to trust answers given by AI is the biggest single hurdle to clear for many applications. Already a lot of comments here (2188 at the time of this comment) but wanted to share my 2c: * It feels a bit more competent, as if it had more nuance or detail to say about each point. * It got a few obscure details about OpenBSD correct right away - both Sonnet 4 and 4o sometimes conflate Linux and OpenBSD commands. * It was fun asking GPT-5 to not only answer the query, but also to provide a brief analysis of the query itself for insights into myself! Not a detailed review, but just a couple things I noticed with some limited usage. The presentation asks for a moving svg to illustrate Bernoulli, that's suspiciously close to a Pelican. Whenever OpenAI releases a new ChatGPT feature or model, it's always a crapshoot when you'll actually be able to use it. The headlines - both from tech media coverage and OpenAI itself - always read ""now available"", but then I go to ChatGPT (and I'm a paid pro user) and it's not available yet. As an engineer I understand rollouts, but maybe don't say it's generally available when it's not? I ran the below prompt to both Kimi2 and GPT5. how many rs in cranberry? -- GPT5's response: The word cranberry has two “r”s. One in cran and one in berry. Kimi2's response: There are three letter rs in the word ""cranberry"". created a summary of comments from this thread about 15 hours after it had been posted and had 1983 comments, using gpt-5-high and gemini-2.5-pro using a prompt similar to simonw [1]. Used a Python script [2] that I wrote to generate the summary. - gpt-5-high summary: https://gist.github.com/primaprashant/1775eb97537362b049d643... - gemini-2.5-pro summary: https://gist.github.com/primaprashant/4d22df9735a1541263c671... [1]: https://news.ycombinator.com/item?id=43477622 [2]: https://gist.github.com/primaprashant/f181ed685ae563fd06c49d... Nay, laddie, that's no' the real AGI Scotsman. He's grander still! Wait til GPT-6 come out, you'll be blown away! https://idiallo.com/byte-size/ai-scotsman"
40447431,Leaked OpenAI documents reveal aggressive tactics toward former employees,nan,https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees,apengwin,1791,2024-05-22T22:22:30+00:00,,query,48,"If this really was a mistake the easiest way to deal with it would be to release people from their non disparagement agreements that were only signed by leaving employees under the duress of losing their vested equity. It's really easy to make people whole for this, so whether that happens or not is the difference between the apologies being real or just them just backpedaling because employees got upset. Edit: Looks like they're doing the right thing here: > Altman’s initial statement was criticized for doing too little to make things right for former employees, but in an emailed statement, OpenAI told me that “we are identifying and reaching out to former employees who signed a standard exit agreement to make it clear that OpenAI has not and will not cancel their vested equity and releases them from nondisparagement obligations” — which goes much further toward fixing their mistake. The amount [and scale] of practices, chaos and controversies caused by OpenAI since ChatGPT was released are ""on par"" with the powerful products it has built since.. in a negative way! These are the hottest controversial events so far , in a chronological order: OpenAI's deviation from its original mission (https://news.ycombinator.com/item?id=34979981). The Altman's Saga (https://news.ycombinator.com/item?id=38309611). The return of Altman (within a week) (https://news.ycombinator.com/item?id=38375239). Musk vs. OpenAI (https://news.ycombinator.com/item?id=39559966). The departure of high-profile employees (Karpathy: https://news.ycombinator.com/item?id=39365935 ,Sutskever: https://news.ycombinator.com/item?id=40361128). ""Why can’t former OpenAI employees talk?"" (https://news.ycombinator.com/item?id=40393121). Great, if these documents are credible, this is exactly what I was implying[1] yesteday. Here, listen to Altman say how he is ""genuinely embarrassed"": ""this is on me and one of the few times i've been genuinely embarrassed running openai; i did not know this was happening and i should have."" The first thing the above conjures up is the other disgraced Sam (Bankman-Fried) saying ""this is on me"" when FTX went bust. I bet euros-to-croissants I'm not the only one to notice this. Some amount of corporate ruthlessness is part of the game, whether we like it or not. But these SV robber-barrons really crank it up to something else. [1] https://news.ycombinator.com/item?id=40425735 Looking forward for a document leak about openai using YouTube data for training their models. When asked if they use it, Murali (CTO) told she doesn't know which makes you believe that for 99% they are using it. I find it hard to believe that Sam didn’t know about something that draconian in something so sensitive as NDAs that affects to equity. He’s not exactly new to this whole startup thing and getting equity right is not a small part of that So what happened to Daniel Kokotajlo, the ex-OAI employee who made a comment saying that his equity was clawed back? Was it a miscommunication and he was referring to unvested equity, or is Sama just lying? In the original context, it sounded very much like he was referring to clawed-back equity. I’m trying to find the link. > ..or agreeing not to criticize the company, with no end date Oh! free speech is on trade! We used to hear the above statement coming from some political regimes but this is the first time I read it in the tech world. Would we live to witness more variations of this behavior on a larger scale?! > High-pressure tactics at OpenAI > That meant the former employees had a week to decide whether to accept OpenAI’s muzzle or risk forfeiting what could be millions of dollars > When ex-employees asked for more time to seek legal aid and review the documents, they faced significant pushback from OpenAI. > “We want to make sure you understand that if you don't sign, it could impact your equity. That's true for everyone, and we're just doing things by the book,” Although they've been able to build the most capable AI models that could replace a lot of human jobs, they struggle to humanely manage the people behind these models!! PDF: https://s3.documentcloud.org/documents/24679729/aestas_reduc... Going to be hard to keep claiming you didn’t know something, if your signature is on it. I don’t really think a CEO gets to say he didn’t read what he was signing. Does someone know why the employees wanted him back so badly? Must be very few employees actually upset with him and his way of doing things. In my third world country, when they do something unethical they say ""everything is in accordance with the law"", here it's ""this is on me"", both are very cynical. From the time they went private, it was apparent that this company is unethical to say the least. Given what it is building, this can be very dangerous, but I think they are more proficient in creating hype, than actually coming up with something meaningful. It's funny how finding out about corporate misdoing has almost a common ritual attached to it. First shock and dismay is expressed to the findings, then the company leadership has to say it was a mistake (rather than an obvious strategy they literally signed off on), we then bring up the contradiction. Does this display of ignorance from every side really need to take place? Why bother asking for an explanation, they obviously did the thing they obviously did and will obviously do as much as possible to keep doing as much of things like that they can get away with. Do OpenAI employees actually get equity in the company (e.g. options or RSUs)? I was under the impression that the company awards ""profit units"" of some kind, and that many employees aren't sure how they work. I'm not following this very closely, but agreements that block employees from selling (private) vested equity are a market term, not something uniquely aggressive OpenAI does. The Vox article calls this ""just as important"" as the clawback terms, but, obviously, no. I wonder if this HN post will get torpedoed as fast as the one from yesterday[0]. 0. https://news.ycombinator.com/item?id=40435440 Nothing quite like a contract’s consideration consisting solely of a pre-existing obligation. I wonder what they were thinking with that? Everyone is out for Sam Altman, and there are reasons to scrutinize him. But on this issue - it is common for a company's Legal and HR teams to make decisions on language in docs like these (exit docs) entirely on their own. So it is plausible that Sam Altman had no idea that this aggressive language existed. One reason to think the same thing is true here, is I recall Sam spoke up for employee friendly equity plans when he was running YC. I'm surprised that an executive or lawyer didn't realise the reputational damage adding these clauses would eventually cause the leadership team. Were they really stupid enough to think that the amount of money being offered would bend some of the most principled people in the world? Whoever allowed those clauses to be added and let them remain has done more damage to the public face of OpenAI than any aggravated ex-employee ever could. Does anyone remember the name of that coder who made a kickstarter for his game but he was unable to finish it because it was a bit too big ( but still epic ) and then due to his talent he got hired at OpenAI? I always wanted to follow him on Twitter but I forgot his name :\ if anyone knows that be great Edit - sry why is this the top comment > this is on me and one of the few times i've been genuinely embarrassed running openai This statement seems to suggest that feeling embarrassed by one's actions is a normal part of running a company. In reality, the expectation is that a CEO should strive to lead with integrity and foresight to avoid situations that lead to embarrassment. Streisand Effect at work I think it’s time to cancel that Chat GPT subscription and move to something else. I am tired of the arrogance of these companies and particularly their narcissistic leaders who constantly want to make themselves the centre of the piece. It’s absolutely ridiculous to run a company as if you’re the lead in a contemporary drama. I don't understand whenever you read about something like this, why the head of HR at a company like this (just google (head of people|hr|""human resources"" openai linkedin) and see the first result) doesn't end up on a public blacklist of bad actors who are knowingly aggressive toward employees! Who is bullish or bearish on OpenAI? Now that LLM alternatives are getting better and better, as well as having well funded competitors. They don't yet have seem to developed a new, more advanced technology. What's their long term moat? PG is Altman's godfather more or less. I am disappoint of these OpenAI news as of late. 5. Sam Altman I was told I shouldn't mention founders of YC-funded companies in this list. But Sam Altman can't be stopped by such flimsy rules. If he wants to be on this list, he's going to be. Honestly, Sam is, along with Steve Jobs, the founder I refer to most when I'm advising startups. On questions of design, I ask ""What would Steve do?"" but on questions of strategy or ambition I ask ""What would Sama do?"" What I learned from meeting Sama is that the doctrine of the elect applies to startups. It applies way less than most people think: startup investing does not consist of trying to pick winners the way you might in a horse race. But there are a few people with such force of will that they're going to get whatever they want. https://p@ulgraham.com/5founders.html *edited link due to first post getting deleted Imagine if these people, obviously narrow-minded and greedy, gain access to AGI. It really would be a thread to mankind. I don’t believe in the AGI claims, or in X-Risk. But I do think it’s apparent that AI will only become more powerful and ubiquitous. Very concerning that someone like Sam, with a history of dishonesty and narcissism that is only becoming more obvious time, may stand to control a large chunk of this technology. He can’t be trusted, and as a result OpenAI cannot be trusted. we're deeply sorry we got caught, we need to do better. i take full responsibility for this mistake, i should have ensured all incriminating documents were destroyed. ps ""responsibility"" means ""zero consequences"" It's okay everyone. Silicon Valley will save us. Pay no mind to the ""mistakes"" they've made over the last 60 years. I feel there is a smear campaign going on to tarnish OpenAI Protip: you can’t negotiate terms after you agree to them. From OpenAI's ""fuller statement"": > “We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be.” Yeah, right. Words don't necessarily reflect one's true values, but actions do. And to the extent that they really are ""incredibly sorry"", it's not because of what they did, but that they got caught doing it. The company that fails in even a simple good faith gesture in their employee aggreement, claims it is the only one who can handle AGI while government creating regulation to lock out open source. AI-native companies seem to bring a new form of working culture. It could be different from tech industries environment. yikes... turns out that lily is actually a venus fly trap... This really is OpenAI's Downing Street Christmas Party week isn't it. It's for the good of humanity... that part of humanity that may not want bad PR. What surprises me about these stories surrounding openAI is how they apologize while lying and downplaying any blame. Do they expect anybody to believe they didn’t know about clawback clauses? I tried to delete my ChatGPT account but the confirmation button remained locked. Anyone else have the same issue? OpenAI's terrible, horrible, no good, very bad month only continues to worsen. It's pretty established now that they had some exceptionally anti-employee provisions in their exit policies to protect their fragile reputation. Sam Altman is bluntly a liar, and his credibility is gone. Their stance as a pro-artist platform is a joke after the ScarJo fiasco, that clearly illustrates that creative consent was an afterthought. Litigation is assumed, and ScarJo is directly advocating for legislation to prevent this sort of fiasco in the future. Sam Altman's involvement is again evident from his trite ""her"" tweet. And then they fired their ""superalignment"" safety team for good measure. As if to shred any last measure of doubt that this company is somehow more ethical than any other big tech company in their pursuit of AI. Frankly, at this point, the board should fire Sam Altman again, this time for good. This is not the company that can, or should, usher humanity into the artificial intelligence era. From Sam Altman: > this is on me and one of the few times i've been genuinely embarrassed running openai; i did not know this was happening and i should have. Bullshit. Presumably Sam Altman has 20 IQ points on me. He obviously knows better. I was a CEO for 25 years and no contract was issued without my knowing every element in it. In fact, I had them all written by lawyers in plain English, resorting to all caps and legal boilerplate only when it was deemed necessary. For every house, business, or other major asset I sold if there were 1 or more legal documents associated with the transaction I read them all, every time. When I go to the doctor and they have a privacy or HIPAA form, I read those too. Everything the kids' schools sent to me for signing--read those as well. He lies. And if he doesn't... then he is being libeled right and left by his sister. https://twitter.com/anniealtman108 I've learned to interpret anything Sam Altman says as-if an Aes Sedai said it. That is: every word is true, but leads the listener to making false assumptions. Even if in this specific instance he means well, it's still quite entertaining to interpret his statements this way: ""we have never clawed back anyone's vested equity"" => But we can and will, if we decide to. ""nor will we do that if people do not sign a separation agreement"" => But we made everyone sign the separation agreement. ""vested equity is vested equity, full stop."" => Our employees don't have vested equity, they have something else we tricked them into. ""there was a provision about potential equity cancellation in our previous exit docs;"" => And also in our current docs. ""although we never clawed anything back"" => Not yet, anyway. ""the team was already in the process of fixing the standard exit paperwork over the past month or so."" => By ""fixing"", I don't mean removing the non-disparagement clause, I mean make it ironclad while making the language less controversial and harder to argue with. ""if any former employee who signed one of those old agreements is worried about it, they can contact me and we'll fix that too."" => We'll fix the employee, not the problem. ""very sorry about this."" => Very sorry we got caught. Mr. Altman seems like a quite pedantic and evil people to work with—absolute psychopath. So disappointing of OpenAI. I hope they'll make things right with all their former employees. I thought freedom of speech was a foundational thing in the US. But I guess anyone could be silenced with enough economic incentive? Are there more than 2 former openai employees ? > “The team did catch this ~month ago. The fact that it went this long before the catch is on me.” I love this bullshit sentence formulation that claims to both have known this already--as in, don't worry we're ALREADY on the case--and they're simultaneously embarrassed that they ""just"" caught it--a.k.a. ""wow, we JUST heard about this, how outRAGEOUS"". Unfortunately it is unlikely to result in Altman's dismissal but imagine being fired from the same company, twice, in less than 12 months.","Leaked OpenAI documents reveal aggressive tactics toward former employees nan If this really was a mistake the easiest way to deal with it would be to release people from their non disparagement agreements that were only signed by leaving employees under the duress of losing their vested equity. It's really easy to make people whole for this, so whether that happens or not is the difference between the apologies being real or just them just backpedaling because employees got upset. Edit: Looks like they're doing the right thing here: > Altman’s initial statement was criticized for doing too little to make things right for former employees, but in an emailed statement, OpenAI told me that “we are identifying and reaching out to former employees who signed a standard exit agreement to make it clear that OpenAI has not and will not cancel their vested equity and releases them from nondisparagement obligations” — which goes much further toward fixing their mistake. The amount [and scale] of practices, chaos and controversies caused by OpenAI since ChatGPT was released are ""on par"" with the powerful products it has built since.. in a negative way! These are the hottest controversial events so far , in a chronological order: OpenAI's deviation from its original mission (https://news.ycombinator.com/item?id=34979981). The Altman's Saga (https://news.ycombinator.com/item?id=38309611). The return of Altman (within a week) (https://news.ycombinator.com/item?id=38375239). Musk vs. OpenAI (https://news.ycombinator.com/item?id=39559966). The departure of high-profile employees (Karpathy: https://news.ycombinator.com/item?id=39365935 ,Sutskever: https://news.ycombinator.com/item?id=40361128). ""Why can’t former OpenAI employees talk?"" (https://news.ycombinator.com/item?id=40393121). Great, if these documents are credible, this is exactly what I was implying[1] yesteday. Here, listen to Altman say how he is ""genuinely embarrassed"": ""this is on me and one of the few times i've been genuinely embarrassed running openai; i did not know this was happening and i should have."" The first thing the above conjures up is the other disgraced Sam (Bankman-Fried) saying ""this is on me"" when FTX went bust. I bet euros-to-croissants I'm not the only one to notice this. Some amount of corporate ruthlessness is part of the game, whether we like it or not. But these SV robber-barrons really crank it up to something else. [1] https://news.ycombinator.com/item?id=40425735 Looking forward for a document leak about openai using YouTube data for training their models. When asked if they use it, Murali (CTO) told she doesn't know which makes you believe that for 99% they are using it. I find it hard to believe that Sam didn’t know about something that draconian in something so sensitive as NDAs that affects to equity. He’s not exactly new to this whole startup thing and getting equity right is not a small part of that So what happened to Daniel Kokotajlo, the ex-OAI employee who made a comment saying that his equity was clawed back? Was it a miscommunication and he was referring to unvested equity, or is Sama just lying? In the original context, it sounded very much like he was referring to clawed-back equity. I’m trying to find the link. > ..or agreeing not to criticize the company, with no end date Oh! free speech is on trade! We used to hear the above statement coming from some political regimes but this is the first time I read it in the tech world. Would we live to witness more variations of this behavior on a larger scale?! > High-pressure tactics at OpenAI > That meant the former employees had a week to decide whether to accept OpenAI’s muzzle or risk forfeiting what could be millions of dollars > When ex-employees asked for more time to seek legal aid and review the documents, they faced significant pushback from OpenAI. > “We want to make sure you understand that if you don't sign, it could impact your equity. That's true for everyone, and we're just doing things by the book,” Although they've been able to build the most capable AI models that could replace a lot of human jobs, they struggle to humanely manage the people behind these models!! PDF: https://s3.documentcloud.org/documents/24679729/aestas_reduc... Going to be hard to keep claiming you didn’t know something, if your signature is on it. I don’t really think a CEO gets to say he didn’t read what he was signing. Does someone know why the employees wanted him back so badly? Must be very few employees actually upset with him and his way of doing things. In my third world country, when they do something unethical they say ""everything is in accordance with the law"", here it's ""this is on me"", both are very cynical. From the time they went private, it was apparent that this company is unethical to say the least. Given what it is building, this can be very dangerous, but I think they are more proficient in creating hype, than actually coming up with something meaningful. It's funny how finding out about corporate misdoing has almost a common ritual attached to it. First shock and dismay is expressed to the findings, then the company leadership has to say it was a mistake (rather than an obvious strategy they literally signed off on), we then bring up the contradiction. Does this display of ignorance from every side really need to take place? Why bother asking for an explanation, they obviously did the thing they obviously did and will obviously do as much as possible to keep doing as much of things like that they can get away with. Do OpenAI employees actually get equity in the company (e.g. options or RSUs)? I was under the impression that the company awards ""profit units"" of some kind, and that many employees aren't sure how they work. I'm not following this very closely, but agreements that block employees from selling (private) vested equity are a market term, not something uniquely aggressive OpenAI does. The Vox article calls this ""just as important"" as the clawback terms, but, obviously, no. I wonder if this HN post will get torpedoed as fast as the one from yesterday[0]. 0. https://news.ycombinator.com/item?id=40435440 Nothing quite like a contract’s consideration consisting solely of a pre-existing obligation. I wonder what they were thinking with that? Everyone is out for Sam Altman, and there are reasons to scrutinize him. But on this issue - it is common for a company's Legal and HR teams to make decisions on language in docs like these (exit docs) entirely on their own. So it is plausible that Sam Altman had no idea that this aggressive language existed. One reason to think the same thing is true here, is I recall Sam spoke up for employee friendly equity plans when he was running YC. I'm surprised that an executive or lawyer didn't realise the reputational damage adding these clauses would eventually cause the leadership team. Were they really stupid enough to think that the amount of money being offered would bend some of the most principled people in the world? Whoever allowed those clauses to be added and let them remain has done more damage to the public face of OpenAI than any aggravated ex-employee ever could. Does anyone remember the name of that coder who made a kickstarter for his game but he was unable to finish it because it was a bit too big ( but still epic ) and then due to his talent he got hired at OpenAI? I always wanted to follow him on Twitter but I forgot his name :\ if anyone knows that be great Edit - sry why is this the top comment > this is on me and one of the few times i've been genuinely embarrassed running openai This statement seems to suggest that feeling embarrassed by one's actions is a normal part of running a company. In reality, the expectation is that a CEO should strive to lead with integrity and foresight to avoid situations that lead to embarrassment. Streisand Effect at work I think it’s time to cancel that Chat GPT subscription and move to something else. I am tired of the arrogance of these companies and particularly their narcissistic leaders who constantly want to make themselves the centre of the piece. It’s absolutely ridiculous to run a company as if you’re the lead in a contemporary drama. I don't understand whenever you read about something like this, why the head of HR at a company like this (just google (head of people|hr|""human resources"" openai linkedin) and see the first result) doesn't end up on a public blacklist of bad actors who are knowingly aggressive toward employees! Who is bullish or bearish on OpenAI? Now that LLM alternatives are getting better and better, as well as having well funded competitors. They don't yet have seem to developed a new, more advanced technology. What's their long term moat? PG is Altman's godfather more or less. I am disappoint of these OpenAI news as of late. 5. Sam Altman I was told I shouldn't mention founders of YC-funded companies in this list. But Sam Altman can't be stopped by such flimsy rules. If he wants to be on this list, he's going to be. Honestly, Sam is, along with Steve Jobs, the founder I refer to most when I'm advising startups. On questions of design, I ask ""What would Steve do?"" but on questions of strategy or ambition I ask ""What would Sama do?"" What I learned from meeting Sama is that the doctrine of the elect applies to startups. It applies way less than most people think: startup investing does not consist of trying to pick winners the way you might in a horse race. But there are a few people with such force of will that they're going to get whatever they want. https://p@ulgraham.com/5founders.html *edited link due to first post getting deleted Imagine if these people, obviously narrow-minded and greedy, gain access to AGI. It really would be a thread to mankind. I don’t believe in the AGI claims, or in X-Risk. But I do think it’s apparent that AI will only become more powerful and ubiquitous. Very concerning that someone like Sam, with a history of dishonesty and narcissism that is only becoming more obvious time, may stand to control a large chunk of this technology. He can’t be trusted, and as a result OpenAI cannot be trusted. we're deeply sorry we got caught, we need to do better. i take full responsibility for this mistake, i should have ensured all incriminating documents were destroyed. ps ""responsibility"" means ""zero consequences"" It's okay everyone. Silicon Valley will save us. Pay no mind to the ""mistakes"" they've made over the last 60 years. I feel there is a smear campaign going on to tarnish OpenAI Protip: you can’t negotiate terms after you agree to them. From OpenAI's ""fuller statement"": > “We're incredibly sorry that we're only changing this language now; it doesn't reflect our values or the company we want to be.” Yeah, right. Words don't necessarily reflect one's true values, but actions do. And to the extent that they really are ""incredibly sorry"", it's not because of what they did, but that they got caught doing it. The company that fails in even a simple good faith gesture in their employee aggreement, claims it is the only one who can handle AGI while government creating regulation to lock out open source. AI-native companies seem to bring a new form of working culture. It could be different from tech industries environment. yikes... turns out that lily is actually a venus fly trap... This really is OpenAI's Downing Street Christmas Party week isn't it. It's for the good of humanity... that part of humanity that may not want bad PR. What surprises me about these stories surrounding openAI is how they apologize while lying and downplaying any blame. Do they expect anybody to believe they didn’t know about clawback clauses? I tried to delete my ChatGPT account but the confirmation button remained locked. Anyone else have the same issue? OpenAI's terrible, horrible, no good, very bad month only continues to worsen. It's pretty established now that they had some exceptionally anti-employee provisions in their exit policies to protect their fragile reputation. Sam Altman is bluntly a liar, and his credibility is gone. Their stance as a pro-artist platform is a joke after the ScarJo fiasco, that clearly illustrates that creative consent was an afterthought. Litigation is assumed, and ScarJo is directly advocating for legislation to prevent this sort of fiasco in the future. Sam Altman's involvement is again evident from his trite ""her"" tweet. And then they fired their ""superalignment"" safety team for good measure. As if to shred any last measure of doubt that this company is somehow more ethical than any other big tech company in their pursuit of AI. Frankly, at this point, the board should fire Sam Altman again, this time for good. This is not the company that can, or should, usher humanity into the artificial intelligence era. From Sam Altman: > this is on me and one of the few times i've been genuinely embarrassed running openai; i did not know this was happening and i should have. Bullshit. Presumably Sam Altman has 20 IQ points on me. He obviously knows better. I was a CEO for 25 years and no contract was issued without my knowing every element in it. In fact, I had them all written by lawyers in plain English, resorting to all caps and legal boilerplate only when it was deemed necessary. For every house, business, or other major asset I sold if there were 1 or more legal documents associated with the transaction I read them all, every time. When I go to the doctor and they have a privacy or HIPAA form, I read those too. Everything the kids' schools sent to me for signing--read those as well. He lies. And if he doesn't... then he is being libeled right and left by his sister. https://twitter.com/anniealtman108 I've learned to interpret anything Sam Altman says as-if an Aes Sedai said it. That is: every word is true, but leads the listener to making false assumptions. Even if in this specific instance he means well, it's still quite entertaining to interpret his statements this way: ""we have never clawed back anyone's vested equity"" => But we can and will, if we decide to. ""nor will we do that if people do not sign a separation agreement"" => But we made everyone sign the separation agreement. ""vested equity is vested equity, full stop."" => Our employees don't have vested equity, they have something else we tricked them into. ""there was a provision about potential equity cancellation in our previous exit docs;"" => And also in our current docs. ""although we never clawed anything back"" => Not yet, anyway. ""the team was already in the process of fixing the standard exit paperwork over the past month or so."" => By ""fixing"", I don't mean removing the non-disparagement clause, I mean make it ironclad while making the language less controversial and harder to argue with. ""if any former employee who signed one of those old agreements is worried about it, they can contact me and we'll fix that too."" => We'll fix the employee, not the problem. ""very sorry about this."" => Very sorry we got caught. Mr. Altman seems like a quite pedantic and evil people to work with—absolute psychopath. So disappointing of OpenAI. I hope they'll make things right with all their former employees. I thought freedom of speech was a foundational thing in the US. But I guess anyone could be silenced with enough economic incentive? Are there more than 2 former openai employees ? > “The team did catch this ~month ago. The fact that it went this long before the catch is on me.” I love this bullshit sentence formulation that claims to both have known this already--as in, don't worry we're ALREADY on the case--and they're simultaneously embarrassed that they ""just"" caught it--a.k.a. ""wow, we JUST heard about this, how outRAGEOUS"". Unfortunately it is unlikely to result in Altman's dismissal but imagine being fired from the same company, twice, in less than 12 months."
44567857,LLM Inevitabilism,nan,https://tomrenner.com/posts/llm-inevitabilism/,SwoopsFromAbove,1773,2025-07-15T04:35:35+00:00,,query,50,"I think two things can be true simultaneously: 1. LLMs are a new technology and it's hard to put the genie back in the bottle with that. It's difficult to imagine a future where they don't continue to exist in some form, with all the timesaving benefits and social issues that come with them. 2. Almost three years in, companies investing in LLMs have not yet discovered a business model that justifies the massive expenditure of training and hosting them, the majority of consumer usage is at the free tier, the industry is seeing the first signs of pulling back investments, and model capabilities are plateauing at a level where most people agree that the output is trite and unpleasant to consume. There are many technologies that have seemed inevitable and seen retreats under the lack of commensurate business return (the supersonic jetliner), and several that seemed poised to displace both old tech and labor but have settled into specific use cases (the microwave oven). Given the lack of a sufficiently profitable business model, it feels as likely as not that LLMs settle somewhere a little less remarkable, and hopefully less annoying, than today's almost universally disliked attempts to cram it everywhere. One of the negative consequences of the “modern secular age” is that many very intelligent, thoughtful people feel justified in brushing away millennia of philosophical and religious thought because they deem it outdated or no longer relevant. (The book A Secular Age is a great read on this, btw, I think I’ve recommended it here on HN at least half a dozen times.) And so a result of this is that they fail to notice the same recurring psychological patterns that underly thoughts about how the world is, and how it will be in the future - and then adjust their positions because of this awareness. For example - this AI inevitabilism stuff is not dissimilar to many ideas originally from the Reformation, like predestination. The notion that history is just on some inevitable pre-planned path is not a new idea, except now the actor has changed from God to technology. On a psychological level it’s the same thing: an offloading of freedom and responsibility to a powerful, vaguely defined force that may or may not exist outside the collective minds of human society. If in 2009 you claimed that the dominance of the smartphone was inevitable, it would have been because you were using one and understood its power, not because you were reframing away our free choice for some agenda. In 2025 I don't think you can really be taking advantage of AI to do real work and still see its mass adaptation as evitable. It's coming faster and harder than any tech in history. As scary as that is we can't wish it away. There may be an ""LLM Winter"" as people discover that LLMs can't be trusted to do anything. Look for frantic efforts by companies to offload responsibility for LLM mistakes onto consumers. We've got to have something that has solid ""I don't know"" and ""I don't know how to do this"" outputs. We're starting to see reports of LLM usage having negative value for programmers, even though they think it's helping. Too much effort goes into cleaning up LLM messes. Two things are very clearly true: 1) LLMs can do a lot of things that previous computing techniques could not do and we need time to figure out how best to harness and utilize those capabilities; but also 2) there is a wide range of powerful people who have tons of incentive to ride the hype wave regardless of where things will actually land. To the article's point—I don't think it's useful to accept the tech CEO framing and engage on their terms at all. They are mostly talking to the markets anyway. We are the ones who understand how technology works, so we're best positioned to evaluate LLMs more objectively, and we should decide our own framing. My framing is that LLMs are just another tool in a long line of software tooling improvements. Sure, it feels sort of miraculous and perhaps threatening that LLMs can write working code so easily. But when you think of all the repetitive CRUD and business logic that has been written over the decades to address myriad permutations and subtly varying contexts of the many human organizations that are willing to pay for software to be written, it's not surprising that we could figure out how to make a giant stochastic generator that can do an adequate job generating new permutations based on the right context and prompts. As a technologist I want to understand what LLMs can do and how they can serve my personal goals. If I don't want to use them I won't, but I also owe it to myself to understand how their capabilities evolve so I can make an informed decision. I am not going to start a crusade against them out of nostalgia or wishful thinking as I can think of nothing so futile as positioning myself in direct opposition to a massive hype tsunami. In the 90s a friend told me about the internet. And that he knows someone who is in a university and has access to it and can show us. An hour later, we were sitting in front of a computer in that university and watched his friend surfing the web. Clicking on links, receiving pages of text. Faster than one could read. In a nice layout. Even with images. And links to other pages. We were shocked. No printing, no shipping, no waiting. This was the future. It was inevitable. Yesterday I wanted to rewrite a program to use a large library that would have required me to dive deep down into the documentation or read its code to tackle my use case. As a first try, I just copy+pasted the whole library and my whole program into GPT 4.1 and told it to rewrite it using the library. It succeeded at the first attempt. The rewrite itself was small enough that I could read all code changes in 15 minutes and make a few stylistic changes. Done. Hours of time saved. This is the future. It is inevitable. PS: Most replies seem to compare my experience to experiences that the responders have with agentic coding, where the developer is iteratively changing the code by chatting with an LLM. I am not doing that. I use a ""One prompt one file. No code edits."" approach, which I describe here: https://www.gibney.org/prompt_coding The hardest part about inevitablism here is that the people who are making the argument this is inevitable are the same people who are the people who are shoveling hundreds of millions of dollars into it. Into the development, the use, the advertisement. The foxes are building doors into the hen houses and saying there’s nothing to be done, foxes are going to get in so we might as well make it something that works for everyone. I have a foreboding of an America in my children's or grandchildren's time -- when the United States is a service and information economy; when nearly all the manufacturing industries have slipped away to other countries; when awesome technological powers are in the hands of a very few, and no one representing the public interest can even grasp the issues; when the people have lost the ability to set their own agendas or knowledgeably question those in authority; when, clutching our crystals and nervously consulting our horoscopes, our critical faculties in decline, unable to distinguish between what feels good and what's true, we slide, almost without noticing, back into superstition and darkness... This concept is closely reated to politics of inevitability coined by Timothy Snyder. ""...the politics of inevitability – a sense that the future is just more of the present, that the laws of progress are known, that there are no alternatives, and therefore nothing really to be done.""[0] [0] https://www.theguardian.com/news/2018/mar/16/vladimir-putin-... This article in question obviously applied it within the commercial world but at the end it has to do with language that takes away agency. People like communicating in natural language. LLMs are the first step in the movement away from the ""early days"" of computing where you needed to learn the logic based language and interface of computers to interact with them. That is where the inevitabilism comes from. No one* wants to learn how to use a computer, they want it to be another entity that they can just talk to. *I'm rounding off the <5% who deeply love computers. How do you differentiate between an effective debater using inevitabilism as a technique to win a debate, and an effective thinker making a convincing argument that something is likely to be inevitable? How do you differentiate between an effective debater ""controlling the framing of a conversation"" and an effective thinker providing a new perspective on a shared experience? How do you differentiate between a good argument and a good idea? I don't think you can really? You could say intent plays a part -- that someone with an intent to manipulate can use debating tools as tricks. But still, even if someone with bad intentions makes a good argument, isn't it still a good argument? Earlier today I was scrolling at the “work at a startup” posts. Seems like everyone is doing LLM stuff. We are back at the “uber for X” but now it is “ChatGPT for X”. I get it, but I’ve never felt more uninspired looking at what yc startups are working on today. For the first time they all feel incredibly generic The author seems to imply that the ""framing"" of an argument is done so in bad faith in order to win an argument but only provides one-line quotes where there is no contextual argument. This tactic by the author is a straw-man argument - he's framing the position of tech leaders and our acceptance of it as the reason AI exists, instead of being honest, which is that they were simply right in their predictions: AI was inevitable. The IT industry is full of pride and arrogance. We deny the power of AI and LLMs. I think that's fair, I welcome the pushback. But the real word the IT crowd needs to learn is ""denialism"" - if you still don't see how LLMs is changing our entire industry, you haven't been paying attention. Edit: Lots of denialists using false dichotomy arguments that my opinion is invalid because I'm not producing examples and proof. I guess I'll just leave this: https://tools.simonwillison.net/ It's inevitable because it's here. LLMs aren't the ""future"" anymore, they're the present. They're unseating Google as the SOTA method of finding information on the internet. People have been trying to do that for decades. The future probably holds even bigger things, but even if it plateaus for a while, showing real ability to defeat traditional search is a crazy start and just one example. My belief is that whatever technology can be invented by humans (under the constraints of the laws of physics, etc) will eventually be invented. I don't have a strong argument for this; it's just what makes sense to me. If true, then an immediate corollary is that if it is possible for humans to create LLMs (or other AI systems) which can program, or do some other tasks, better than humans can, that will happen. Inevitabilism? I don't think so. If that comes to pass, then what people will do with that technology, and what will change as a result, will be up to the people who are alive at the time. But not creating the technology is not an option, if it's within the realm of what humans can possibly create. Did anyone even read the article? Maybe you should get an LLM to bullet point it for you. The author isn't arguing about whether LLMs (or AI) is inevitable or not. They are saying you don't have to operate within their framing. You should be thinking about whether this thing is really good for us and not just jumping on the wagon and toeing the line because you're told it's inevitable. I've noticed more and more the go to technique for marketing anything now is FOMO. It works. Don't let it work on you. Don't buy into a thing just because everyone else is. Most of the time you aren't missing out on anything at all. Some of the time the thing is actively harmful to the participants and society. This inevitabilist framing rests on an often unspoken assumption: that LLM's will decisively outperform human capabilities in myriad domains. If that assumption holds true, then the inevitabilist quotes featured in the article are convincing to me. If LLM's turn out to be less worthwhile at scale than many people assume, the inevitabilist interpretation is another dream of AI summer. Burying the core assumption and focusing on its implication is indeed a fantastic way of framing the argument to win some sort of debate. The company name was changed from Facebook to Meta because Mark thought the metaverse was inevitable, it's ironic that you use a quote from him (commenting late in the game, so the point may have been made already) I personally believe that ""AI"" is mostly marketing for the current shiny LLM thing that will end up finding some sort of actual useful niche (or two) once the dust has settled. But for now, it's more of a solution being carpet-bombed for problems, most of them inappropriate IMHO (e.g, replacing HR). For now there'll be collateral damage as carbon-based lifeforms are displaced, with an inevitable shortage of pesky humans to do cleanup once the limitations of ""AI"" are realized. Any the humans will probably be contract/gig at half their previous rates to do the cleanup. People and companies that use LLMs will be seen as tacky and cheap. They already are. Eew you have an ai generated profile photo? You write (code) with ai? You use ai to create marketing and graphics? You use non deterministic LLMs to brute force instead of paying humans to write efficient algorithms? Yuck yuck yuck This is a sharp dissection of ‘inevitabilism’ as a rhetorical strategy. I’ve noticed it too: the moment someone says ‘X is inevitable’, the burden of proof disappears and dissent becomes ‘denial’. But isn’t that framing itself... fragile? We’ve seen plenty of ‘inevitable’ futures (crypto, the Metaverse, even Web3) collapse under public pushback or internal rot. The question I’m left with: if inevitabilism is so effective rhetorically, how do we counter it without sounding naïve or regressive? It seemed inevitable that the Internet would allow understanding of other cultures and make future war impossible, as the people united and stood in opposition to oppression and stupidity the world over. Reality worked out differently. I suspect the same is about to happen with our LLM overlords. I think what scares people who code for a living the most is the loss of their craft. Many of you have spent years or decades honing the craft of producing clear, fast, beautiful code. Now there is something that can spit out (often) beautiful code in seconds. An existential threat to your self worth and livelihood. A perfectly reasonable thing to react to. I do think, however, that this is an inevitable change. Industries and crafts being massively altered by technology is a tale as old as time. In a world that constantly changes, adaptation is key. I also think that almost all of you who have this craft should have no problem pivoting to higher level software architecture design. Work with an llm and produce things it would have taken a small team to do in 2019. I find it to be a very exciting time. “The ultimate hidden truth of the world is that it is something that we make, and could just as easily make differently.” David Graeber Right now, I’m noticing how my colleagues who aren’t very comfortable using LLMs for most of their work are getting sidelined. It's a bit sad seeing them struggle by not keeping pace with everyone else who is using it for ~90% of our tasks. They seem to really care about writing code themselves, but, if they don't pivot, things are probably not going to end well for them. So is LLM inevitable? Pretty much if you want to remain competitive. > I’m certainly not convinced that they’re the future I want. But what I’m most certain of is that we have choices about what our future should look like, and how we choose to use machines to build it. While I must admit we have some choice here, it is limited. No matter what, there will be models of language, we know how they work, there is no turning back from it. We might wish many things but one thing we can't do is to revert time to a moment when these discoveries did not exist. LLM is an almost complete waste of time. Advocates of LLM are not accurately measuring their time and productivity, and comparing that to LLM-free alternative approaches. I hate AI. I'm so sick of it. I read a story about 14 year olds that are adopting AI boyfriends. They spend 18 hours a day in conversation with chatbots. Their parents are worried because they are withdrawing from school and losing their friends. I hate second guessing emails that I've read, wondering if my colleagues are even talking to me or if they are using AI. I hate the idea that AI will replace my job. Even if it unlocks ""economic value"" -- what does that even mean? We'll live in fucking blade runner but at least we'll all have a ton of money? I agree, nobody asked what I wanted. But if they did I'd tell them, I don't want it, I don't want any of it. Excuse me, I'll go outside now and play with my dogs and stare at a tree. I was going to make an argument that it's inevitable, because at some point compute will get so cheap that someone could just train one at home, and since the knowledge of how to do it is out there, people will do it. But seeing that a company like Meta is using >100k GPUs to train these models, even at 25% yearly improvement it would still take until the year ~2060 before someone could buy 50 GPUs and have the equivalent power to train one privately. So I suppose if society decided to outlaw LLM training, or a market crash put off companies from continuing to do it, it might be possible to put the genie back in the bottle for a few decades. I wouldn't be surprised however if there are still 10x algorithmic improvements to be found too... The argument doesn't work because whatever you think of where generative AI is taking us or not taking us - it is 100% demonstrably better at doing a wide range of tasks than other technologies we have available to us - even in its current exact form. Once computers started to be connected could we have stopped the development of the world wide web. If there's a way of getting humanity to collectively agree on things - please let's start by using it to stop climate change and create world peace before moving on to getting rid of LLM's. These articles kill me. The reason LLMs (or next-gen AI architecture) is inevitably going to take over the world in one way or another is simple: recursive self-improvement. 3 years ago they could barely write a coherent poem and today they're performing at at least graduate student level across most tasks. As of today, AI is writing a significant chunk of the code around itself. Once AI crosses that threshold of consistently being above senior-level engineer level at coding it will reach a tipping point where it can improve itself faster than the best human expert. That's core technological recursive self-improvement but we have another avenue of recursive self-improvement as well: Agentic recursive self-improvement. First there was LLMs, then there was LLMs with tool usage, then we abstracted the tool usage to MCP servers. Next, we will create agents that autodiscover remote MCP servers, then we will create agents which can autodiscover tools as well as write their own. Final stage of agents are generalized agents similar to Claude Code which can find remote MCP servers, perform a task, then analyze their first run of completing a task to figure out how to improve the process. Then write its own tools to use to complete the task faster than they did before. Agentic recursive self-improvement. As an agent engineer, I suspect this pattern will become viable in about 2 years. Absolutely perfect blog post. You provoked some new thoughts, convinced me of your position, taught me something concrete and practical about debating, had a human narrative, gave me a good book recommendation, didn't feel manipulative or formulaic, wrote something that an employed person can read in a reasonable amount of time AND most importantly made a solid Matrix reference. You're my blog hero, thank you for being cool and setting a good example. Also really important LLM hype reminder. I do agree that those who claim AI is inevitable are essentially threatening you. Most of us that are somewhat into the tech behind AI know that it's all based on simple matrix math... and anyone can do that... So ""inevitibalism"" is how we sound because we see that if OpenAI doesn't do it, someone else will. Even if all the countries in the world agree to ban AI, its not based on something with actual scarcity (like purified uranium, or gold) so someone somewhere will keep moving this tech forward... I don't think it's inevitable, for very few things are really inevitable. However, I find LLM-s good and useful. First the chat bots, now the coding agents. Looks to me medical consultation, 2nd opinion and the like - are not far behind. Enough people already use them for that. I give my lab tests results to ChatGPT. Tbh can't fault the author for motivated reasoning. Looks to me it goes like: this is not a future I want -> therefore it should not happen -> therefore it will not happen. Because by the same motivated reasoning: for me it is the future I want. To be able to interact with a computer via language, speech and more. For the computer to be smart, instead of dumb, as it is now. If I can have the computer enhance my smarts, my information processing power, my memory - the way writing allows me to off-load from my head onto paper, a calculator allows me to manipulate numbers, and computer toils for days instead myself - then I will probably want for the AI to complement, enhance me too. If someone invested a lot of money in something, they probably are convinced that something is inevitable. Otherwise they would not invest their money. However, sometimes they may be a little bit helping their luck Things which are both powerful and possible become inevitable. We know that LLMs are powerful, but we aren't sure how powerful yet, and there's a large range this might eventually land in. We know they're possible in their current form, of course, but we don't know if actual GAI is possible. At this time, humanity seems to be estimating that both power and possibility will be off the charts. Why? Because getting this wrong can be so negatively impactful that it makes sense to move forward as if GAI will inevitably exist. Imagine supposing that this will all turn out to be fluff and GAI will never work, so you stop investing in it. Now imagine what happens if you're wrong and your enemy gets it to work first. This isn't some arguing device for AI-inevitabilists. It's knowledge of human nature, and it's been repeating itself for millennia. If the author believes that's going to suddenly change, they really should back that up with what, exactly, has changed in human nature. This is a fantastic framing method. Anyone who sees the future differently to you can be brushed aside as “an inevitablist,” and the only conversations worth engaging are those that already accept your premise. --- This argument so easily commits sudoku that I couldn't help myself. It's philosophical relativism, and self-immolates for the same reason -- it's inconsistent. It eats itself. I really like what is hidden between the lines of this text, it is only something a human can understand. The entire comment section over here reflects the uncanny valley. This blog post is a work of art LOL Wasn’t crypto supposed to have replaced fiat currency by now, or something? A few days ago I saw a nice tweet being shared and it wend something like: I am not allowed to use my airco as it eats to much power and we must think about the environment. Meanwhile: people non-stop generating rule 34 images using AI... I wish I can show my gratitude more than this but i'm truly grateful thank you for restoring my happiness and hopes back and more than i could even ask for, my marriage of 2 years crashed out of no good reason but i'm grateful Dr Alfred helped me restore everything back and now my husband even loves me more than he ever used to love me , I can't repay you Dr Alfred, I just want to say thank you and let everyone know that you are truly amazing, and indeed your love spell is powerful, you can reach him on dralfredspellhome@gmail.com or on whatsapp +2348134653457 Disclaimer - I am building an AI web retriever (Linkup.so) so I have a natural bias - LLMs aren’t just a better Google, they’re a redefinition of search itself. Traditional search is an app: you type, scroll through ads and 10 blue links, and dig for context. That model worked when the web was smaller, but now it’s overwhelming. LLMs shift search to an infrastructure, a way to get contextualized, synthesized answers directly, tailored to your specific need. Yes, they can hallucinate, but so can the web. It’s not about replacing Google—it’s about replacing the experience of searching (actually they probably will less and less 'experience' of searching) I think you are confusing ""I don't like it"" with ""It's not going to happen"". Just because you don't like it, it doesn't mean it's not going to happen. Observe the world without prejudice. Think rationally without prejudice. Not sure I get the author of this piece. The tech leaders are clearly saying AI is inevitable, they're not saying LLMs are inevitable. Big tech is constantly working on new types of AI such as world models. > ""AI ..."" > I’m not convinced that LLMs are the future. Was this an intentional bait/switch? LLM != AI. I'm quite sure LLMs are not the future. It's merely the step after AlexNet, AlphaGo, and before the next major advancement. AI is not inevitable, because technological progress in general is not inevitable. It is shapeable by economic incentives just like everything else. It can be ground into powder by resource starvation. We've long known that certain forms of financial bounties levied upon scientists working at the frontier of sciences we want to freeze in place work effectively with a minimum of policing and international cooperation. If a powerful country is willing to be a jerk (heavens!) and allow these kinds of bounties to be turned in even on extranationals, you don't need the international cooperation. But you do get a way to potentially kickstart a new Nash equilibrium that keeps itself going as soon as other countries adopt the same bounty-based policy. This mechanism has been floating around for at least a decade now. It's not news. Even the most inevitable seeming scientific developments can be effectively rerouted around using it. The question is whether you genuinely, earnestly believe what lies beyond the frontier is too dangerous to be let out, and in almost all cases the answer to that should be no. I post this mostly because inevitabilist arguments will always retain their power so long as you can come up with a coherent profit motive for something to be pursued. You don't get far with good-feeling spiels that amount to plaintive cries in a tornado. You need actual object level proposals on how to make the inevitable evitable. Language is not knowledge and knowledge when reduced to a language becomes here say until it is redone and implemented in our context. Both of them have nothing to do with wisdom. LLM's hash out our language and art to death but AI doesn't mind what they mean to us. Without our constraints and use, they would stop running. We should be building guardian angels to save us from ourselves and not evil demons to conquer the world. - John Eischen © adagp paris art humanitarian use is authorized except for any Al uses The article talks about being thrown off-balance by debating tricks and then proceed to do just that with a kind of bait and switch from talking about AI to talking about LLMs. Eg. it quotes >“AI is the new electricity.” – Andrew Ng as framing AI as kind of inevitable and then flips to >I’m not convinced that LLMs are the future. It seems to me AI is inevitable and LLMs will be replaced soon with some better algorithm. It's like video is inevitable but betamax wasn't. Two different things. Like a lot of blog posts, this feels like a premise worth exploring, lacking a critical exploration of that premise. Yes, ""inevitabilism"" is a thing, both in tech and in politics. But, crucially, it's not always wrong! Other comments have pointed out examples, such as the internet in the 90s. But when considering new cultural and technological developments that seem like a glimpse of the future, how do we know if they're an inevitability or not? The post says: > what I’m most certain of is that we have choices about what our future should look like, and how we choose to use machines to build it. To me, that sounds like mere wishful thinking. Yeah, sometimes society can turn back the tide of harmful developments; for instance, the ozone layer is well on its way to complete recovery. Other times, even when public opinion is mixed, such as with bitcoin, the technology does become quite successful, but doesn't seem to become quite as ubiquitous as its most fervent adherents expect. So how do we know which category LLM usage falls into? I don't know the answer, because I think it's a difficult thing to know in advance.","LLM Inevitabilism nan I think two things can be true simultaneously: 1. LLMs are a new technology and it's hard to put the genie back in the bottle with that. It's difficult to imagine a future where they don't continue to exist in some form, with all the timesaving benefits and social issues that come with them. 2. Almost three years in, companies investing in LLMs have not yet discovered a business model that justifies the massive expenditure of training and hosting them, the majority of consumer usage is at the free tier, the industry is seeing the first signs of pulling back investments, and model capabilities are plateauing at a level where most people agree that the output is trite and unpleasant to consume. There are many technologies that have seemed inevitable and seen retreats under the lack of commensurate business return (the supersonic jetliner), and several that seemed poised to displace both old tech and labor but have settled into specific use cases (the microwave oven). Given the lack of a sufficiently profitable business model, it feels as likely as not that LLMs settle somewhere a little less remarkable, and hopefully less annoying, than today's almost universally disliked attempts to cram it everywhere. One of the negative consequences of the “modern secular age” is that many very intelligent, thoughtful people feel justified in brushing away millennia of philosophical and religious thought because they deem it outdated or no longer relevant. (The book A Secular Age is a great read on this, btw, I think I’ve recommended it here on HN at least half a dozen times.) And so a result of this is that they fail to notice the same recurring psychological patterns that underly thoughts about how the world is, and how it will be in the future - and then adjust their positions because of this awareness. For example - this AI inevitabilism stuff is not dissimilar to many ideas originally from the Reformation, like predestination. The notion that history is just on some inevitable pre-planned path is not a new idea, except now the actor has changed from God to technology. On a psychological level it’s the same thing: an offloading of freedom and responsibility to a powerful, vaguely defined force that may or may not exist outside the collective minds of human society. If in 2009 you claimed that the dominance of the smartphone was inevitable, it would have been because you were using one and understood its power, not because you were reframing away our free choice for some agenda. In 2025 I don't think you can really be taking advantage of AI to do real work and still see its mass adaptation as evitable. It's coming faster and harder than any tech in history. As scary as that is we can't wish it away. There may be an ""LLM Winter"" as people discover that LLMs can't be trusted to do anything. Look for frantic efforts by companies to offload responsibility for LLM mistakes onto consumers. We've got to have something that has solid ""I don't know"" and ""I don't know how to do this"" outputs. We're starting to see reports of LLM usage having negative value for programmers, even though they think it's helping. Too much effort goes into cleaning up LLM messes. Two things are very clearly true: 1) LLMs can do a lot of things that previous computing techniques could not do and we need time to figure out how best to harness and utilize those capabilities; but also 2) there is a wide range of powerful people who have tons of incentive to ride the hype wave regardless of where things will actually land. To the article's point—I don't think it's useful to accept the tech CEO framing and engage on their terms at all. They are mostly talking to the markets anyway. We are the ones who understand how technology works, so we're best positioned to evaluate LLMs more objectively, and we should decide our own framing. My framing is that LLMs are just another tool in a long line of software tooling improvements. Sure, it feels sort of miraculous and perhaps threatening that LLMs can write working code so easily. But when you think of all the repetitive CRUD and business logic that has been written over the decades to address myriad permutations and subtly varying contexts of the many human organizations that are willing to pay for software to be written, it's not surprising that we could figure out how to make a giant stochastic generator that can do an adequate job generating new permutations based on the right context and prompts. As a technologist I want to understand what LLMs can do and how they can serve my personal goals. If I don't want to use them I won't, but I also owe it to myself to understand how their capabilities evolve so I can make an informed decision. I am not going to start a crusade against them out of nostalgia or wishful thinking as I can think of nothing so futile as positioning myself in direct opposition to a massive hype tsunami. In the 90s a friend told me about the internet. And that he knows someone who is in a university and has access to it and can show us. An hour later, we were sitting in front of a computer in that university and watched his friend surfing the web. Clicking on links, receiving pages of text. Faster than one could read. In a nice layout. Even with images. And links to other pages. We were shocked. No printing, no shipping, no waiting. This was the future. It was inevitable. Yesterday I wanted to rewrite a program to use a large library that would have required me to dive deep down into the documentation or read its code to tackle my use case. As a first try, I just copy+pasted the whole library and my whole program into GPT 4.1 and told it to rewrite it using the library. It succeeded at the first attempt. The rewrite itself was small enough that I could read all code changes in 15 minutes and make a few stylistic changes. Done. Hours of time saved. This is the future. It is inevitable. PS: Most replies seem to compare my experience to experiences that the responders have with agentic coding, where the developer is iteratively changing the code by chatting with an LLM. I am not doing that. I use a ""One prompt one file. No code edits."" approach, which I describe here: https://www.gibney.org/prompt_coding The hardest part about inevitablism here is that the people who are making the argument this is inevitable are the same people who are the people who are shoveling hundreds of millions of dollars into it. Into the development, the use, the advertisement. The foxes are building doors into the hen houses and saying there’s nothing to be done, foxes are going to get in so we might as well make it something that works for everyone. I have a foreboding of an America in my children's or grandchildren's time -- when the United States is a service and information economy; when nearly all the manufacturing industries have slipped away to other countries; when awesome technological powers are in the hands of a very few, and no one representing the public interest can even grasp the issues; when the people have lost the ability to set their own agendas or knowledgeably question those in authority; when, clutching our crystals and nervously consulting our horoscopes, our critical faculties in decline, unable to distinguish between what feels good and what's true, we slide, almost without noticing, back into superstition and darkness... This concept is closely reated to politics of inevitability coined by Timothy Snyder. ""...the politics of inevitability – a sense that the future is just more of the present, that the laws of progress are known, that there are no alternatives, and therefore nothing really to be done.""[0] [0] https://www.theguardian.com/news/2018/mar/16/vladimir-putin-... This article in question obviously applied it within the commercial world but at the end it has to do with language that takes away agency. People like communicating in natural language. LLMs are the first step in the movement away from the ""early days"" of computing where you needed to learn the logic based language and interface of computers to interact with them. That is where the inevitabilism comes from. No one* wants to learn how to use a computer, they want it to be another entity that they can just talk to. *I'm rounding off the <5% who deeply love computers. How do you differentiate between an effective debater using inevitabilism as a technique to win a debate, and an effective thinker making a convincing argument that something is likely to be inevitable? How do you differentiate between an effective debater ""controlling the framing of a conversation"" and an effective thinker providing a new perspective on a shared experience? How do you differentiate between a good argument and a good idea? I don't think you can really? You could say intent plays a part -- that someone with an intent to manipulate can use debating tools as tricks. But still, even if someone with bad intentions makes a good argument, isn't it still a good argument? Earlier today I was scrolling at the “work at a startup” posts. Seems like everyone is doing LLM stuff. We are back at the “uber for X” but now it is “ChatGPT for X”. I get it, but I’ve never felt more uninspired looking at what yc startups are working on today. For the first time they all feel incredibly generic The author seems to imply that the ""framing"" of an argument is done so in bad faith in order to win an argument but only provides one-line quotes where there is no contextual argument. This tactic by the author is a straw-man argument - he's framing the position of tech leaders and our acceptance of it as the reason AI exists, instead of being honest, which is that they were simply right in their predictions: AI was inevitable. The IT industry is full of pride and arrogance. We deny the power of AI and LLMs. I think that's fair, I welcome the pushback. But the real word the IT crowd needs to learn is ""denialism"" - if you still don't see how LLMs is changing our entire industry, you haven't been paying attention. Edit: Lots of denialists using false dichotomy arguments that my opinion is invalid because I'm not producing examples and proof. I guess I'll just leave this: https://tools.simonwillison.net/ It's inevitable because it's here. LLMs aren't the ""future"" anymore, they're the present. They're unseating Google as the SOTA method of finding information on the internet. People have been trying to do that for decades. The future probably holds even bigger things, but even if it plateaus for a while, showing real ability to defeat traditional search is a crazy start and just one example. My belief is that whatever technology can be invented by humans (under the constraints of the laws of physics, etc) will eventually be invented. I don't have a strong argument for this; it's just what makes sense to me. If true, then an immediate corollary is that if it is possible for humans to create LLMs (or other AI systems) which can program, or do some other tasks, better than humans can, that will happen. Inevitabilism? I don't think so. If that comes to pass, then what people will do with that technology, and what will change as a result, will be up to the people who are alive at the time. But not creating the technology is not an option, if it's within the realm of what humans can possibly create. Did anyone even read the article? Maybe you should get an LLM to bullet point it for you. The author isn't arguing about whether LLMs (or AI) is inevitable or not. They are saying you don't have to operate within their framing. You should be thinking about whether this thing is really good for us and not just jumping on the wagon and toeing the line because you're told it's inevitable. I've noticed more and more the go to technique for marketing anything now is FOMO. It works. Don't let it work on you. Don't buy into a thing just because everyone else is. Most of the time you aren't missing out on anything at all. Some of the time the thing is actively harmful to the participants and society. This inevitabilist framing rests on an often unspoken assumption: that LLM's will decisively outperform human capabilities in myriad domains. If that assumption holds true, then the inevitabilist quotes featured in the article are convincing to me. If LLM's turn out to be less worthwhile at scale than many people assume, the inevitabilist interpretation is another dream of AI summer. Burying the core assumption and focusing on its implication is indeed a fantastic way of framing the argument to win some sort of debate. The company name was changed from Facebook to Meta because Mark thought the metaverse was inevitable, it's ironic that you use a quote from him (commenting late in the game, so the point may have been made already) I personally believe that ""AI"" is mostly marketing for the current shiny LLM thing that will end up finding some sort of actual useful niche (or two) once the dust has settled. But for now, it's more of a solution being carpet-bombed for problems, most of them inappropriate IMHO (e.g, replacing HR). For now there'll be collateral damage as carbon-based lifeforms are displaced, with an inevitable shortage of pesky humans to do cleanup once the limitations of ""AI"" are realized. Any the humans will probably be contract/gig at half their previous rates to do the cleanup. People and companies that use LLMs will be seen as tacky and cheap. They already are. Eew you have an ai generated profile photo? You write (code) with ai? You use ai to create marketing and graphics? You use non deterministic LLMs to brute force instead of paying humans to write efficient algorithms? Yuck yuck yuck This is a sharp dissection of ‘inevitabilism’ as a rhetorical strategy. I’ve noticed it too: the moment someone says ‘X is inevitable’, the burden of proof disappears and dissent becomes ‘denial’. But isn’t that framing itself... fragile? We’ve seen plenty of ‘inevitable’ futures (crypto, the Metaverse, even Web3) collapse under public pushback or internal rot. The question I’m left with: if inevitabilism is so effective rhetorically, how do we counter it without sounding naïve or regressive? It seemed inevitable that the Internet would allow understanding of other cultures and make future war impossible, as the people united and stood in opposition to oppression and stupidity the world over. Reality worked out differently. I suspect the same is about to happen with our LLM overlords. I think what scares people who code for a living the most is the loss of their craft. Many of you have spent years or decades honing the craft of producing clear, fast, beautiful code. Now there is something that can spit out (often) beautiful code in seconds. An existential threat to your self worth and livelihood. A perfectly reasonable thing to react to. I do think, however, that this is an inevitable change. Industries and crafts being massively altered by technology is a tale as old as time. In a world that constantly changes, adaptation is key. I also think that almost all of you who have this craft should have no problem pivoting to higher level software architecture design. Work with an llm and produce things it would have taken a small team to do in 2019. I find it to be a very exciting time. “The ultimate hidden truth of the world is that it is something that we make, and could just as easily make differently.” David Graeber Right now, I’m noticing how my colleagues who aren’t very comfortable using LLMs for most of their work are getting sidelined. It's a bit sad seeing them struggle by not keeping pace with everyone else who is using it for ~90% of our tasks. They seem to really care about writing code themselves, but, if they don't pivot, things are probably not going to end well for them. So is LLM inevitable? Pretty much if you want to remain competitive. > I’m certainly not convinced that they’re the future I want. But what I’m most certain of is that we have choices about what our future should look like, and how we choose to use machines to build it. While I must admit we have some choice here, it is limited. No matter what, there will be models of language, we know how they work, there is no turning back from it. We might wish many things but one thing we can't do is to revert time to a moment when these discoveries did not exist. LLM is an almost complete waste of time. Advocates of LLM are not accurately measuring their time and productivity, and comparing that to LLM-free alternative approaches. I hate AI. I'm so sick of it. I read a story about 14 year olds that are adopting AI boyfriends. They spend 18 hours a day in conversation with chatbots. Their parents are worried because they are withdrawing from school and losing their friends. I hate second guessing emails that I've read, wondering if my colleagues are even talking to me or if they are using AI. I hate the idea that AI will replace my job. Even if it unlocks ""economic value"" -- what does that even mean? We'll live in fucking blade runner but at least we'll all have a ton of money? I agree, nobody asked what I wanted. But if they did I'd tell them, I don't want it, I don't want any of it. Excuse me, I'll go outside now and play with my dogs and stare at a tree. I was going to make an argument that it's inevitable, because at some point compute will get so cheap that someone could just train one at home, and since the knowledge of how to do it is out there, people will do it. But seeing that a company like Meta is using >100k GPUs to train these models, even at 25% yearly improvement it would still take until the year ~2060 before someone could buy 50 GPUs and have the equivalent power to train one privately. So I suppose if society decided to outlaw LLM training, or a market crash put off companies from continuing to do it, it might be possible to put the genie back in the bottle for a few decades. I wouldn't be surprised however if there are still 10x algorithmic improvements to be found too... The argument doesn't work because whatever you think of where generative AI is taking us or not taking us - it is 100% demonstrably better at doing a wide range of tasks than other technologies we have available to us - even in its current exact form. Once computers started to be connected could we have stopped the development of the world wide web. If there's a way of getting humanity to collectively agree on things - please let's start by using it to stop climate change and create world peace before moving on to getting rid of LLM's. These articles kill me. The reason LLMs (or next-gen AI architecture) is inevitably going to take over the world in one way or another is simple: recursive self-improvement. 3 years ago they could barely write a coherent poem and today they're performing at at least graduate student level across most tasks. As of today, AI is writing a significant chunk of the code around itself. Once AI crosses that threshold of consistently being above senior-level engineer level at coding it will reach a tipping point where it can improve itself faster than the best human expert. That's core technological recursive self-improvement but we have another avenue of recursive self-improvement as well: Agentic recursive self-improvement. First there was LLMs, then there was LLMs with tool usage, then we abstracted the tool usage to MCP servers. Next, we will create agents that autodiscover remote MCP servers, then we will create agents which can autodiscover tools as well as write their own. Final stage of agents are generalized agents similar to Claude Code which can find remote MCP servers, perform a task, then analyze their first run of completing a task to figure out how to improve the process. Then write its own tools to use to complete the task faster than they did before. Agentic recursive self-improvement. As an agent engineer, I suspect this pattern will become viable in about 2 years. Absolutely perfect blog post. You provoked some new thoughts, convinced me of your position, taught me something concrete and practical about debating, had a human narrative, gave me a good book recommendation, didn't feel manipulative or formulaic, wrote something that an employed person can read in a reasonable amount of time AND most importantly made a solid Matrix reference. You're my blog hero, thank you for being cool and setting a good example. Also really important LLM hype reminder. I do agree that those who claim AI is inevitable are essentially threatening you. Most of us that are somewhat into the tech behind AI know that it's all based on simple matrix math... and anyone can do that... So ""inevitibalism"" is how we sound because we see that if OpenAI doesn't do it, someone else will. Even if all the countries in the world agree to ban AI, its not based on something with actual scarcity (like purified uranium, or gold) so someone somewhere will keep moving this tech forward... I don't think it's inevitable, for very few things are really inevitable. However, I find LLM-s good and useful. First the chat bots, now the coding agents. Looks to me medical consultation, 2nd opinion and the like - are not far behind. Enough people already use them for that. I give my lab tests results to ChatGPT. Tbh can't fault the author for motivated reasoning. Looks to me it goes like: this is not a future I want -> therefore it should not happen -> therefore it will not happen. Because by the same motivated reasoning: for me it is the future I want. To be able to interact with a computer via language, speech and more. For the computer to be smart, instead of dumb, as it is now. If I can have the computer enhance my smarts, my information processing power, my memory - the way writing allows me to off-load from my head onto paper, a calculator allows me to manipulate numbers, and computer toils for days instead myself - then I will probably want for the AI to complement, enhance me too. If someone invested a lot of money in something, they probably are convinced that something is inevitable. Otherwise they would not invest their money. However, sometimes they may be a little bit helping their luck Things which are both powerful and possible become inevitable. We know that LLMs are powerful, but we aren't sure how powerful yet, and there's a large range this might eventually land in. We know they're possible in their current form, of course, but we don't know if actual GAI is possible. At this time, humanity seems to be estimating that both power and possibility will be off the charts. Why? Because getting this wrong can be so negatively impactful that it makes sense to move forward as if GAI will inevitably exist. Imagine supposing that this will all turn out to be fluff and GAI will never work, so you stop investing in it. Now imagine what happens if you're wrong and your enemy gets it to work first. This isn't some arguing device for AI-inevitabilists. It's knowledge of human nature, and it's been repeating itself for millennia. If the author believes that's going to suddenly change, they really should back that up with what, exactly, has changed in human nature. This is a fantastic framing method. Anyone who sees the future differently to you can be brushed aside as “an inevitablist,” and the only conversations worth engaging are those that already accept your premise. --- This argument so easily commits sudoku that I couldn't help myself. It's philosophical relativism, and self-immolates for the same reason -- it's inconsistent. It eats itself. I really like what is hidden between the lines of this text, it is only something a human can understand. The entire comment section over here reflects the uncanny valley. This blog post is a work of art LOL Wasn’t crypto supposed to have replaced fiat currency by now, or something? A few days ago I saw a nice tweet being shared and it wend something like: I am not allowed to use my airco as it eats to much power and we must think about the environment. Meanwhile: people non-stop generating rule 34 images using AI... I wish I can show my gratitude more than this but i'm truly grateful thank you for restoring my happiness and hopes back and more than i could even ask for, my marriage of 2 years crashed out of no good reason but i'm grateful Dr Alfred helped me restore everything back and now my husband even loves me more than he ever used to love me , I can't repay you Dr Alfred, I just want to say thank you and let everyone know that you are truly amazing, and indeed your love spell is powerful, you can reach him on dralfredspellhome@gmail.com or on whatsapp +2348134653457 Disclaimer - I am building an AI web retriever (Linkup.so) so I have a natural bias - LLMs aren’t just a better Google, they’re a redefinition of search itself. Traditional search is an app: you type, scroll through ads and 10 blue links, and dig for context. That model worked when the web was smaller, but now it’s overwhelming. LLMs shift search to an infrastructure, a way to get contextualized, synthesized answers directly, tailored to your specific need. Yes, they can hallucinate, but so can the web. It’s not about replacing Google—it’s about replacing the experience of searching (actually they probably will less and less 'experience' of searching) I think you are confusing ""I don't like it"" with ""It's not going to happen"". Just because you don't like it, it doesn't mean it's not going to happen. Observe the world without prejudice. Think rationally without prejudice. Not sure I get the author of this piece. The tech leaders are clearly saying AI is inevitable, they're not saying LLMs are inevitable. Big tech is constantly working on new types of AI such as world models. > ""AI ..."" > I’m not convinced that LLMs are the future. Was this an intentional bait/switch? LLM != AI. I'm quite sure LLMs are not the future. It's merely the step after AlexNet, AlphaGo, and before the next major advancement. AI is not inevitable, because technological progress in general is not inevitable. It is shapeable by economic incentives just like everything else. It can be ground into powder by resource starvation. We've long known that certain forms of financial bounties levied upon scientists working at the frontier of sciences we want to freeze in place work effectively with a minimum of policing and international cooperation. If a powerful country is willing to be a jerk (heavens!) and allow these kinds of bounties to be turned in even on extranationals, you don't need the international cooperation. But you do get a way to potentially kickstart a new Nash equilibrium that keeps itself going as soon as other countries adopt the same bounty-based policy. This mechanism has been floating around for at least a decade now. It's not news. Even the most inevitable seeming scientific developments can be effectively rerouted around using it. The question is whether you genuinely, earnestly believe what lies beyond the frontier is too dangerous to be let out, and in almost all cases the answer to that should be no. I post this mostly because inevitabilist arguments will always retain their power so long as you can come up with a coherent profit motive for something to be pursued. You don't get far with good-feeling spiels that amount to plaintive cries in a tornado. You need actual object level proposals on how to make the inevitable evitable. Language is not knowledge and knowledge when reduced to a language becomes here say until it is redone and implemented in our context. Both of them have nothing to do with wisdom. LLM's hash out our language and art to death but AI doesn't mind what they mean to us. Without our constraints and use, they would stop running. We should be building guardian angels to save us from ourselves and not evil demons to conquer the world. - John Eischen © adagp paris art humanitarian use is authorized except for any Al uses The article talks about being thrown off-balance by debating tricks and then proceed to do just that with a kind of bait and switch from talking about AI to talking about LLMs. Eg. it quotes >“AI is the new electricity.” – Andrew Ng as framing AI as kind of inevitable and then flips to >I’m not convinced that LLMs are the future. It seems to me AI is inevitable and LLMs will be replaced soon with some better algorithm. It's like video is inevitable but betamax wasn't. Two different things. Like a lot of blog posts, this feels like a premise worth exploring, lacking a critical exploration of that premise. Yes, ""inevitabilism"" is a thing, both in tech and in politics. But, crucially, it's not always wrong! Other comments have pointed out examples, such as the internet in the 90s. But when considering new cultural and technological developments that seem like a glimpse of the future, how do we know if they're an inevitability or not? The post says: > what I’m most certain of is that we have choices about what our future should look like, and how we choose to use machines to build it. To me, that sounds like mere wishful thinking. Yeah, sometimes society can turn back the tide of harmful developments; for instance, the ozone layer is well on its way to complete recovery. Other times, even when public opinion is mixed, such as with bitcoin, the technology does become quite successful, but doesn't seem to become quite as ubiquitous as its most fervent adherents expect. So how do we know which category LLM usage falls into? I don't know the answer, because I think it's a difficult thing to know in advance."
42473321,OpenAI O3 breakthrough high score on ARC-AGI-PUB,nan,https://arcprize.org/blog/oai-o3-pub-breakthrough,maurycy,1724,2024-12-20T18:11:13+00:00,,query,50,"Efficiency is now key. ~=$3400 per single task to meet human performance on this benchmark is a lot. Also it shows the bullets as ""ARC-AGI-TUNED"", which makes me think they did some undisclosed amount of fine-tuning (eg. via the API they showed off last week), so even more compute went into this task. We can compare this roughly to a human doing ARC-AGI puzzles, where a human will take (high variance in my subjective experience) between 5 second and 5 minutes to solve the task. (So i'd argue a human is at 0.03USD - 1.67USD per puzzle at 20USD/hr, and they include in their document an average mechancal turker at $2 USD task in their document) Going the other direction: I am interpreting this result as human level reasoning now costs (approximately) 41k/hr to 2.5M/hr with current compute. Super exciting that OpenAI pushed the compute out this far so we could see he O-series scaling continue and intersect humans on ARC, now we get to work towards making this economical! The programming task they gave o3-mini high (creating Python server that allows chatting with OpenAI API and run some code in terminal) didn't seem very hard? Strange choice of example for something that's claimed to be a big step forwards. YT timestamped link: https://www.youtube.com/watch?v=SKBG1sqdyIU&t=768s (thanks for the fixed link @photonboom) Updated: I gave the task to Claude 3.5 Sonnet and it worked first shot: https://claude.site/artifacts/36cecd49-0e0b-4a8c-befa-faa5aa... Congratulations to Francois Chollet on making the most interesting and challenging LLM benchmark so far. A lot of people have criticized ARC as not being relevant or indicative of true reasoning, but I think it was exactly the right thing. The fact that scaled reasoning models are finally showing progress on ARC proves that what it measures really is relevant and important for reasoning. It's obvious to everyone that these models can't perform as well as humans on everyday tasks despite blowout scores on the hardest tests we give to humans. Yet nobody could quantify exactly the ways the models were deficient. ARC is the best effort in that direction so far. We don't need more ""hard"" benchmarks. What we need right now are ""easy"" benchmarks that these models nevertheless fail. I hope Francois has something good cooked up for ARC 2! Human performance is 85% [1]. o3 high gets 87.5%. This means we have an algorithm to get to human level performance on this task. If you think this task is an eval of general reasoning ability, we have an algorithm for that now. There's a lot of work ahead to generalize o3 performance to all domains. I think this explains why many researchers feel AGI is within reach, now that we have an algorithm that works. Congrats to both Francois Chollet for developing this compelling eval, and to the researchers who saturated it! [1] https://x.com/SmokeAwayyy/status/1870171624403808366 , https://arxiv.org/html/2409.01374v1 Let me go against some skeptics and explain why I think full o3 is pretty much AGI or at least embodies most essential aspects of AGI. What has been lacking so far in frontier LLMs is the ability to reliably deal with the right level of abstraction for a given problem. Reasoning is useful but often comes out lacking if one cannot reason at the right level of abstraction. (Note that many humans can't either when they deal with unfamiliar domains, although that is not the case with these models.) ARC has been challenging precisely because solving its problems often requires: 1) using multiple different *kinds* of core knowledge [1], such as symmetry, counting, color, AND 2) using the right level(s) of abstraction Achieving human-level performance in the ARC benchmark, as well as top human performance in GPQA, Codeforces, AIME, and Frontier Math suggests the model can potentially solve any problem at the human level if it possesses essential knowledge about it. Yes, this includes out-of-distribution problems that most humans can solve. It might not yet be able to generate highly novel theories, frameworks, or artifacts to the degree that Einstein, Grothendieck, or van Gogh could. But not many humans can either. [1] https://www.harvardlds.org/wp-content/uploads/2017/01/Spelke... ADDED: Thanks to the link to Chollet's posts by lswainemoore below. I've analyzed some easy problems that o3 failed at. They involve spatial intelligence, including connection and movement. This skill is very hard to learn from textual and still image data. I believe this sort of core knowledge is learnable through movement and interaction data in a simulated world and it will not present a very difficult barrier to cross. (OpenAI purchased a company behind a Minecraft clone a while ago. I've wondered if this is the purpose.) Incredibly impressive. Still can't really shake the feeling that this is o3 gaming the system more than it is actually being able to reason. If the reasoning capabilities are there, there should be no reason why it achieves 90% on one version and 30% on the next. If a human maintains the same performance across the two versions, an AI with reason should too. The cost to run the highest performance o3 model is estimated to be somewhere between $2,000 and $3,400 per task.[1] Based on these estimates, o3 costs about 100x what it would cost to have a human perform the exact same task. Many people are therefore dismissing the near-term impact of these models because of these extremely expensive costs. I think this is a mistake. Even if very high costs make o3 uneconomic for businesses, it could be an epoch defining development for nation states, assuming that it is true that o3 can reason like an averagely intelligent person. Consider the following questions that a state actor might ask itself: What is the cost to raise and educate an average person? Correspondingly, what is the cost to build and run a datacenter with a nuclear power plant attached to it? And finally, how many person-equivilant AIs could be run in parallel per datacenter? There are many state actors, corporations, and even individual people who can afford to ask these questions. There are also many things that they'd like to do but can't because there just aren't enough people available to do them. o3 might change that despite its high cost. So if it is true that we've now got something like human-equivilant intelligence on demand - and that's a really big if - then we may see its impacts much sooner than we would otherwise intuit, especially in areas where economics takes a back seat to other priorities like national security and state competitiveness. [1] https://news.ycombinator.com/item?id=42473876 Direct quote from the ARC-AGI blog: “SO IS IT AGI? ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI – as we've repeated dozens of times this year. It's a research tool designed to focus attention on the most challenging unsolved problems in AI, a role it has fulfilled well over the past five years. Passing ARC-AGI does not equate achieving AGI, and, as a matter of fact, I don't think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence. Furthermore, early data points suggest that the upcoming ARC-AGI-2 benchmark will still pose a significant challenge to o3, potentially reducing its score to under 30% even at high compute (while a smart human would still be able to score over 95% with no training). This demonstrates the continued possibility of creating challenging, unsaturated benchmarks without having to rely on expert domain knowledge. You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.” The high compute variant sounds like it costed around *$350,000* which is kinda wild. Lol the blog post specifically mentioned how OpenAPI asked ARC-AGI to not disclose the exact cost for the high compute version. Also, 1 odd thing I noticed is that the graph in their blog post shows the top 2 scores as “tuned” (this was not displayed in the live demo graph). This suggest in those cases that the model was trained to better handle these types of questions, so I do wonder about data / answer contamination in those cases… Sad to see everyone so focused on compute expense during this massive breakthrough. GPT-2 originally cost $50k to train, but now can be trained for ~$150. The key part is that scaling test-time compute will likely be a key to achieving AGI/ASI. Costs will definitely come down as is evidenced by precedents, Moore’s law, o3-mini being cheaper than o1 with improved performance, etc. Whenever a benchmark that was thought to be extremely difficult is (nearly) solved, it's a mix of two causes. One is that progress on AI capabilities was faster than we expected, and the other is that there was an approach that made the task easier than we expected. I feel like the there's a lot of the former here, but the compute cost per task (thousands of dollars to solve one little color grid puzzle??) suggests to me that there's some amount of the latter. Chollet also mentions ARC-AGI-2 might be more resistant to this approach. Of course, o3 looks strong on other benchmarks as well, and sometimes ""spend a huge amount of compute for one problem"" is a great feature to have available if it gets you the answer you needed. So even if there's some amount of ""ARC-AGI wasn't quite as robust as we thought"", o3 is clearly a very powerful model. I’m not sure if people realize what a weird test this is. They’re these simple visual puzzles that people can usually solve at a glance, but for the LLMs, they’re converted into a json format, and then the LLMs have to reconstruct the 2D visual scene from the json and pick up the patterns. If humans were given the json as input rather than the images, they’d have a hard time, too. I would like to see this repeated with my highly innovative HARC-HAGI, which is ARC-AGI but it uses hexagons instead of squares. I suspect humans would only make slightly more brain farts on HARC-HAGI than ARC-AGI, but O3 would fail very badly since it almost certainly has been specifically trained on squares. I am not really trying to downplay O3. But this would be a simple test as to whether O3 is truly ""a system capable of adapting to tasks it has never encountered before"" versus novel ARC-AGI tasks it hasn't encountered before. Very cool. I recommend scrolling down to look at the example problem that O3 still can’t solve. It’s clear what goes on in the human brain to solve this problem: we look at one example, hypothesize a simple rule that explains it, and then check that hypothesis against the other examples. It doesn’t quite work, so we zoom into an example that we got wrong and refine the hypothesis so that it solves that sample. We keep iterating in this fashion until we have the simplest hypothesis that satisfies all the examples. In other words, how humans do science - iteratively formulating, rejecting and refining hypotheses against collected data. From this it makes sense why the original models did poorly and why iterative chain of thought is required - the challenge is designed to be inherently iterative such that a zero shot model, no matter how big, is extremely unlikely to get it right on the first try. Of course, it also requires a broad set of human-like priors about what hypotheses are “simple”, based on things like object permanence, directionality and cardinality. But as the author says, these basic world models were already encoded in the GPT 3/4 line by simply training a gigantic model on a gigantic dataset. What was missing was iterative hypothesis generation and testing against contradictory examples. My guess is that O3 does something like this: 1. Prompt the model to produce a simple rule to explain the nth example (randomly chosen) 2. Choose a different example, ask the model to check whether the hypothesis explains this case as well. If yes, keep going. If no, ask the model to revise the hypothesis in the simplest possible way that also explains this example. 3. Keep iterating over examples like this until the hypothesis explains all cases. Occasionally, new revisions will invalidate already solved examples. That’s fine, just keep iterating. 4. Induce randomness in the process (through next-word sampling noise, example ordering, etc) to run this process a large number of times, resulting in say 1,000 hypotheses which all explain all examples. Due to path dependency, anchoring and consistency effects, some of these paths will end in awful hypotheses - super convoluted and involving a large number of arbitrary rules. But some will be simple. 5. Ask the model to select among the valid hypotheses (meaning those that satisfy all examples) and choose the one that it views as the simplest for a human to discover. My initial impression: it's very impressive and very exciting. My skeptical impression: it's complete hubris to conflate ARC or any benchmark with truly general intelligence. I know my skepticism here is identical to moving goalposts. More and more I am shifting my personal understanding of general intelligence as a phenomenon we will only ever be able to identify with the benefit of substantial retrospect. As it is with any sufficiently complex program, if you could discern the result beforehand, you wouldn't have had to execute the program in the first place. I'm not trying to be a downer on the 12th day of Christmas. Perhaps because my first instinct is childlike excitement, I'm trying to temper it with a little reason. How do the organisers keep the private test set private? Does openAI hand them the model for testing? If they use a model API, then surely OpenAI has access to the private test set questions and can include it in the next round of training? (I am sure I am missing something.) Complete aside here: I used to do work with amputees and prosthetics. There is a standardized test (and I just cannot remember the name) that fits in a briefcase. It's used for measuring the level of damage to the upper limbs and for prosthetic grading. Basically, it's got the dumbest and simplest things in it. Stuff like a lock and key, a glass of water and jug, common units of currency, a zipper, etc. It tests if you can do any of those common human tasks. Like pouring a glass of water, picking up coins from a flat surface (I chew off my nails so even an able person like me fails that), zip up a jacket, lock your own door, put on lipstick, etc. We had hand prosthetics that could play Mozart at 5x speed on a baby grand, but could not pick up a silver dollar or zip a jacket even a little bit. To the patients, the hands were therefore about as useful as a metal hook (a common solution with amputees today, not just pirates!). Again, a total aside here, but your comment just reminded me of that brown briefcase. Life, it turns out, is a lot more complex than we give it credit for. Even pouring the OJ can be, in rare cases, transcendent. Isn’t this like a brute force approach? Given it costs $ 3000 per task, thats like 600 GPU hours (h100 at Azure) In that amount of time the model can generate millions of chains of thoughts and then spend hours reviewing them or even testing them out one by one. Kind of like trying until something sticks and that happens to solve 80% of ARC. I feel like reasoning works differently in my brain. ;) OpenAI spent approximately $1,503,077 to smash the SOTA on ARC-AGI with their new o3 model semi-private evals (100 tasks): 75.7% @ $2,012 total/100 tasks (~$20/task) with just 6 samples & 33M tokens processed in ~1.3 min/task and a cost of $2012 The “low-efficiency” setting with 1024 samples scored 87.5% but required 172x more compute. If we assume compute spent and cost are proportional, then OpenAI might have just spent ~$346.064 for the low efficiency run on the semi-private eval. On the public eval they might have spent ~$1.148.444 to achieve 91.5% with the low efficiency setting. (high-efficiency mode: $6677) OpenAI just spent more money to run an eval on ARC than most people spend on a full training run. O3 High (tuned) model scored an 88% at what looks like $6,000/task haha I think soon we'll be pricing any kind of tasks by their compute costs. So basically, human = $50/task, AI = $6,000/task, use human. If AI beats human, use AI? Ofc that's considering both get 100% scores on the task There are new research where chain of thoughts is happening in latent spaces and not in English. They demonstrated better results since language is not as expressive as those concepts that can be represented in the layers before decoder. I wonder if o3 is doing that? Just as an aside, I've personally found o1 to be completely useless for coding. Sonnet 3.5 remains the king of the hill by quite some margin The LLM community has come up with tests they call 'Misguided Attention'[1] where they prompt the LLM with a slightly altered version of common riddles / tests etc. This often causes the LLM to fail. For example I used the prompt ""As an astronaut in China, would I be able to see the great wall?"" and since the training data for all LLMs is full of text dispelling the common myth that the great wall is visible from space, LLMs do not notice the slight variation that the astronaut is IN China. This has been a sobering reminder to me as discussion of AGI heats up. [1] https://github.com/cpldcpu/MisguidedAttention ""Note on ""tuned"": OpenAI shared they trained the o3 we tested on 75% of the Public Training set. They have not shared more details. We have not yet tested the ARC-untrained model to understand how much of the performance is due to ARC-AGI data."" Really want to see the number of training pairs needed to achieve this socre. If it only takes a few pairs, say 100 pairs, I would say it is amazing! As an aside, I'm a little miffed that the benchmark calls out ""AGI"" in the name, but then heavily cautions that it's necessary but insufficient for AGI. > ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI Can I just say what a dick move it was to do this as a 12 days of Christmas. I mean to be honest I agree with the arguments this isn’t as impressive as my initial impression, but they clearly intended it to be shocking/a show of possible AGI, which is rightly scary. It feels so insensitive to that right before a major holiday when the likely outcome is a lot of people feeling less secure in their career/job/life. Thanks again openAI for showing us you don’t give a shit about actual people. In (1) the author use a technique to improve the performance of an LLM, he trained sonnet 3.5 to obtain 53,6% in the arc-agi-pub benchmark moreover he said that more computer power would give better results. So the results of o3 could be produced in this way using the same method with more computer power, so if this is the case the result of o3 is not very interesting. (1) https://params.com/@jeremy-berman/arc-agi I just noticed this bit: >> Second, you need the ability to recombine these functions into a brand new program when facing a new task – a program that models the task at hand. Program synthesis. ""Program synthesis"" is here used in an entirely idiosyncratic manner, to mean ""combining programs"". Everyone else in CS and AI for the last many decades has used ""Program Synthesis"" to mean ""generating a program that satisfies a specification"". Note that ""synthesis"" can legitimately be used to mean ""combining"". In Greek it translates literally to ""putting [things] together"": ""Syn"" (plus) ""thesis"" (place). But while generating programs by combining parts of other programs is an old-fashioned way to do Program Synthesis, in the standard sense, the end result is always desired to be a program. The LLMs used in the article to do what F. Chollet calls ""Porgram Synthesis"" generate no code. One thing I have not seen commented on is that ARC-AGI is a visual benchmark but LLMs are primarily text. For instance when I see one of the ARC-AGI puzzles, I have a visual representation in my brain and apply some sort of visual reasoning solve it. I can ""see"" in my mind's eye the solution to the puzzle. If I didn't have that capability, I don't think I could reason through words how to go about solving it - it would certainly be much more difficult. I hypothesize that something similar is going on here. OpenAI has not published (or I have not seen) the number of reasoning tokens it took to solve these - we do know that each tasks was thoussands of dollars. If ""a picture is worth a thousand words"", could we make AI systems that can reason visually with much better performance? > o3 fixes the fundamental limitation of the LLM paradigm – the inability to recombine knowledge at test time I don't understand this mindset. We have all experienced that LLMs can produce words never spoken before. Thus there is recombination of knowledge at play. We might not be satisfied with the depth/complexity of the combination, but there isn't any reason to believe something fundamental is missing. Given more compute and enough recursiveness we should be able to reach any kind of result from the LLM. The linked article says that LLMs are like a collection of vector programs. It has always been my thinking that computations in vector space are easy to make turing complete if we just have an eigenvector representation figured out. The cost axis is interesting. O3 Low is $10+ per task and 03 High is over $1000 (it's logarithmic graph so it's like $50 and $5000 respectively?) I'm 22 and have no clue what I'm meant to do in a world where this is a thing. I'm moving to a semi rural, outdoorsy area where they teach data science and marine science and I can enjoy my days hiking, and the march of technology is a little slower. I know this will disrupt so much of our way of life, so I'm chasing what fun innocent years are left before things change dramatically. The chart is super misleading, since the test was obscure until recently. A few months ago he announced he'd made the only good AGI test and offered a cash prize for solving it, only to find out in as much time that it's no different from other benchmarks. Isn't this at the level now where it can sort of self improve. My guess is that they will just use it to improve the model and the cost they are showing per evaluation will go down drastically. So, next step in reasoning is open world reasoning now? Deciphering patterns in natural language is more complex than these puzzles. If you train your AI to solve these puzzles, we end up in the same spot. The difficulty of solving would be with creating training data for a foreign medium. The ""tokens"" are the grids and squares instead of words (for words, we have the internet of words, solving that). If we're inferring the answers of the block patterns from minimal or no additional training, it's very impressive, but how much time have they had to work on O3 after sharing puzzle data with O1? Seems there's some room for questionable antics! I have a very naive question. Why is the ARC challenge difficult but coding problems are easy? The two examples they give for ARC (border width and square filling) are much simpler than pattern awareness I see simple models find in code everyday. What am I misunderstanding? Is it that one is a visual grid context which is unfamiliar? It sucks that I would love to be excited about this... but I mostly feel anxiety and sadness. I was impressed until I read the caveat about the high-compute version using 172x more compute. Assuming for a moment that the cost per task has a linear relationship with compute, then it costs a little more than $1 million to get that score on the public eval. The results are cool, but man, this sounds like such a busted approach. > You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible. You'll know AGI is here when traditional captchas stop being a thing due to their lack of usefulness. How can there be ""private"" taks when you have use the OpenAI API to run queries? OpenAI sees everything. For many people and businesses, navigating the frequently dangerous landscape of financial loss can be an intimidating and overwhelming process. Nevertheless, the knowledgeable staff at Wizard Hilton Cyber Tech provides a ray of hope and direction with their indispensable range of services. Their offerings are based on a profound grasp of the far-reaching and terrible effects that financial setbacks, whether they be the result of cyberattacks, data breaches, or other unforeseen tragedies, can have. Their highly-trained analysts work tirelessly to assess the scope of the damage, identifying the root causes and developing tailored strategies to mitigate the fallout. From recovering lost or corrupted data to restoring compromised systems and securing networks, Wizard Hilton Cyber Tech employs the latest cutting-edge technologies and industry best practices to help clients regain their financial footing. But their support goes beyond the technical realm, as their compassionate case managers provide a empathetic ear and practical advice to navigate the emotional and logistical challenges that often accompany financial upheaval. With a steadfast commitment to client success, Wizard Hilton Cyber Tech is a trusted partner in weathering the storm of financial loss, offering the essential services and peace of mind needed to emerge stronger and more resilient than before. At about 12-14 minutes in OpenAI's YouTube vid they show that o3-mini beats o1 on Codeforces despite using much less compute. I feel like AI is already changing how we work and live - I've been using it myself for a lot of my development work. Though, what I'm really concerned about is what happens when it gets smart enough to do pretty much everything better (or even close) than humans can. We're talking about a huge shift where first knowledge workers get automated, then physical work too. The thing is, our whole society is built around people working to earn money, so what happens when AI can do most jobs? It's not just about losing jobs - it's about how people will pay for basic stuff like food and housing, and what they'll do with their lives when work isn't really a thing anymore. Or do people feel like there will be jobs safe from AI? (hopefully also fulfilling) Some folks say we could fix this with universal basic income, where everyone gets enough money to live on, but I'm not optimistic that it'll be an easy transition. Plus, there's this possibility that whoever controls these 'AGI' systems basically controls everything. We definitely need to figure this stuff out before it hits us, because once these changes start happening, they're probably going to happen really fast. It's kind of like we're building this awesome but potentially dangerous new technology without really thinking through how it's going to affect regular people's lives. I feel like we need a parachute before we attempt a skydive. Some people feel pretty safe about their jobs and think they can't be replaced. I don't think that will be the case. Even if AI doesn't take your job, you now have a lot more unemployed people competing for the same job that is safe from AI. The more Hacker News worthy discussion is the part where the author talks about search through the possible mini-program space of LLMs. It makes sense because tree search can be endlessly optimized. In a sense, LLMs turn the unstructured, open system of general problems into a structured, closed system of possible moves. Which is really cool, IMO. It seems O3 following trend of Chess engine that you can cut your search depth depends on state. It's good for games with clear signal of success (Win/Lose for Chess, tests for programming). One of the blocker for AGI is we don't have clear evaluation for most of our tasks and we cannot verify them fast enough. Interesting that in the video, there is an admission that they have been targeting this benchmark. A comment that was quickly shut down by Sam. A bit puzzling to me. Why does it matter ? If anyone else is curious about which ARC-AGI public eval puzzles o3 got right vs wrong (and its attempts at the ones it did get right), here's a quick visualization: https://arcagi-o3-viz.netlify.app The general message here seems to be that inference-time brute-forcing works as long as you have a good search and evaluation strategy. We’ve seemingly hit a ceiling on the base LLM forward-pass capability so any further wins are going to be in how we juggle multiple inferences to solve the problem space. It feels like a scripting problem now. Which is cool! A fun space for hacker-engineers. Also: > My mental model for LLMs is that they work as a repository of vector programs. When prompted, they will fetch the program that your prompt maps to and ""execute"" it on the input at hand. LLMs are a way to store and operationalize millions of useful mini-programs via passive exposure to human-generated content. I found this such an intriguing way of thinking about it. Terrifying. This news makes me happy I save all my money. My only hope for the future is that I can retire early before I’m unemployable Am I understanding correctly, and the only thing with a bit of actual data released so far is the ARC-AGI piece from Francois Chollet? And every other claim has no further data released on it? Serious question. I've browsed around, looked for the official release, but it seems to be just hear-say for now, except for the few little bits in the ARC-AGI article. So some of the reactions seems quite far-fetched. I was quite amazed at first seeing the benchmarks, but then actually read the ARC-AGI article and a few other things about how it worked, learned a bit more about the different benchmarks, and realised we've no proper idea yet how o3 is working under the hood, the thing isn't even realeased. It could be doing the same thing that chess-engines do except in several specific domains. Which would be very cool, but not necessarily ""intelligent"" or ""generally intelligent"" in any sense whatsoever! Will that kind of model lead to finding novel mathematical proofs, or actually ""reasoning"" or ""thinking"" in any way similar to a human, remains entirely uncertain. This might sound dumb, and I'm not sure how to phrase this, but is there a way to measure the raw model output quality without all the more ""traditional"" engineering work (mountain of `if` statements I assume) done on top of the output? And if so, would that be a better measure of when scaling up the input data will start showing diminishing returns? (I know very little about the guts of LLMs or how they're tested, so the distinction between ""raw"" output and the more deterministic engineering work might be incorrect)","OpenAI O3 breakthrough high score on ARC-AGI-PUB nan Efficiency is now key. ~=$3400 per single task to meet human performance on this benchmark is a lot. Also it shows the bullets as ""ARC-AGI-TUNED"", which makes me think they did some undisclosed amount of fine-tuning (eg. via the API they showed off last week), so even more compute went into this task. We can compare this roughly to a human doing ARC-AGI puzzles, where a human will take (high variance in my subjective experience) between 5 second and 5 minutes to solve the task. (So i'd argue a human is at 0.03USD - 1.67USD per puzzle at 20USD/hr, and they include in their document an average mechancal turker at $2 USD task in their document) Going the other direction: I am interpreting this result as human level reasoning now costs (approximately) 41k/hr to 2.5M/hr with current compute. Super exciting that OpenAI pushed the compute out this far so we could see he O-series scaling continue and intersect humans on ARC, now we get to work towards making this economical! The programming task they gave o3-mini high (creating Python server that allows chatting with OpenAI API and run some code in terminal) didn't seem very hard? Strange choice of example for something that's claimed to be a big step forwards. YT timestamped link: https://www.youtube.com/watch?v=SKBG1sqdyIU&t=768s (thanks for the fixed link @photonboom) Updated: I gave the task to Claude 3.5 Sonnet and it worked first shot: https://claude.site/artifacts/36cecd49-0e0b-4a8c-befa-faa5aa... Congratulations to Francois Chollet on making the most interesting and challenging LLM benchmark so far. A lot of people have criticized ARC as not being relevant or indicative of true reasoning, but I think it was exactly the right thing. The fact that scaled reasoning models are finally showing progress on ARC proves that what it measures really is relevant and important for reasoning. It's obvious to everyone that these models can't perform as well as humans on everyday tasks despite blowout scores on the hardest tests we give to humans. Yet nobody could quantify exactly the ways the models were deficient. ARC is the best effort in that direction so far. We don't need more ""hard"" benchmarks. What we need right now are ""easy"" benchmarks that these models nevertheless fail. I hope Francois has something good cooked up for ARC 2! Human performance is 85% [1]. o3 high gets 87.5%. This means we have an algorithm to get to human level performance on this task. If you think this task is an eval of general reasoning ability, we have an algorithm for that now. There's a lot of work ahead to generalize o3 performance to all domains. I think this explains why many researchers feel AGI is within reach, now that we have an algorithm that works. Congrats to both Francois Chollet for developing this compelling eval, and to the researchers who saturated it! [1] https://x.com/SmokeAwayyy/status/1870171624403808366 , https://arxiv.org/html/2409.01374v1 Let me go against some skeptics and explain why I think full o3 is pretty much AGI or at least embodies most essential aspects of AGI. What has been lacking so far in frontier LLMs is the ability to reliably deal with the right level of abstraction for a given problem. Reasoning is useful but often comes out lacking if one cannot reason at the right level of abstraction. (Note that many humans can't either when they deal with unfamiliar domains, although that is not the case with these models.) ARC has been challenging precisely because solving its problems often requires: 1) using multiple different *kinds* of core knowledge [1], such as symmetry, counting, color, AND 2) using the right level(s) of abstraction Achieving human-level performance in the ARC benchmark, as well as top human performance in GPQA, Codeforces, AIME, and Frontier Math suggests the model can potentially solve any problem at the human level if it possesses essential knowledge about it. Yes, this includes out-of-distribution problems that most humans can solve. It might not yet be able to generate highly novel theories, frameworks, or artifacts to the degree that Einstein, Grothendieck, or van Gogh could. But not many humans can either. [1] https://www.harvardlds.org/wp-content/uploads/2017/01/Spelke... ADDED: Thanks to the link to Chollet's posts by lswainemoore below. I've analyzed some easy problems that o3 failed at. They involve spatial intelligence, including connection and movement. This skill is very hard to learn from textual and still image data. I believe this sort of core knowledge is learnable through movement and interaction data in a simulated world and it will not present a very difficult barrier to cross. (OpenAI purchased a company behind a Minecraft clone a while ago. I've wondered if this is the purpose.) Incredibly impressive. Still can't really shake the feeling that this is o3 gaming the system more than it is actually being able to reason. If the reasoning capabilities are there, there should be no reason why it achieves 90% on one version and 30% on the next. If a human maintains the same performance across the two versions, an AI with reason should too. The cost to run the highest performance o3 model is estimated to be somewhere between $2,000 and $3,400 per task.[1] Based on these estimates, o3 costs about 100x what it would cost to have a human perform the exact same task. Many people are therefore dismissing the near-term impact of these models because of these extremely expensive costs. I think this is a mistake. Even if very high costs make o3 uneconomic for businesses, it could be an epoch defining development for nation states, assuming that it is true that o3 can reason like an averagely intelligent person. Consider the following questions that a state actor might ask itself: What is the cost to raise and educate an average person? Correspondingly, what is the cost to build and run a datacenter with a nuclear power plant attached to it? And finally, how many person-equivilant AIs could be run in parallel per datacenter? There are many state actors, corporations, and even individual people who can afford to ask these questions. There are also many things that they'd like to do but can't because there just aren't enough people available to do them. o3 might change that despite its high cost. So if it is true that we've now got something like human-equivilant intelligence on demand - and that's a really big if - then we may see its impacts much sooner than we would otherwise intuit, especially in areas where economics takes a back seat to other priorities like national security and state competitiveness. [1] https://news.ycombinator.com/item?id=42473876 Direct quote from the ARC-AGI blog: “SO IS IT AGI? ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI – as we've repeated dozens of times this year. It's a research tool designed to focus attention on the most challenging unsolved problems in AI, a role it has fulfilled well over the past five years. Passing ARC-AGI does not equate achieving AGI, and, as a matter of fact, I don't think o3 is AGI yet. o3 still fails on some very easy tasks, indicating fundamental differences with human intelligence. Furthermore, early data points suggest that the upcoming ARC-AGI-2 benchmark will still pose a significant challenge to o3, potentially reducing its score to under 30% even at high compute (while a smart human would still be able to score over 95% with no training). This demonstrates the continued possibility of creating challenging, unsaturated benchmarks without having to rely on expert domain knowledge. You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.” The high compute variant sounds like it costed around *$350,000* which is kinda wild. Lol the blog post specifically mentioned how OpenAPI asked ARC-AGI to not disclose the exact cost for the high compute version. Also, 1 odd thing I noticed is that the graph in their blog post shows the top 2 scores as “tuned” (this was not displayed in the live demo graph). This suggest in those cases that the model was trained to better handle these types of questions, so I do wonder about data / answer contamination in those cases… Sad to see everyone so focused on compute expense during this massive breakthrough. GPT-2 originally cost $50k to train, but now can be trained for ~$150. The key part is that scaling test-time compute will likely be a key to achieving AGI/ASI. Costs will definitely come down as is evidenced by precedents, Moore’s law, o3-mini being cheaper than o1 with improved performance, etc. Whenever a benchmark that was thought to be extremely difficult is (nearly) solved, it's a mix of two causes. One is that progress on AI capabilities was faster than we expected, and the other is that there was an approach that made the task easier than we expected. I feel like the there's a lot of the former here, but the compute cost per task (thousands of dollars to solve one little color grid puzzle??) suggests to me that there's some amount of the latter. Chollet also mentions ARC-AGI-2 might be more resistant to this approach. Of course, o3 looks strong on other benchmarks as well, and sometimes ""spend a huge amount of compute for one problem"" is a great feature to have available if it gets you the answer you needed. So even if there's some amount of ""ARC-AGI wasn't quite as robust as we thought"", o3 is clearly a very powerful model. I’m not sure if people realize what a weird test this is. They’re these simple visual puzzles that people can usually solve at a glance, but for the LLMs, they’re converted into a json format, and then the LLMs have to reconstruct the 2D visual scene from the json and pick up the patterns. If humans were given the json as input rather than the images, they’d have a hard time, too. I would like to see this repeated with my highly innovative HARC-HAGI, which is ARC-AGI but it uses hexagons instead of squares. I suspect humans would only make slightly more brain farts on HARC-HAGI than ARC-AGI, but O3 would fail very badly since it almost certainly has been specifically trained on squares. I am not really trying to downplay O3. But this would be a simple test as to whether O3 is truly ""a system capable of adapting to tasks it has never encountered before"" versus novel ARC-AGI tasks it hasn't encountered before. Very cool. I recommend scrolling down to look at the example problem that O3 still can’t solve. It’s clear what goes on in the human brain to solve this problem: we look at one example, hypothesize a simple rule that explains it, and then check that hypothesis against the other examples. It doesn’t quite work, so we zoom into an example that we got wrong and refine the hypothesis so that it solves that sample. We keep iterating in this fashion until we have the simplest hypothesis that satisfies all the examples. In other words, how humans do science - iteratively formulating, rejecting and refining hypotheses against collected data. From this it makes sense why the original models did poorly and why iterative chain of thought is required - the challenge is designed to be inherently iterative such that a zero shot model, no matter how big, is extremely unlikely to get it right on the first try. Of course, it also requires a broad set of human-like priors about what hypotheses are “simple”, based on things like object permanence, directionality and cardinality. But as the author says, these basic world models were already encoded in the GPT 3/4 line by simply training a gigantic model on a gigantic dataset. What was missing was iterative hypothesis generation and testing against contradictory examples. My guess is that O3 does something like this: 1. Prompt the model to produce a simple rule to explain the nth example (randomly chosen) 2. Choose a different example, ask the model to check whether the hypothesis explains this case as well. If yes, keep going. If no, ask the model to revise the hypothesis in the simplest possible way that also explains this example. 3. Keep iterating over examples like this until the hypothesis explains all cases. Occasionally, new revisions will invalidate already solved examples. That’s fine, just keep iterating. 4. Induce randomness in the process (through next-word sampling noise, example ordering, etc) to run this process a large number of times, resulting in say 1,000 hypotheses which all explain all examples. Due to path dependency, anchoring and consistency effects, some of these paths will end in awful hypotheses - super convoluted and involving a large number of arbitrary rules. But some will be simple. 5. Ask the model to select among the valid hypotheses (meaning those that satisfy all examples) and choose the one that it views as the simplest for a human to discover. My initial impression: it's very impressive and very exciting. My skeptical impression: it's complete hubris to conflate ARC or any benchmark with truly general intelligence. I know my skepticism here is identical to moving goalposts. More and more I am shifting my personal understanding of general intelligence as a phenomenon we will only ever be able to identify with the benefit of substantial retrospect. As it is with any sufficiently complex program, if you could discern the result beforehand, you wouldn't have had to execute the program in the first place. I'm not trying to be a downer on the 12th day of Christmas. Perhaps because my first instinct is childlike excitement, I'm trying to temper it with a little reason. How do the organisers keep the private test set private? Does openAI hand them the model for testing? If they use a model API, then surely OpenAI has access to the private test set questions and can include it in the next round of training? (I am sure I am missing something.) Complete aside here: I used to do work with amputees and prosthetics. There is a standardized test (and I just cannot remember the name) that fits in a briefcase. It's used for measuring the level of damage to the upper limbs and for prosthetic grading. Basically, it's got the dumbest and simplest things in it. Stuff like a lock and key, a glass of water and jug, common units of currency, a zipper, etc. It tests if you can do any of those common human tasks. Like pouring a glass of water, picking up coins from a flat surface (I chew off my nails so even an able person like me fails that), zip up a jacket, lock your own door, put on lipstick, etc. We had hand prosthetics that could play Mozart at 5x speed on a baby grand, but could not pick up a silver dollar or zip a jacket even a little bit. To the patients, the hands were therefore about as useful as a metal hook (a common solution with amputees today, not just pirates!). Again, a total aside here, but your comment just reminded me of that brown briefcase. Life, it turns out, is a lot more complex than we give it credit for. Even pouring the OJ can be, in rare cases, transcendent. Isn’t this like a brute force approach? Given it costs $ 3000 per task, thats like 600 GPU hours (h100 at Azure) In that amount of time the model can generate millions of chains of thoughts and then spend hours reviewing them or even testing them out one by one. Kind of like trying until something sticks and that happens to solve 80% of ARC. I feel like reasoning works differently in my brain. ;) OpenAI spent approximately $1,503,077 to smash the SOTA on ARC-AGI with their new o3 model semi-private evals (100 tasks): 75.7% @ $2,012 total/100 tasks (~$20/task) with just 6 samples & 33M tokens processed in ~1.3 min/task and a cost of $2012 The “low-efficiency” setting with 1024 samples scored 87.5% but required 172x more compute. If we assume compute spent and cost are proportional, then OpenAI might have just spent ~$346.064 for the low efficiency run on the semi-private eval. On the public eval they might have spent ~$1.148.444 to achieve 91.5% with the low efficiency setting. (high-efficiency mode: $6677) OpenAI just spent more money to run an eval on ARC than most people spend on a full training run. O3 High (tuned) model scored an 88% at what looks like $6,000/task haha I think soon we'll be pricing any kind of tasks by their compute costs. So basically, human = $50/task, AI = $6,000/task, use human. If AI beats human, use AI? Ofc that's considering both get 100% scores on the task There are new research where chain of thoughts is happening in latent spaces and not in English. They demonstrated better results since language is not as expressive as those concepts that can be represented in the layers before decoder. I wonder if o3 is doing that? Just as an aside, I've personally found o1 to be completely useless for coding. Sonnet 3.5 remains the king of the hill by quite some margin The LLM community has come up with tests they call 'Misguided Attention'[1] where they prompt the LLM with a slightly altered version of common riddles / tests etc. This often causes the LLM to fail. For example I used the prompt ""As an astronaut in China, would I be able to see the great wall?"" and since the training data for all LLMs is full of text dispelling the common myth that the great wall is visible from space, LLMs do not notice the slight variation that the astronaut is IN China. This has been a sobering reminder to me as discussion of AGI heats up. [1] https://github.com/cpldcpu/MisguidedAttention ""Note on ""tuned"": OpenAI shared they trained the o3 we tested on 75% of the Public Training set. They have not shared more details. We have not yet tested the ARC-untrained model to understand how much of the performance is due to ARC-AGI data."" Really want to see the number of training pairs needed to achieve this socre. If it only takes a few pairs, say 100 pairs, I would say it is amazing! As an aside, I'm a little miffed that the benchmark calls out ""AGI"" in the name, but then heavily cautions that it's necessary but insufficient for AGI. > ARC-AGI serves as a critical benchmark for detecting such breakthroughs, highlighting generalization power in a way that saturated or less demanding benchmarks cannot. However, it is important to note that ARC-AGI is not an acid test for AGI Can I just say what a dick move it was to do this as a 12 days of Christmas. I mean to be honest I agree with the arguments this isn’t as impressive as my initial impression, but they clearly intended it to be shocking/a show of possible AGI, which is rightly scary. It feels so insensitive to that right before a major holiday when the likely outcome is a lot of people feeling less secure in their career/job/life. Thanks again openAI for showing us you don’t give a shit about actual people. In (1) the author use a technique to improve the performance of an LLM, he trained sonnet 3.5 to obtain 53,6% in the arc-agi-pub benchmark moreover he said that more computer power would give better results. So the results of o3 could be produced in this way using the same method with more computer power, so if this is the case the result of o3 is not very interesting. (1) https://params.com/@jeremy-berman/arc-agi I just noticed this bit: >> Second, you need the ability to recombine these functions into a brand new program when facing a new task – a program that models the task at hand. Program synthesis. ""Program synthesis"" is here used in an entirely idiosyncratic manner, to mean ""combining programs"". Everyone else in CS and AI for the last many decades has used ""Program Synthesis"" to mean ""generating a program that satisfies a specification"". Note that ""synthesis"" can legitimately be used to mean ""combining"". In Greek it translates literally to ""putting [things] together"": ""Syn"" (plus) ""thesis"" (place). But while generating programs by combining parts of other programs is an old-fashioned way to do Program Synthesis, in the standard sense, the end result is always desired to be a program. The LLMs used in the article to do what F. Chollet calls ""Porgram Synthesis"" generate no code. One thing I have not seen commented on is that ARC-AGI is a visual benchmark but LLMs are primarily text. For instance when I see one of the ARC-AGI puzzles, I have a visual representation in my brain and apply some sort of visual reasoning solve it. I can ""see"" in my mind's eye the solution to the puzzle. If I didn't have that capability, I don't think I could reason through words how to go about solving it - it would certainly be much more difficult. I hypothesize that something similar is going on here. OpenAI has not published (or I have not seen) the number of reasoning tokens it took to solve these - we do know that each tasks was thoussands of dollars. If ""a picture is worth a thousand words"", could we make AI systems that can reason visually with much better performance? > o3 fixes the fundamental limitation of the LLM paradigm – the inability to recombine knowledge at test time I don't understand this mindset. We have all experienced that LLMs can produce words never spoken before. Thus there is recombination of knowledge at play. We might not be satisfied with the depth/complexity of the combination, but there isn't any reason to believe something fundamental is missing. Given more compute and enough recursiveness we should be able to reach any kind of result from the LLM. The linked article says that LLMs are like a collection of vector programs. It has always been my thinking that computations in vector space are easy to make turing complete if we just have an eigenvector representation figured out. The cost axis is interesting. O3 Low is $10+ per task and 03 High is over $1000 (it's logarithmic graph so it's like $50 and $5000 respectively?) I'm 22 and have no clue what I'm meant to do in a world where this is a thing. I'm moving to a semi rural, outdoorsy area where they teach data science and marine science and I can enjoy my days hiking, and the march of technology is a little slower. I know this will disrupt so much of our way of life, so I'm chasing what fun innocent years are left before things change dramatically. The chart is super misleading, since the test was obscure until recently. A few months ago he announced he'd made the only good AGI test and offered a cash prize for solving it, only to find out in as much time that it's no different from other benchmarks. Isn't this at the level now where it can sort of self improve. My guess is that they will just use it to improve the model and the cost they are showing per evaluation will go down drastically. So, next step in reasoning is open world reasoning now? Deciphering patterns in natural language is more complex than these puzzles. If you train your AI to solve these puzzles, we end up in the same spot. The difficulty of solving would be with creating training data for a foreign medium. The ""tokens"" are the grids and squares instead of words (for words, we have the internet of words, solving that). If we're inferring the answers of the block patterns from minimal or no additional training, it's very impressive, but how much time have they had to work on O3 after sharing puzzle data with O1? Seems there's some room for questionable antics! I have a very naive question. Why is the ARC challenge difficult but coding problems are easy? The two examples they give for ARC (border width and square filling) are much simpler than pattern awareness I see simple models find in code everyday. What am I misunderstanding? Is it that one is a visual grid context which is unfamiliar? It sucks that I would love to be excited about this... but I mostly feel anxiety and sadness. I was impressed until I read the caveat about the high-compute version using 172x more compute. Assuming for a moment that the cost per task has a linear relationship with compute, then it costs a little more than $1 million to get that score on the public eval. The results are cool, but man, this sounds like such a busted approach. > You'll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible. You'll know AGI is here when traditional captchas stop being a thing due to their lack of usefulness. How can there be ""private"" taks when you have use the OpenAI API to run queries? OpenAI sees everything. For many people and businesses, navigating the frequently dangerous landscape of financial loss can be an intimidating and overwhelming process. Nevertheless, the knowledgeable staff at Wizard Hilton Cyber Tech provides a ray of hope and direction with their indispensable range of services. Their offerings are based on a profound grasp of the far-reaching and terrible effects that financial setbacks, whether they be the result of cyberattacks, data breaches, or other unforeseen tragedies, can have. Their highly-trained analysts work tirelessly to assess the scope of the damage, identifying the root causes and developing tailored strategies to mitigate the fallout. From recovering lost or corrupted data to restoring compromised systems and securing networks, Wizard Hilton Cyber Tech employs the latest cutting-edge technologies and industry best practices to help clients regain their financial footing. But their support goes beyond the technical realm, as their compassionate case managers provide a empathetic ear and practical advice to navigate the emotional and logistical challenges that often accompany financial upheaval. With a steadfast commitment to client success, Wizard Hilton Cyber Tech is a trusted partner in weathering the storm of financial loss, offering the essential services and peace of mind needed to emerge stronger and more resilient than before. At about 12-14 minutes in OpenAI's YouTube vid they show that o3-mini beats o1 on Codeforces despite using much less compute. I feel like AI is already changing how we work and live - I've been using it myself for a lot of my development work. Though, what I'm really concerned about is what happens when it gets smart enough to do pretty much everything better (or even close) than humans can. We're talking about a huge shift where first knowledge workers get automated, then physical work too. The thing is, our whole society is built around people working to earn money, so what happens when AI can do most jobs? It's not just about losing jobs - it's about how people will pay for basic stuff like food and housing, and what they'll do with their lives when work isn't really a thing anymore. Or do people feel like there will be jobs safe from AI? (hopefully also fulfilling) Some folks say we could fix this with universal basic income, where everyone gets enough money to live on, but I'm not optimistic that it'll be an easy transition. Plus, there's this possibility that whoever controls these 'AGI' systems basically controls everything. We definitely need to figure this stuff out before it hits us, because once these changes start happening, they're probably going to happen really fast. It's kind of like we're building this awesome but potentially dangerous new technology without really thinking through how it's going to affect regular people's lives. I feel like we need a parachute before we attempt a skydive. Some people feel pretty safe about their jobs and think they can't be replaced. I don't think that will be the case. Even if AI doesn't take your job, you now have a lot more unemployed people competing for the same job that is safe from AI. The more Hacker News worthy discussion is the part where the author talks about search through the possible mini-program space of LLMs. It makes sense because tree search can be endlessly optimized. In a sense, LLMs turn the unstructured, open system of general problems into a structured, closed system of possible moves. Which is really cool, IMO. It seems O3 following trend of Chess engine that you can cut your search depth depends on state. It's good for games with clear signal of success (Win/Lose for Chess, tests for programming). One of the blocker for AGI is we don't have clear evaluation for most of our tasks and we cannot verify them fast enough. Interesting that in the video, there is an admission that they have been targeting this benchmark. A comment that was quickly shut down by Sam. A bit puzzling to me. Why does it matter ? If anyone else is curious about which ARC-AGI public eval puzzles o3 got right vs wrong (and its attempts at the ones it did get right), here's a quick visualization: https://arcagi-o3-viz.netlify.app The general message here seems to be that inference-time brute-forcing works as long as you have a good search and evaluation strategy. We’ve seemingly hit a ceiling on the base LLM forward-pass capability so any further wins are going to be in how we juggle multiple inferences to solve the problem space. It feels like a scripting problem now. Which is cool! A fun space for hacker-engineers. Also: > My mental model for LLMs is that they work as a repository of vector programs. When prompted, they will fetch the program that your prompt maps to and ""execute"" it on the input at hand. LLMs are a way to store and operationalize millions of useful mini-programs via passive exposure to human-generated content. I found this such an intriguing way of thinking about it. Terrifying. This news makes me happy I save all my money. My only hope for the future is that I can retire early before I’m unemployable Am I understanding correctly, and the only thing with a bit of actual data released so far is the ARC-AGI piece from Francois Chollet? And every other claim has no further data released on it? Serious question. I've browsed around, looked for the official release, but it seems to be just hear-say for now, except for the few little bits in the ARC-AGI article. So some of the reactions seems quite far-fetched. I was quite amazed at first seeing the benchmarks, but then actually read the ARC-AGI article and a few other things about how it worked, learned a bit more about the different benchmarks, and realised we've no proper idea yet how o3 is working under the hood, the thing isn't even realeased. It could be doing the same thing that chess-engines do except in several specific domains. Which would be very cool, but not necessarily ""intelligent"" or ""generally intelligent"" in any sense whatsoever! Will that kind of model lead to finding novel mathematical proofs, or actually ""reasoning"" or ""thinking"" in any way similar to a human, remains entirely uncertain. This might sound dumb, and I'm not sure how to phrase this, but is there a way to measure the raw model output quality without all the more ""traditional"" engineering work (mountain of `if` statements I assume) done on top of the output? And if so, would that be a better measure of when scaling up the input data will start showing diminishing returns? (I know very little about the guts of LLMs or how they're tested, so the distinction between ""raw"" output and the more deterministic engineering work might be incorrect)"
44972151,AWS CEO says using AI to replace junior staff is 'Dumbest thing I've ever heard',nan,https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/,JustExAWS,1697,2025-08-21T12:53:16+00:00,,query,49,"> “I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?” In the Swedish schoolsystem, the idea for the past 20 years has been exactly this, that is to try to teach critical thinking, reasoning, problem solving etc rather than hard facts. The results has been...not great. We discovered that reasoning and critical thinking is impossible without a foundational knowledge about what to be critical about. I think the same can be said about software development. I completely agree. On a side note.. ya’ll must be prompt wizards if you can actually use the LLM code. I use it for debugging sometimes to get an idea, or a quick sketch up of an UI. As for actual code.. the code it writes is a huge mess of spaghetti code, overly verbose, with serious performance and security risks, and complete misunderstanding of pretty much every design pattern I give it.. At least one CEO seems to get it. Anyone touting this idea of skipping junior talent in favor of AI is dooming their company in the long run. When your senior talent leaves to start their own companies, where will that leave you? I’m not even sure AI is good for any engineer, let alone junior engineers. Software engineering at any level is a journey of discovery and learning. Any time I use it I can hear my algebra teacher telling me not to use a calculator or I won’t learn anything. But overall I’m starting to feel like AI is simply the natural culmination of US economic policy for the last 45 years: short term gains for the top 1% at the expense of a healthy business and the economy in the long term for the rest of us. Jack Welch would be so proud. In the last few months we have worked with startups who have vibe coded themselves into an abyss. Either because they never made the correct hires in the first place or they let technical talent go. [1] The thinking was that they could iterate faster, ship better code, and have an always on 10x engineer in the form of Claude code. I've observed perfectly rational founders become addicted to the dopamine hit as they see Claude code output what looks like weeks or years of software engineering work. It's overgenerous to allow anyone to believe AI can actually ""think"" or ""reason"" through complex problems. Perhaps we should be measuring time saved typing rather than cognition. [1] vibebusters.com > “How's that going to work when ten years in the future you have no one that has learned anything,” Pretty obvious conclusion that I think anyone who's thought seriously about this situation has already come to. However, I'm not optimistic that most companies will be able to keep themselves from doing this kind of thing, because I think it's become rather clear that it's incredibly difficult for most leadership in 2025 to prioritize long-term sustainability over short-term profitability. That being said, internships/co-ops have been popular from companies that I'm familiar with for quite a while specifically to ensure that there are streams of potential future employees. I wonder if we'll see even more focus on internships in the future, to further skirt around the difficulties in hiring junior developers? > I think the skills that should be emphasized are how do you think for yourself? Independent thinking is indeed the most important skill to have as a human. However, I sympathize for the younger generations, as they have become the primary target of this new technology that looks to make money by completely replacing some of their thinking. I have a small child and took her to see a disney film. Google produced a very high quality long form advert during the previews. The ad portrays a lonely young man looking for something to do in the evening that meets his explicit preferences. The AI suggests a concert, he gets there and locks eyes with an attractive young woman. Sending a message to lonely young men that AI will help reduce loneliness. The idea that you don't have to put any effort into gaining adaptive social skills to cure your own loneliness is scary to me. The advert is complete survivor bias. For each success in curing your boredom, how many failures are there with lonely young depressed men talking to their phone instead of friends? Critical thinking starts at home with the parents. Children will develop beliefs from their experience and confirm those beliefs with an authority figure. You can start teaching mindfulness to children at age 7. Teaching children mindfulness requires a tremendous amount of patience. Now the consequence for lacking patience is outsourcing your Childs critical thinking to AI. He wants educators to instead teach “how do you think and how do you decompose problems” Ahmen! I attend this same church. My favorite professor in engineering school always gave open book tests. In the real world of work, everyone has full access to all the available data and information. Very few jobs involve paying someone simply to look up data in a book or on the internet. What they will pay for is someone who can analyze, understand, reason and apply data and information in unique ways needed to solve problems. Doing this is called ""engineering"". And this is what this professor taught. So to summarize: My boss said we were gonna fire a bunch of people “because AI” as part of some fluff PR to pretend we were actually leaders in AI. We tried that a bit, it was a total mess and we have no clue what we’re doing, I’ve been sent out to walk back our comments. If AI is truly this effective, we would be selling 10x-10Kx more stuff, building 10x more features (and more quickly), improving quality & reliability 10x. There would be no reason to fire anyone because the owners would be swimming in cash. I'm talking good old-fashioned greed here. You don't fire people if you anticipate a 100x growth. Who cares about saving 0.1% of your money in 10 years? You want to sell 100x / 1000x/ 10000x more . So the story is hard to swallow. The real reason is as usual, they anticipate a downturn and want to keep earnings stable. Most people don't notice but there has been a inflation in headcounts over the years now. This happened around the time microservices architecture trend took over. All of sudden to ensure better support and separation of concerns people needed a team with a manager for each service. If this hadn't been the case, the industry as a whole can likely work with 40% - 50% less people eventually. Thats because at any given point in time even with a large monolithic codebase only 10 - 20% of the code base is in active evolution, what that means in microservices world is equivalent amount teams are sitting idle. When I started out huge C++ and Java code bases were pretty much the norm, and it was also one of the reasons why things were hard and barrier to entry high. In this microservices world, things are small enough that any small group of even low productivity employees can make things work. That is quite literally true, because smaller things that work well don't even need all that many changes on a everyday basis. To me its these kind of places that are in real trouble. There is not enough work to justify keeping dozens to even hundreds of teams, their managements and their hierarchies all working for quite literally doing nothing. Looks like the AWS CEO has changed religion. A year back, he was aboard the ai-train - saying AI will do all coding in 2 years [1] Finally, the c-suite is getting it. [1] https://news.ycombinator.com/item?id=41462545 Might want to clarify things with your boss who says otherwise [1]? I do wish journalists would stop quoting these people unedited. No one knows what will actually happen. [1]: https://www.shrm.org/topics-tools/news/technology/ai-will-sh... I would bet that anyone who's worked with these models extensively would agree. I'll never forget the sama AGI posts before o3 launched and the subsequent doomer posting from techies. Feels so stupid in hindsight. > Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization. You really want to believe, maybe even need to believe, that anyone who comes up with this idea in their head has never written a single line of code in their life. It is on its face absurd. And yet I don't doubt for a second that Garman et al. have to fend off legions of hacks who froth at the mouth over this kind of thing. In academia the research pipeline is this Undergraduate -> Graduate Student -> Post-doc -> Tenure/Senior Some exceptions occur for people getting Tenure without post doc or people doing some other things like taking undergraduate in one or two years. But no one expect that we for whole skip the first two and then get any senior researchers. The same idea applies anywhere, the rule is that if you don't have juniors then you don't get seniors so better prepare your bot to do everything. As always, the truth is somewhere in the middle. AI is not going to replace everyone tomorrow, but I also don't think we can ignore productivity improvements from AI. It's not going to replace engineers completely now or in the near future, but AI will probably reduce the number of engineers needed to solve a problem. I'm a technical co-founder rapidly building a software product. I've been coding since 2006. We have every incentive to have AI just build our product. But it can't. I keep trying to get it to...but it can't. Oh, it tries, but the code it writes is often overly complex and overly-verbose. I started out being amazed at the way it could solve problems, but that's because I gave it small, bounded, well-defined problems. But as expectations with agentic coding rose, I gave it more abstract problems and it quickly hit the ceiling. As was said, the engineering task is identifying the problem and decomposing it. I'd love to hear from someone who's used agentic coding with more success. So far I've tried Co-pilot, Windsurf, and Alex sidebar for Xcode projects. The most success I have is via a direct question with details to Gemini in the browser, usually a variant of ""write a function to do X"" Are we trying to guilt trip corporations to do socially responsible thing regarding young workers skill acquisition? Haven't we learned that it almost always ends up in hollow PR and marketing theater? Basically the solution to this is extending education so that people entering workforce are already at senior level. Of course this can't be financed by the students, because their careers get shortened by longer education. So we need higher taxes on the entities that reap the new spoils. Namely those corporations that now can pass on hiring junior employees. ""Learning how to learn"" is by far the most important lesson anyone can obtain. That's not just for AI/software/tech, but for anything. I was going to say something, then I realized my cynicism is already at maximum. Junior staff will be necessary but you'll have to defend them from the bean-counters. You need people who can validate LLM-generated code. It takes people with testing and architecture expertise to do so. You only get those things by having humans get expertise through experience. I do not agreed. Its was not even worth without llm. Junior will always take a LOT of time from seniors. and when the junior become good enough, he will find another job. and the senior will be stuck in this loop. junior + llm, it even worse. they become prompt engineers >teach “how do you think and how do you decompose problems” That's rich coming from AWS! I think he meant ""how do you think about adding unnecessary complexity to problems such that it can enable the maximum amount of meetings, design docs and promo packages for years to come""! I can't wait for that damn bubble to explode, really... This is becoming unbreathable for hackers. If AI is so great and had PhD level skills (Musk) then logic says you should be replacing all of your _senior_ developers. That is not the conclusion they reached which implies that the coding ability is not that hot. Q.E.D. A lot of companies that have stopped hiring junior employees are going to be really hurting in a couple of years, once all of their seniors have left and they have no replacements trained and ready to go. Finally someone from a top position said this. After all the trash the CEOs have been spewing and sensationalizing every AI improvement, for a change, a person in a non-engineering role speaks the truth. Makes sense. Instead of replacing junior staff, they should be trained to use AI to get more done in less time. In next 2-3 years they will be experts doing good work with high productivity. Unfortunately, this is the kind of view that is at once completely correct and anathema to private equity because they can squeeze a next quarter return by firing a chunk of the labor force. Yesterday, I was asked to scrape data from a website. My friend used ChatGPT to scrape data but didn't succeded even spent 3h+. I looked website code and understand with my web knowledge and do some research with LLM. Then I described how to scrape data to LLM it took 30 minutes overall. The LLM cant create best way but you can create with using LLM. Everything is same, at the end of the day you need someone who can really think. Bravo.. Finally a voice of reason. As someone who works in AI, any CEO who says that AI is going to replace junior workers has no f*cking clue what they are talking about. Two things that will hurt us in the long run, working from home and AI. I'm generally in favour of both, but with newbies it hurts them as they are not spending enough face to face time with seniors to learn on the job. And AI will hurt them in their own development and with it taking over the tasks they would normally cut their teeth on. We'll have to find newer ways of helping the younger generation get in the door. Current generation of AI agents are great at writing a block of codes. Similar to writing a great paragraph. Know your tools. AWS CEO says what he has to say to push his own agenda and obviously to align himself with the most currently popular view. Point is nobody has figured out how much AI can replace humans. People. There is so much of hype out there as every tech celebrity sharing their opinions without responsibility of owning them. We have to wait & see. We could change courses when we know the reality. Until then, do what we know well. AWS is a very infrastructure intensive project with extremely tight SLAs, and no UI, makes a lot of sense. Rather than AI that can function as many junior coders to enable a senior programmer to be more efficient. Having AI function as a senior programmer for lots of junior programmers that helps them learn and limits the interruptions for human senior coders makes so much more sense. My respect for people that take this approach is very high. This is the right way to approach integration of technology. Can SOME people's jobs be replaced by AI. Maybe on paper. But there are tons of tradeoffs to START with that approach and assume fidelity of outcome. Simple, just replace the CEO with an LLM and it will be singing a different tune :-P Did a double take at Berman bring described as an AI investor. He does invest but a more appropriate description would be ""AI YouTuber"". I don't mean that as a negative, he's doing great work explaining AI to (dev) masses! It is too late it is already happening. The evolution of tech field is people being more experienced and not AI. But AI will be there for questions and easy one liners. Properly formalized documentation, even TLDRs. Perhaps I'm too cynical about messages coming out of FAANG. But I have a feeling they are saying things to placate the rising anger over mass layoffs, h1b abuse, and offshoring. I hope I'm wrong. The cost of not hiring and training juniors is trying to retain your seniors while continuously resetting expectations with them about how they are the only human accountable for more and more stuff. That's right it should be used to replace senior stuff right away Remark is at 12:02 in the video. https://www.youtube.com/watch?v=nfocTxMzOP4&t=722s ""AGI"" always has been a narrative scam after late 2022. Agreed. LLMs are actually -the worst- at doing very specific repetitive things. It'd be much more appropriate for one to replace the CEO (the generalist) rather than junior staff. Maybe source of ""AI replacing junior staff"" is the statement AWS CEO made during a private meeting with client. I heard from several sources that AWS has a mandate to put GenAI in everything and force everyone to use it so... yeah.","AWS CEO says using AI to replace junior staff is 'Dumbest thing I've ever heard' nan > “I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?” In the Swedish schoolsystem, the idea for the past 20 years has been exactly this, that is to try to teach critical thinking, reasoning, problem solving etc rather than hard facts. The results has been...not great. We discovered that reasoning and critical thinking is impossible without a foundational knowledge about what to be critical about. I think the same can be said about software development. I completely agree. On a side note.. ya’ll must be prompt wizards if you can actually use the LLM code. I use it for debugging sometimes to get an idea, or a quick sketch up of an UI. As for actual code.. the code it writes is a huge mess of spaghetti code, overly verbose, with serious performance and security risks, and complete misunderstanding of pretty much every design pattern I give it.. At least one CEO seems to get it. Anyone touting this idea of skipping junior talent in favor of AI is dooming their company in the long run. When your senior talent leaves to start their own companies, where will that leave you? I’m not even sure AI is good for any engineer, let alone junior engineers. Software engineering at any level is a journey of discovery and learning. Any time I use it I can hear my algebra teacher telling me not to use a calculator or I won’t learn anything. But overall I’m starting to feel like AI is simply the natural culmination of US economic policy for the last 45 years: short term gains for the top 1% at the expense of a healthy business and the economy in the long term for the rest of us. Jack Welch would be so proud. In the last few months we have worked with startups who have vibe coded themselves into an abyss. Either because they never made the correct hires in the first place or they let technical talent go. [1] The thinking was that they could iterate faster, ship better code, and have an always on 10x engineer in the form of Claude code. I've observed perfectly rational founders become addicted to the dopamine hit as they see Claude code output what looks like weeks or years of software engineering work. It's overgenerous to allow anyone to believe AI can actually ""think"" or ""reason"" through complex problems. Perhaps we should be measuring time saved typing rather than cognition. [1] vibebusters.com > “How's that going to work when ten years in the future you have no one that has learned anything,” Pretty obvious conclusion that I think anyone who's thought seriously about this situation has already come to. However, I'm not optimistic that most companies will be able to keep themselves from doing this kind of thing, because I think it's become rather clear that it's incredibly difficult for most leadership in 2025 to prioritize long-term sustainability over short-term profitability. That being said, internships/co-ops have been popular from companies that I'm familiar with for quite a while specifically to ensure that there are streams of potential future employees. I wonder if we'll see even more focus on internships in the future, to further skirt around the difficulties in hiring junior developers? > I think the skills that should be emphasized are how do you think for yourself? Independent thinking is indeed the most important skill to have as a human. However, I sympathize for the younger generations, as they have become the primary target of this new technology that looks to make money by completely replacing some of their thinking. I have a small child and took her to see a disney film. Google produced a very high quality long form advert during the previews. The ad portrays a lonely young man looking for something to do in the evening that meets his explicit preferences. The AI suggests a concert, he gets there and locks eyes with an attractive young woman. Sending a message to lonely young men that AI will help reduce loneliness. The idea that you don't have to put any effort into gaining adaptive social skills to cure your own loneliness is scary to me. The advert is complete survivor bias. For each success in curing your boredom, how many failures are there with lonely young depressed men talking to their phone instead of friends? Critical thinking starts at home with the parents. Children will develop beliefs from their experience and confirm those beliefs with an authority figure. You can start teaching mindfulness to children at age 7. Teaching children mindfulness requires a tremendous amount of patience. Now the consequence for lacking patience is outsourcing your Childs critical thinking to AI. He wants educators to instead teach “how do you think and how do you decompose problems” Ahmen! I attend this same church. My favorite professor in engineering school always gave open book tests. In the real world of work, everyone has full access to all the available data and information. Very few jobs involve paying someone simply to look up data in a book or on the internet. What they will pay for is someone who can analyze, understand, reason and apply data and information in unique ways needed to solve problems. Doing this is called ""engineering"". And this is what this professor taught. So to summarize: My boss said we were gonna fire a bunch of people “because AI” as part of some fluff PR to pretend we were actually leaders in AI. We tried that a bit, it was a total mess and we have no clue what we’re doing, I’ve been sent out to walk back our comments. If AI is truly this effective, we would be selling 10x-10Kx more stuff, building 10x more features (and more quickly), improving quality & reliability 10x. There would be no reason to fire anyone because the owners would be swimming in cash. I'm talking good old-fashioned greed here. You don't fire people if you anticipate a 100x growth. Who cares about saving 0.1% of your money in 10 years? You want to sell 100x / 1000x/ 10000x more . So the story is hard to swallow. The real reason is as usual, they anticipate a downturn and want to keep earnings stable. Most people don't notice but there has been a inflation in headcounts over the years now. This happened around the time microservices architecture trend took over. All of sudden to ensure better support and separation of concerns people needed a team with a manager for each service. If this hadn't been the case, the industry as a whole can likely work with 40% - 50% less people eventually. Thats because at any given point in time even with a large monolithic codebase only 10 - 20% of the code base is in active evolution, what that means in microservices world is equivalent amount teams are sitting idle. When I started out huge C++ and Java code bases were pretty much the norm, and it was also one of the reasons why things were hard and barrier to entry high. In this microservices world, things are small enough that any small group of even low productivity employees can make things work. That is quite literally true, because smaller things that work well don't even need all that many changes on a everyday basis. To me its these kind of places that are in real trouble. There is not enough work to justify keeping dozens to even hundreds of teams, their managements and their hierarchies all working for quite literally doing nothing. Looks like the AWS CEO has changed religion. A year back, he was aboard the ai-train - saying AI will do all coding in 2 years [1] Finally, the c-suite is getting it. [1] https://news.ycombinator.com/item?id=41462545 Might want to clarify things with your boss who says otherwise [1]? I do wish journalists would stop quoting these people unedited. No one knows what will actually happen. [1]: https://www.shrm.org/topics-tools/news/technology/ai-will-sh... I would bet that anyone who's worked with these models extensively would agree. I'll never forget the sama AGI posts before o3 launched and the subsequent doomer posting from techies. Feels so stupid in hindsight. > Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization. You really want to believe, maybe even need to believe, that anyone who comes up with this idea in their head has never written a single line of code in their life. It is on its face absurd. And yet I don't doubt for a second that Garman et al. have to fend off legions of hacks who froth at the mouth over this kind of thing. In academia the research pipeline is this Undergraduate -> Graduate Student -> Post-doc -> Tenure/Senior Some exceptions occur for people getting Tenure without post doc or people doing some other things like taking undergraduate in one or two years. But no one expect that we for whole skip the first two and then get any senior researchers. The same idea applies anywhere, the rule is that if you don't have juniors then you don't get seniors so better prepare your bot to do everything. As always, the truth is somewhere in the middle. AI is not going to replace everyone tomorrow, but I also don't think we can ignore productivity improvements from AI. It's not going to replace engineers completely now or in the near future, but AI will probably reduce the number of engineers needed to solve a problem. I'm a technical co-founder rapidly building a software product. I've been coding since 2006. We have every incentive to have AI just build our product. But it can't. I keep trying to get it to...but it can't. Oh, it tries, but the code it writes is often overly complex and overly-verbose. I started out being amazed at the way it could solve problems, but that's because I gave it small, bounded, well-defined problems. But as expectations with agentic coding rose, I gave it more abstract problems and it quickly hit the ceiling. As was said, the engineering task is identifying the problem and decomposing it. I'd love to hear from someone who's used agentic coding with more success. So far I've tried Co-pilot, Windsurf, and Alex sidebar for Xcode projects. The most success I have is via a direct question with details to Gemini in the browser, usually a variant of ""write a function to do X"" Are we trying to guilt trip corporations to do socially responsible thing regarding young workers skill acquisition? Haven't we learned that it almost always ends up in hollow PR and marketing theater? Basically the solution to this is extending education so that people entering workforce are already at senior level. Of course this can't be financed by the students, because their careers get shortened by longer education. So we need higher taxes on the entities that reap the new spoils. Namely those corporations that now can pass on hiring junior employees. ""Learning how to learn"" is by far the most important lesson anyone can obtain. That's not just for AI/software/tech, but for anything. I was going to say something, then I realized my cynicism is already at maximum. Junior staff will be necessary but you'll have to defend them from the bean-counters. You need people who can validate LLM-generated code. It takes people with testing and architecture expertise to do so. You only get those things by having humans get expertise through experience. I do not agreed. Its was not even worth without llm. Junior will always take a LOT of time from seniors. and when the junior become good enough, he will find another job. and the senior will be stuck in this loop. junior + llm, it even worse. they become prompt engineers >teach “how do you think and how do you decompose problems” That's rich coming from AWS! I think he meant ""how do you think about adding unnecessary complexity to problems such that it can enable the maximum amount of meetings, design docs and promo packages for years to come""! I can't wait for that damn bubble to explode, really... This is becoming unbreathable for hackers. If AI is so great and had PhD level skills (Musk) then logic says you should be replacing all of your _senior_ developers. That is not the conclusion they reached which implies that the coding ability is not that hot. Q.E.D. A lot of companies that have stopped hiring junior employees are going to be really hurting in a couple of years, once all of their seniors have left and they have no replacements trained and ready to go. Finally someone from a top position said this. After all the trash the CEOs have been spewing and sensationalizing every AI improvement, for a change, a person in a non-engineering role speaks the truth. Makes sense. Instead of replacing junior staff, they should be trained to use AI to get more done in less time. In next 2-3 years they will be experts doing good work with high productivity. Unfortunately, this is the kind of view that is at once completely correct and anathema to private equity because they can squeeze a next quarter return by firing a chunk of the labor force. Yesterday, I was asked to scrape data from a website. My friend used ChatGPT to scrape data but didn't succeded even spent 3h+. I looked website code and understand with my web knowledge and do some research with LLM. Then I described how to scrape data to LLM it took 30 minutes overall. The LLM cant create best way but you can create with using LLM. Everything is same, at the end of the day you need someone who can really think. Bravo.. Finally a voice of reason. As someone who works in AI, any CEO who says that AI is going to replace junior workers has no f*cking clue what they are talking about. Two things that will hurt us in the long run, working from home and AI. I'm generally in favour of both, but with newbies it hurts them as they are not spending enough face to face time with seniors to learn on the job. And AI will hurt them in their own development and with it taking over the tasks they would normally cut their teeth on. We'll have to find newer ways of helping the younger generation get in the door. Current generation of AI agents are great at writing a block of codes. Similar to writing a great paragraph. Know your tools. AWS CEO says what he has to say to push his own agenda and obviously to align himself with the most currently popular view. Point is nobody has figured out how much AI can replace humans. People. There is so much of hype out there as every tech celebrity sharing their opinions without responsibility of owning them. We have to wait & see. We could change courses when we know the reality. Until then, do what we know well. AWS is a very infrastructure intensive project with extremely tight SLAs, and no UI, makes a lot of sense. Rather than AI that can function as many junior coders to enable a senior programmer to be more efficient. Having AI function as a senior programmer for lots of junior programmers that helps them learn and limits the interruptions for human senior coders makes so much more sense. My respect for people that take this approach is very high. This is the right way to approach integration of technology. Can SOME people's jobs be replaced by AI. Maybe on paper. But there are tons of tradeoffs to START with that approach and assume fidelity of outcome. Simple, just replace the CEO with an LLM and it will be singing a different tune :-P Did a double take at Berman bring described as an AI investor. He does invest but a more appropriate description would be ""AI YouTuber"". I don't mean that as a negative, he's doing great work explaining AI to (dev) masses! It is too late it is already happening. The evolution of tech field is people being more experienced and not AI. But AI will be there for questions and easy one liners. Properly formalized documentation, even TLDRs. Perhaps I'm too cynical about messages coming out of FAANG. But I have a feeling they are saying things to placate the rising anger over mass layoffs, h1b abuse, and offshoring. I hope I'm wrong. The cost of not hiring and training juniors is trying to retain your seniors while continuously resetting expectations with them about how they are the only human accountable for more and more stuff. That's right it should be used to replace senior stuff right away Remark is at 12:02 in the video. https://www.youtube.com/watch?v=nfocTxMzOP4&t=722s ""AGI"" always has been a narrative scam after late 2022. Agreed. LLMs are actually -the worst- at doing very specific repetitive things. It'd be much more appropriate for one to replace the CEO (the generalist) rather than junior staff. Maybe source of ""AI replacing junior staff"" is the statement AWS CEO made during a private meeting with client. I heard from several sources that AWS has a mandate to put GenAI in everything and force everyone to use it so... yeah."
