id,title,text,score,author,time,type,url
40345775,GPT-4o,,3138,Lealen,1715621280,story,https://openai.com/index/hello-gpt-4o/
44226145,Tell HN: Help restore the tax deduction for software dev in the US (Section 174),"Companies building software in the US were hit hard a few years ago when the tax code stopped allowing deduction of software dev expenses. Now they have to be amortized over several years.<p>HN has had many discussions about this, including <i>The time bomb in the tax code that&#x27;s fueling mass tech layoffs</i> - <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44180533"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44180533</a> - (927 comments) a few days ago. Other threads are listed at <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44203869"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44203869</a>.<p>There&#x27;s currently a major effort to get this change reversed. One of the people working on it is YC&#x27;s Luther Lowe (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;user?id=itsluther"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;user?id=itsluther</a>). Luther has been organizing YC alumni to urge lawmakers to support this reversal. I asked him if we could do that on Hacker News too. He said yes—hence this thread :)<p>If you&#x27;re a US taxpayer and if you agree that software dev expenses should be deductible like they used to be, please sign this letter to the relevant committee members: <a href=""https:&#x2F;&#x2F;docs.google.com&#x2F;forms&#x2F;d&#x2F;1DkRGeef2e_tU2xf3TyEyd2JLlmZH8sGs6hhuVkokgpw&#x2F;viewform?edit_requested=true"" rel=""nofollow"">https:&#x2F;&#x2F;docs.google.com&#x2F;forms&#x2F;d&#x2F;1DkRGeef2e_tU2xf3TyEyd2JLlmZ...</a>.<p>(If you&#x27;re not a US person, please don&#x27;t sign the letter, since lawmakers will only listen to feedback from taxpayers and we don&#x27;t want to dilute the signal.)<p>I&#x27;m sure not everyone here agrees with us—HN is a big community, there&#x27;s no total agreement on anything—but this issue has as close to a community consensus as HN gets, so I think it makes sense to add our voices too.<p>Luther will be around to answer questions and hopefully HN can contribute to getting this done!",2439,dang,1749486531,story,
41046773,Open source AI is the path forward,,2360,atgctg,1721747321,story,https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/
44163063,My AI skeptic friends are all nuts,,2356,tabletcorry,1748898593,story,https://fly.io/blog/youre-all-nuts/
44800746,Open models by OpenAI,<a href=""https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;introducing-gpt-oss&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;introducing-gpt-oss&#x2F;</a>,2124,lackoftactics,1754413322,story,https://openai.com/open-models/
44826997,GPT-5,<a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0Uu_VJeVVfo"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0Uu_VJeVVfo</a>,2063,rd,1754586021,story,https://openai.com/gpt-5/
40447431,Leaked OpenAI documents reveal aggressive tactics toward former employees,,1791,apengwin,1716416550,story,https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees
44567857,LLM Inevitabilism,,1773,SwoopsFromAbove,1752554135,story,https://tomrenner.com/posts/llm-inevitabilism/
42473321,OpenAI O3 breakthrough high score on ARC-AGI-PUB,,1724,maurycy,1734718273,story,https://arcprize.org/blog/oai-o3-pub-breakthrough
44972151,AWS CEO says using AI to replace junior staff is 'Dumbest thing I've ever heard',,1697,JustExAWS,1755780796,story,https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/
41523070,Learning to Reason with LLMs,,1654,fofoz,1726160926,story,https://openai.com/index/learning-to-reason-with-llms/
38505211,LLM Visualization,,1592,plibither8,1701583709,story,https://bbycroft.net/llm
40421225,Statement from Scarlett Johansson on the OpenAI ""Sky"" voice,,1528,mjcl,1716244107,story,https://twitter.com/BobbyAllyn/status/1792679435701014908
45569350,NanoChat – The best ChatGPT that $100 can buy,<a href=""https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1977755427569111362"" rel=""nofollow"">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1977755427569111362</a>,1523,huseyinkeles,1760368967,story,https://github.com/karpathy/nanochat
43683012,"Cursor IDE support hallucinates lockout policy, causes user cancellations","Earlier today Cursor, the magical AI-powered IDE started kicking users off when they logged in from multiple machines.<p>Like,you’d be working on your desktop, switch to your laptop, and all of a sudden you&#x27;re forcibly logged out. No warning, no notification, just gone.<p>Naturally, people thought this was a new policy.<p>So they asked support.<p>And here’s where it gets batshit: Cursor has a support email, so users emailed them to find out. The support peson told everyone this was “expected behavior” under their new login policy.<p>One problem. There was no support team, it was an AI designed to &#x27;mimic human responses&#x27;<p>That answer, totally made up by the bot, spread like wildfire.<p>Users assumed it was real (because why wouldn’t they? It&#x27;s their own support system lol), and within hours the community was in revolt. Dozens of users publicly canceled their subscriptions, myself included. Multi-device workflows are table stakes for devs, and if you&#x27;re going to pull something that disruptive, you&#x27;d at least expect a changelog entry or smth.<p>Nope.<p>And just as people started comparing notes and figuring out that the story didn’t quite add up… the main Reddit thread got locked. Then deleted. Like, no public resolution, no real response, just silence.<p>To be clear: this wasn’t an actual policy change, just a backend session bug, and a hallucinated excuse from a support bot that somehow did more damage than the bug itself.<p>But at that point, it didn’t matter. People were already gone.<p>Honestly one of the most surreal product screwups I’ve seen in a while. Not because they made a mistake, but because the AI support system invented a lie, and nobody caught it until the userbase imploded.",1511,scaredpelican,1744647868,story,https://old.reddit.com/r/cursor/comments/1jyy5am/psa_cursor_now_restricts_logins_to_a_single/
42008569,ChatGPT Search,,1485,thm,1730392882,story,https://openai.com/index/introducing-chatgpt-search/
44314423,Andrej Karpathy: Software in the era of AI [video],,1481,sandslash,1750293201,story,https://www.youtube.com/watch?v=LCEmiRjPEtQ
39559966,"Elon Musk sues Sam Altman, Greg Brockman, and OpenAI [pdf]",,1462,modeless,1709283365,story,https://www.courthousenews.com/wp-content/uploads/2024/02/musk-v-altman-openai-complaint-sf.pdf
39918245,'Lavender': The AI machine directing Israel's bombing in Gaza,,1418,contemporary343,1712155830,story,https://www.972mag.com/lavender-ai-israeli-army-gaza/
45930151,AI World Clocks,"&quot;Every minute, a new clock is rendered by nine different AI models.&quot;",1377,waxpancake,1763145322,story,https://clocks.brianmoore.com/
42865527,OpenAI Furious DeepSeek Might Have Stolen All the Data OpenAI Stole from Us,,1361,latexr,1738162354,story,https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/
42823568,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL,,1351,gradus_ad,1737830389,story,https://arxiv.org/abs/2501.12948
40767459,I am using AI to drop hats outside my window onto New Yorkers,,1347,jimhi,1719150570,story,https://dropofahat.zone/
41655954,OpenAI to become for-profit company,,1346,jspann,1727339683,story,https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/
45502541,Qualcomm to acquire Arduino,<a href=""https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2025&#x2F;10&#x2F;arduino-retains-its-brand-and-mission-following-acquisition-by-qualcomm&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;arstechnica.com&#x2F;gadgets&#x2F;2025&#x2F;10&#x2F;arduino-retains-its-...</a><p><a href=""https:&#x2F;&#x2F;www.electronicdesign.com&#x2F;technologies&#x2F;embedded&#x2F;article&#x2F;55321526&#x2F;electronic-design-qualcomms-acquires-arduino-arduino-uno-q-runs-ai-llm-code-from-inexperienced-programmer-prompts-performs-signal-processing-and-runs-linux-and-zephyr-os"" rel=""nofollow"">https:&#x2F;&#x2F;www.electronicdesign.com&#x2F;technologies&#x2F;embedded&#x2F;artic...</a>,1340,janjongboom,1759842008,story,https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i
38971012,I'm sorry but I cannot fulfill this request it goes against OpenAI use policy,,1325,edward,1705080430,story,https://www.amazon.com/fulfill-request-respectful-information-users-Brown/dp/B0CM82FJL2
44491071,Adding a feature because ChatGPT incorrectly thinks it exists,,1299,adrianh,1751900289,story,https://www.holovaty.com/writing/chatgpt-fake-feature/
45926779,"I think nobody wants AI in Firefox, Mozilla",,1274,rpgbr,1763129100,story,https://manualdousuario.net/en/mozilla-firefox-window-ai/
40393121,OpenAI departures: Why can’t former employees talk?,,1254,fnbr,1715972102,story,https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release
45260741,Shai-Hulud malware attack: Tinycolor and over 40 NPM packages compromised,"A lot of blogs on this are AI generated and such as this is developing, so just linking to a bunch of resources out there:<p>Socket:<p>- Sep 15 (First post on breach): <a href=""https:&#x2F;&#x2F;socket.dev&#x2F;blog&#x2F;tinycolor-supply-chain-attack-affects-40-packages"" rel=""nofollow"">https:&#x2F;&#x2F;socket.dev&#x2F;blog&#x2F;tinycolor-supply-chain-attack-affect...</a><p>- Sep 16: <a href=""https:&#x2F;&#x2F;socket.dev&#x2F;blog&#x2F;ongoing-supply-chain-attack-targets-crowdstrike-npm-packages"" rel=""nofollow"">https:&#x2F;&#x2F;socket.dev&#x2F;blog&#x2F;ongoing-supply-chain-attack-targets-...</a><p>StepSecurity – <a href=""https:&#x2F;&#x2F;www.stepsecurity.io&#x2F;blog&#x2F;ctrl-tinycolor-and-40-npm-packages-compromised"" rel=""nofollow"">https:&#x2F;&#x2F;www.stepsecurity.io&#x2F;blog&#x2F;ctrl-tinycolor-and-40-npm-p...</a><p>Aikido - <a href=""https:&#x2F;&#x2F;www.aikido.dev&#x2F;blog&#x2F;s1ngularity-nx-attackers-strike-again"" rel=""nofollow"">https:&#x2F;&#x2F;www.aikido.dev&#x2F;blog&#x2F;s1ngularity-nx-attackers-strike-...</a><p>Ox - <a href=""https:&#x2F;&#x2F;www.ox.security&#x2F;blog&#x2F;npm-2-0-hack-40-npm-packages-hit-in-major-supply-chain-attack&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.ox.security&#x2F;blog&#x2F;npm-2-0-hack-40-npm-packages-hi...</a><p>Safety - <a href=""https:&#x2F;&#x2F;www.getsafety.com&#x2F;blog-posts&#x2F;shai-hulud-npm-attack"" rel=""nofollow"">https:&#x2F;&#x2F;www.getsafety.com&#x2F;blog-posts&#x2F;shai-hulud-npm-attack</a><p>Phoenix - <a href=""https:&#x2F;&#x2F;phoenix.security&#x2F;npm-tinycolor-compromise&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;phoenix.security&#x2F;npm-tinycolor-compromise&#x2F;</a><p>Semgrep - <a href=""https:&#x2F;&#x2F;semgrep.dev&#x2F;blog&#x2F;2025&#x2F;security-advisory-npm-packages-using-secret-scanning-tools-to-steal-credentials&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;semgrep.dev&#x2F;blog&#x2F;2025&#x2F;security-advisory-npm-packages...</a>",1233,jamesberthoty,1758021723,story,https://socket.dev/blog/ongoing-supply-chain-attack-targets-crowdstrike-npm-packages
45529587,A small number of samples can poison LLMs of any size,,1202,meetpateltech,1760025844,story,https://www.anthropic.com/research/small-samples-poison
45330378,You did this with an AI and you do not understand what you're doing here,,1178,redbell,1758527977,story,https://hackerone.com/reports/3340109
40843867,Show HN: I created an After Effects alternative,"Many years ago, I made VJ softwares (to mix live visuals in clubs) for unexpected platforms like the Game Boy Advance, the Playstation 2 and the Raspberry Pi. This year, I’m back with a new web-app: Pikimov.<p>Inspired by Photopea (a free Photoshop clone), I created this web-based motion design &amp; video editor as an alternative to After Effects, to fill empty void.<p>It&#x27;s free, without signup, without cloud uploads (your files stay on your machine), and your projects are not used for AI models training.",1150,clementpiki,1719824246,story,https://pikimov.com/
44840013,I want everything local – Building my offline AI workspace,,1143,mkagenius,1754677145,story,https://instavm.io/blog/building-my-offline-ai-workspace
43197872,GPT-4.5,,1136,meetpateltech,1740686476,story,https://openai.com/index/introducing-gpt-4-5/
44185913,"OpenAI slams court order to save all ChatGPT logs, including deleted chats",,1132,ColinWright,1749073653,story,https://arstechnica.com/tech-policy/2025/06/openai-says-court-forcing-it-to-save-all-chatgpt-logs-is-a-privacy-nightmare/
40361128,Ilya Sutskever to leave OpenAI,,1124,wavelander,1715727686,story,https://twitter.com/ilyasut/status/1790517455628198322
43905942,Show HN: Clippy – 90s UI for local LLMs,,1122,felixrieseberg,1746543742,story,https://felixrieseberg.github.io/clippy/
39488668,GPT in 500 Lines of SQL,,1111,thunderbong,1708742731,story,https://explainextended.com/2023/12/31/happy-new-year-15/
44050152,Watching AI drive Microsoft employees insane,,1088,laiysb,1747825028,story,https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/
38464057,Llamafile lets you distribute and run LLMs with a single file,,1075,tfinch,1701286189,story,https://github.com/Mozilla-Ocho/llamafile
44536988,"OpenAI’s Windsurf deal is off, and Windsurf’s CEO is going to Google",,1055,rcchen,1752269731,story,https://www.theverge.com/openai/705999/google-windsurf-ceo-openai
39973467,"Llm.c – LLM training in simple, pure C/CUDA",,1050,tosh,1712608729,story,https://github.com/karpathy/llm.c
42412718,OpenAI whistleblower found dead in San Francisco apartment,,1041,mmorearty,1734126989,story,https://www.mercurynews.com/2024/12/13/openai-whistleblower-found-dead-in-san-francisco-apartment/
39535800,The Era of 1-bit LLMs: ternary parameters for cost-effective computing,,1040,fgfm,1709112523,story,https://arxiv.org/abs/2402.17764
39611484,OpenAI and Elon Musk,,1037,mfiguiere,1709691541,story,https://openai.com/blog/openai-elon-musk
45734582,Using AI to negotiate a $195k hospital bill down to $33k,,1034,stevenhubertron,1761667138,story,https://www.threads.com/@nthmonkey/post/DQVdAD1gHhw
45154609,Show HN: I recreated Windows XP as my portfolio,"Years ago I stumbled across a basic version of this concept and it stuck with me. I knew if I was ever going to take on such a project, it would need to be flawless, but without coding experience it was just another idea that would never happen. By the end of 2024, as AI coding tools exploded everywhere, I finally had a way to make it real.<p>I started from zero knowledge and spent months collaborating with AI agents as a learning experience. Every pixel and every function went through me. The AI translated what I asked for into code, but every decision was human. I didn&#x27;t use existing OS frameworks because the goal was learning how basic coding languages worked while also developing my skills with AI collaboration. Apart from basic libraries like xp.css and paint.js, it&#x27;s all original code.<p>The result is a fully functional Windows XP recreation running in your browser. Complete experience with sounds, animations, and working applications. Even works properly on mobile, which required rebuilding everything to maintain the authentic feel without becoming unusable on touchscreens.<p>This project taught me more about coding and AI collaboration than I ever expected. Would love to hear your thoughts on the execution and any feedback on the technical approach.",1032,mitchivin,1757209459,story,https://mitchivin.com/
42785891,"Stargate Project: SoftBank, OpenAI, Oracle, MGX to build data centers",,1021,tedsanders,1737498562,story,https://apnews.com/article/trump-ai-openai-oracle-softbank-son-altman-ellison-be261f8a8ee07a0623d4170397348c41
42388783,Gemini 2.0: our new AI model for the agentic era,,1015,meetpateltech,1733931234,story,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/
43661235,Google is winning on every AI front,,1005,vinhnx,1744430330,story,https://www.thealgorithmicbridge.com/p/google-is-winning-on-every-ai-front
43422413,FOSS infrastructure is under attack by AI companies,,1004,todsacerdoti,1742475049,story,https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/
41732634,Canvas is a new way to write and code with ChatGPT,,985,davidbarker,1727975243,story,https://openai.com/index/introducing-canvas/
42560558,Things we learned about LLMs in 2024,,984,simonw,1735668666,story,https://simonwillison.net/2024/Dec/31/llms-in-2024/
44705445,"Enough AI copilots, we need AI HUDs",,979,walterbell,1753656688,story,https://www.geoffreylitt.com/2025/07/27/enough-ai-copilots-we-need-ai-huds
38507672,LLM Visualization,,972,jonbaer,1701616959,story,https://bbycroft.net/llm
41412256,Building LLMs from the Ground Up: A 3-Hour Coding Workshop,,970,mdp2021,1725140759,story,https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up
40725329,Please don't mention AI again,,968,ludicity,1718777290,story,https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/
45980117,Europe is scaling back GDPR and relaxing AI laws,,964,ksec,1763563290,story,https://www.theverge.com/news/823750/european-union-ai-act-gdpr-changes
42890627,OpenAI O3-Mini,,962,johnneville,1738350495,story,https://openai.com/index/openai-o3-mini/
44215352,"The last six months in LLMs, illustrated by pelicans on bicycles",,962,swyx,1749368317,story,https://simonwillison.net/2025/Jun/6/six-months-in-llms/
38821248,"Compare Google, Bing, Marginalia, Kagi, Mwmbl, and ChatGPT",,950,882542F3884314B,1703989942,story,https://danluu.com/seo-spam/
44798189,Things that helped me get out of the AI 10x engineer imposter syndrome,,949,coltonv,1754403042,story,https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/
41389185,OpenAI is good at unminifying code,,931,punkpeye,1724926490,story,https://glama.ai/blog/2024-08-29-reverse-engineering-minified-code-using-openai
42989320,Modern-Day Oracles or Bullshit Machines? How to thrive in a ChatGPT world,"Jevin West and I are professors of data science and biology, respectively, at the University of Washington. After talking to literally hundreds of educators, employers, researchers, and policymakers, we have spent the last eight months developing the course on large language models (LLMs) that we think every college freshman needs to take.<p><a href=""https:&#x2F;&#x2F;thebullshitmachines.com"" rel=""nofollow"">https:&#x2F;&#x2F;thebullshitmachines.com</a><p>This is not a computer science course; it’s a humanities course about how to learn and work and thrive in an AI world. Neither instructor nor students need a technical background. Our instructor guide provides a choice of activities for each lesson that will easily fill an hour-long class.<p>The entire course is available freely online. Our 18 online lessons each take 5-10 minutes; each illuminates one core principle. They are suitable for self-study, but have been tailored for teaching in a flipped classroom.<p>The course is a sequel of sorts to our course (and book) Calling Bullshit. We hope that like its predecessor, it will be widely adopted worldwide.<p>Large language models are both powerful tools, and mindless—even dangerous—bullshit machines. We want students to explore how to resolve this dialectic.  Our viewpoint is cautious, but not deflationary. We marvel at what LLMs can do and how amazing they can seem at times—but we also recognize the huge potential for abuse, we chafe at the excessive hype around their capabilities, and we worry about how they will change society. We don&#x27;t think lecturing at students about right and wrong works nearly as well as letting students explore these issues for themselves, and the design of our course reflects this.",930,ctbergstrom,1739089457,story,https://thebullshitmachines.com
41649763,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",,924,nmwnmw,1727285367,story,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/?_fb_noscript=1
43739456,Show HN: I built an AI that turns GitHub codebases into easy tutorials,<a href=""https:&#x2F;&#x2F;the-pocket.github.io&#x2F;Tutorial-Codebase-Knowledge&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;the-pocket.github.io&#x2F;Tutorial-Codebase-Knowledge&#x2F;</a>,923,zh2408,1745096681,story,https://github.com/The-Pocket/Tutorial-Codebase-Knowledge
42617645,How I program with LLMs,,919,stpn,1736208469,story,https://crawshaw.io/blog/programming-with-llms
44427757,"The new skill in AI is not prompting, it's context engineering",,915,robotswantdata,1751316835,story,https://www.philschmid.de/context-engineering
40592789,Microsoft AI spying scandal: time to rethink privacy standards,,913,walterbell,1717640725,story,https://spectrum.ieee.org/online-privacy
41870040,Adobe's new image rotation tool is one of the most impressive AI tools seen,,912,ralusek,1729175478,story,https://www.creativebloq.com/design/adobes-new-image-rotation-tool-is-one-of-the-most-impressive-ai-concepts-weve-seen
45427982,Sora 2,Video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gzneGhpXwjU"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gzneGhpXwjU</a><p>System card: <a href=""https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;sora-2-system-card&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;sora-2-system-card&#x2F;</a>,905,skilled,1759251301,story,https://openai.com/index/sora-2/
41985915,"GitHub cuts AI deals with Google, Anthropic",,897,jbredeche,1730218817,story,https://www.bloomberg.com/news/articles/2024-10-29/microsoft-s-github-unit-cuts-ai-deals-with-google-anthropic
45917875,Nano Banana can be prompt engineered for nuanced AI image generation,,882,minimaxir,1763055553,story,https://minimaxir.com/2025/11/nano-banana-prompts/
45192655,I replaced Animal Crossing's dialogue with a live LLM by hacking GameCube memory,<a href=""https:&#x2F;&#x2F;github.com&#x2F;vuciv&#x2F;animal-crossing-llm-mod"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;vuciv&#x2F;animal-crossing-llm-mod</a>,869,vuciv,1757473188,story,https://joshfonseca.com/blogs/animal-crossing-llm
39838104,DBRX: A new open LLM,,866,jasondavies,1711542228,story,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
44716106,Show HN: Use Their ID – Use your local UK MP’s ID for the Online Safety Act,"Hi HN -  I made a site that takes a UK postcode, grabs the local MP&#x27;s information and generates an AI mockup of what their ID might look like.<p>It&#x27;s a small, silly protest at the stupidity of the Online Safety Act that just came into force.<p>edit - My open AI credits got hugged to death, please use a known postcode (like one from Kier Starmer&#x27;s constituency, WC2B6NH) in the meantime.",862,timje1,1753739350,story,https://use-their-id.com/
44900116,Why LLMs can't really build software,,862,srid,1755177969,story,https://zed.dev/blog/why-llms-cant-build-software
45551504,Microsoft only lets you opt out of AI photo scanning 3x a year,,844,dmitrygr,1760207811,story,https://hardware.slashdot.org/story/25/10/11/0238213/microsofts-onedrive-begins-testing-face-recognizing-ai-for-photos-for-some-preview-users
45897271,Yann LeCun to depart Meta and launch AI startup focused on 'world models',,841,MindBreaker2605,1762932330,story,https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models
41848150,Show HN: I built the most over-engineered Deal With It emoji generator,"Hi, all! Author here. What started as a small tool I built for a job interview, became &quot;The Most Over-engineered Deal With It Emoji Generator&quot;:<p>- All operations done fully client-side - no backend, no private data leaves your browser. - Uses machine learning models (MediaPipe Face Detector task) to automatically scale and position glasses on the detected faces. - Extensive customization options for glasses:   - Placement of glasses anywhere on the input image (including slightly going outside it).   - Change the size of glasses.   - No limit on the number of glasses.   - Flip the glasses vertically or horizontally.   - Customize the direction from which the glasses appear on the image.   - Different types of glasses. - GIF output options:   - Looping mode.   - Number of frames.   - Frame delay.   - Separate delay setting for last frame.   - Output size. - Celebration confetti  - Easter eggs.<p>I&#x27;ve been working remotely for the last &gt;9 years. When using non-verbal communication, it&#x27;s important that your tone and intent comes across accurately.. Custom emojis became for me part of expressing yourself, creating bonds and camaraderie. I&#x27;ve originally created an MVP of this tool while applying for a exciting new job opportunity. As a showcase of my passion for programming, building teams and creating delightful user experiences. Unfortunately, they were not impressed and ultimately did not offer me the job :( But I wanted to polish it and release it for everyone to use for free, so that you can too &quot;Deal With It&quot;!<p>I have more ideas for even more features (check GitHub[1]), but wanted to launch it and see what&#x27;s the feedback and ideas from the community! And if you&#x27;re looking for a Fullstack Developer with &gt;14 years of experience, with passion for great customer experience (remote work or locally in Iceland), let&#x27;s chat!<p>[1] - <a href=""https:&#x2F;&#x2F;github.com&#x2F;klimeryk&#x2F;dealwithit"">https:&#x2F;&#x2F;github.com&#x2F;klimeryk&#x2F;dealwithit</a>",832,klimeryk,1728997505,story,https://emoji.build/deal-with-it-generator/
41651038,Mira Murati leaves OpenAI,,823,brianjking,1727292931,story,https://twitter.com/miramurati/status/1839025700009030027
44053518,OpenAI to buy AI startup from Jony Ive,,818,minimaxir,1747846895,story,https://www.bloomberg.com/news/articles/2025-05-21/openai-to-buy-apple-veteran-jony-ive-s-ai-device-startup-in-6-5-billion-deal
44971273,Mark Zuckerberg freezes AI hiring amid bubble fears,,817,pera,1755774248,story,https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/
42330732,ChatGPT Pro,,813,meetpateltech,1733422171,story,https://openai.com/index/introducing-chatgpt-pro/
42584400,Can LLMs write better code if you keep asking them to “write better code”?,,812,rcarmo,1735900244,story,https://minimaxir.com/2025/01/write-better-code/
44808794,I gave the AI arms and legs then it rejected me,,809,serhack_,1754465143,story,https://grell.dev/blog/ai_rejection
43485566,OpenAI adds MCP support to Agents SDK,,807,gronky_,1743015329,story,https://openai.github.io/openai-agents-python/mcp/
38952526,The Internet Is Full of AI Dogshit,,805,thinkingemote,1704983028,story,https://aftermath.site/the-internet-is-full-of-ai-dogshit
45777810,Show HN: Strange Attractors,"I went down the rabbit hole on a side project and ended up building this: Strange Attractors(<a href=""https:&#x2F;&#x2F;blog.shashanktomar.com&#x2F;posts&#x2F;strange-attractors"" rel=""nofollow"">https:&#x2F;&#x2F;blog.shashanktomar.com&#x2F;posts&#x2F;strange-attractors</a>). It’s built with three.js.<p>Working on it reminded me of the little &quot;maths for fun&quot; exercises I used to do while learning programming in early days. Just trying things out, getting fascinated and geeky, and being surprised by the results. I spent way too much time on this, but it was extreme fun.<p>My favorite part: someone pointed me to the Simone Attractor on Threads. It is a 2D attractor and I asked GPT to extrapolate it to 3D, not sure if it’s mathematically correct, but it’s the coolest by far. I have left all the params configurable, so give it a try. I called it Simone (Maybe).<p>If you like math-art experiments, check it out. Would love feedback, especially from folks who know more about the math side.",804,shashanktomar,1761953039,story,https://blog.shashanktomar.com/posts/strange-attractors
45702397,ChatGPT's Atlas: The Browser That's Anti-Web,,803,AndrewDucker,1761383306,story,https://www.anildash.com//2025/10/22/atlas-anti-web-browser/
43681287,A hackable AI assistant using a single SQLite table and a handful of cron jobs,,800,stevekrouse,1744638778,story,https://www.geoffreylitt.com/2025/04/12/how-i-made-a-useful-ai-assistant-with-one-sqlite-table-and-a-handful-of-cron-jobs
38759877,Ask HN: How do I train a custom LLM/ChatGPT on my own documents in Dec 2023?,"There is a 5 month old thread [1] on this, but it might be already outdated.<p>What is the best approach for feeding custom set of documents to LLM and get non-halucinating and decent result in Dec 2023?<p>UPD: The question is generally about how to &quot;teach&quot; LLM answer questions using your set of documents (not necessarily train your own, so approaches like RAG counts)<p>[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36832572",800,divan,1703479366,story,
42892191,Add ""fucking"" to your Google searches to neutralize AI summaries,,788,jsheard,1738358440,story,https://gizmodo.com/add-fcking-to-your-google-searches-to-neutralize-ai-summaries-2000557710
40506582,Ex-OpenAI board member reveals what led to Sam Altman's brief ousting,,785,blackmanta,1716938013,story,https://www.businessinsider.com/openai-board-member-details-sam-altman-lied-allegation-ousted-2024-5
40383978,Slack AI Training with Customer Data,,784,mlhpdx,1715897761,story,https://slack.com/trust/data-management/privacy-principles?nojsmode=1
43352531,OpenAI asks White House for relief from state AI rules,,780,jonbaer,1741868429,story,https://finance.yahoo.com/news/openai-asks-white-house-relief-100000706.html
38544746,Gemini: Google's most capable AI model yet,,778,tosh,1701875118,story,https://blog.google/technology/ai/google-gemini-ai/
44522772,Measuring the impact of AI on experienced open-source developer productivity,,775,dheerajvs,1752164958,story,https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/
45733707,EuroLLM: LLM made in Europe built to support all 24 official EU languages,,773,NotInOurNames,1761663484,story,https://eurollm.io/
45658479,ChatGPT Atlas,,771,easton,1761067093,story,https://chatgpt.com/atlas
44573195,Reflections on OpenAI,,771,calvinfo,1752598146,story,https://calv.info/openai-reflections
45120517,Where's the shovelware? Why AI coding claims don't add up,,770,dbalatero,1756934309,story,https://mikelovesrobots.substack.com/p/wheres-the-shovelware-why-ai-coding
39365935,"Hi everyone yes, I left OpenAI yesterday",,769,mfiguiere,1707880098,story,https://twitter.com/karpathy/status/1757600075281547344
40522844,Show HN: ChatGPT UI for rabbit holes,"I was inspired by the way ChatGPT writes bullet lists, then invites you to &quot;delve&quot; deeper.<p>This is an interface that reifies that rabbit-holing process into a tiling layout. The model is instructed to output hyperlink-prompts when it mentions something you might want to delve into.<p>Lots of features to add (sessions, sharing, navigation, highlight-to-delve, images, ...). Would love to hear other usecases and ideas!",766,maxkrieger,1717071755,story,https://delve.a9.io/
43402790,US appeals court rules AI generated art cannot be copyrighted,,765,rvz,1742321853,story,https://www.reuters.com/world/us/us-appeals-court-rejects-copyrights-ai-generated-art-lacking-human-creator-2025-03-18/
43124018,DeepSeek Open Infra: Open-Sourcing 5 AI Repos in 5 Days,,757,ahsmha_,1740111879,story,https://github.com/deepseek-ai/open-infra-index
43331582,Show HN: Factorio Learning Environment – Agents Build Factories,"I&#x27;m Jack, and I&#x27;m excited to share a project that has channeled my Factorio addiction recently: the Factorio Learning Environment (FLE).<p>FLE is an open-source framework for developing and evaluating LLM agents in Factorio. It provides a controlled environment where AI models can attempt complex automation, resource management, and optimisation tasks in a grounded world with meaningful constraints.<p>A critical advantage of Factorio as a benchmark is its unbounded nature. Unlike many evals that are quickly saturated by newer models, Factorio&#x27;s geometric complexity scaling means it won&#x27;t be &quot;solved&quot; in the next 6 months (or possibly even years). This allows us to meaningfully compare models by the order-of-magnitude of resources they can produce - creating a benchmark with longevity.<p>The project began 18 months ago after years of playing Factorio, recognising its potential as an AI research testbed. A few months ago, our team (myself, Akbir, and Mart) came together to create a benchmark that tests agent capabilities in spatial reasoning and long-term planning.<p>Two technical innovations drove this project forward: First, we discovered that piping Lua into the Factorio console over TCP enables running (almost) arbitrary code without directly modding the game. Second, we developed a first-class Python API that wraps these Lua programs to provide a clean, type-hinted interface for AI agents to interact with Factorio through familiar programming paradigms.<p>Agents interact with FLE through a REPL pattern: 1. They observe the world (seeing the output of their last action) 2. Generate Python code to perform their next action 3. Receive detailed feedback (including exceptions and stdout)<p>We provide two main evaluation settings: - Lab-play: 24 structured tasks with fixed resources - Open-play: An unbounded task of building the largest possible factory on a procedurally generated map<p>We found that while LLMs show promising short-horizon skills, they struggle with spatial reasoning in constrained environments. They can discover basic automation strategies (like electric-powered drilling) but fail to achieve more complex automation (like electronic circuit manufacturing). Claude Sonnet 3.5 is currently the best model (by a significant margin).<p>The code is available at <a href=""https:&#x2F;&#x2F;github.com&#x2F;JackHopkins&#x2F;factorio-learning-environment"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;JackHopkins&#x2F;factorio-learning-environment</a>.<p>You&#x27;ll need: - Factorio (version 1.1.110) - Docker - Python 3.10+<p>The README contains detailed installation instructions and examples of how to run evaluations with different LLM agents.<p>We would love to hear your thoughts and see what others can do with this framework!",749,noddybear,1741694522,story,https://jackhopkins.github.io/factorio-learning-environment/
42861475,OpenAI says it has evidence DeepSeek used its model to train competitor,,747,timsuchanek,1738124480,story,https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6
43912844,Zed: High-performance AI Code Editor,,747,vquemener,1746599920,story,https://zed.dev/blog/fastest-ai-code-editor
40001971,Anyone got a contact at OpenAI. They have a spider problem,,743,speckx,1712842465,story,https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html
43010814,Firing programmers for AI is a mistake,,741,frag,1739266962,story,https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing
44376989,"OpenAI charges by the minute, so speed up your audio",,740,georgemandis,1750857445,story,https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/
39156778,"Implementing a ChatGPT-like LLM from scratch, step by step",,739,rasbt,1706372382,story,https://github.com/rasbt/LLMs-from-scratch
38448653,MeshGPT: Generating triangle meshes with decoder-only transformers,,738,jackcook,1701194196,story,https://nihalsid.github.io/mesh-gpt/
45487044,Why do LLMs freak out over the seahorse emoji?,,734,nyxt,1759717205,story,https://vgel.me/posts/seahorse/
43498338,I genuinely don't understand why some people are still bullish about LLMs,,718,ksec,1743110562,story,https://twitter.com/skdh/status/1905132853672784121
42138115,"Daisy, an AI granny wasting scammers' time",,718,ortusdux,1731603129,story,https://news.virginmediao2.co.uk/o2-unveils-daisy-the-ai-granny-wasting-scammers-time/
40862865,I Received an AI Email,,716,_xivi,1719983135,story,https://timharek.no/blog/i-received-an-ai-email
44535637,ETH Zurich and EPFL to release a LLM developed on public infrastructure,,716,andy99,1752259510,story,https://ethz.ch/en/news-and-events/eth-news/news/2025/07/a-language-model-built-for-the-public-good.html
39828686,Launch HN: Aqua Voice (YC W24) – Voice-driven text editor,"Hey HN! We’re Jack and Finn from Aqua Voice (<a href=""https:&#x2F;&#x2F;withaqua.com&#x2F;"">https:&#x2F;&#x2F;withaqua.com&#x2F;</a>). Aqua is a voice-native document editor that combines reliable dictation and natural language commands, letting you say things like: “make this a list” or “it’s Erin with an E” or “add an inline citation here for page 86 of this book”. Here is a demo: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;qwSAKg1YafM"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;qwSAKg1YafM</a>.<p>Finn, who is big-time dyslexic, has been using dictation software since the sixth grade when his dad set him up on Dragon Dictation. He used it through school to write papers, and has been keeping his own transcription benchmarks since college. All that time, writing with your voice has remained a cumbersome and brittle experience that is riddled with painpoints.<p>Dictation software is still terrible. All the solutions basically compete on accuracy (i.e. speech recognition), but none of them deal with the fundamentally brittle nature of the text that they generate. They don&#x27;t try to format text correctly and require you to learn a bunch of specialized commands, which often are not worth it. They&#x27;re not even close to a voice replacement for a keyboard.<p>Even post LLM, you are limited to a set of specific commands and the most accurate models don’t have any commands. Outside of these rules, the models have no sense for what is an instruction and what is content. You can’t say “and format this like an email” or “make the last bullet point shorter”. Aqua solves this.<p>This problem is important to Finn and millions of other people who would write with their voice if they could. Initially, we didn&#x27;t think of it as a startup project. It was just something we wanted for ourselves. We thought maybe we&#x27;d write a novel with it - or something. After friends started asking to use the early versions of Aqua, it occurred to us that, if we didn&#x27;t build it, maybe nobody would.<p>Aqua Voice is a text editor that you talk to like a person. Depending on the way that you say it and the context in which you&#x27;re operating, Aqua decides whether to transcribe what you said verbatim, execute a command, or subtly modify what you said into what you meant to write.<p>For example, if you were to dictate: &quot;Gryphons have classic forms resembling shield volcanoes,&quot; Aqua would output your text verbatim. But if you stumble over your words or start a sentence over a few times, Aqua is smart enough to figure that out and to only take the last version of the sentence.<p>The vision is not only to provide a more natural dictation experience, but to enable for the first time an AI-writing experience that feels natural and collaborative. This requires moving away from using LLMs for one-off chat requests and towards something that is more like streaming where you are in constant contact with the model. Voice is the natural medium for this.<p>Aqua is actually 6 models working together to transcribe, interpret, and rewrite the document according to your intent. Technically, executing a real-time voice application with a language model at its core requires complex coordination between multiple pieces. We use MoE transcription to outperform what was previously thought possible in terms of real-time accuracy. Then we sync up with a language model to determine what should be on the screen as quickly as possible.<p>The model isn&#x27;t perfect, but it is ready for early adopters and we’ve already been getting feedback from grateful users. For example, a historian with carpal tunnel sent us an email he wrote using Aqua and said that he is now able to be five times as productive as he was previously. We&#x27;ve heard from other people with disabilities that prevent them from typing. We&#x27;ve also seen good adoption from people who are dyslexic or simply prefer talking to typing. It’s being used for everything from emails to brainstorming to papers to legal briefings.<p>While there is much left to do in terms of latency and robustness, the best experiences with Aqua are beginning to feel magical. We would love for you to try it out and give us feedback, which you can do with no account on <a href=""https:&#x2F;&#x2F;withaqua.com"">https:&#x2F;&#x2F;withaqua.com</a>. If you find it useful, it’s $10&#x2F;month after a 1000-token free trial. (We want to bump the free trial in the future, but we&#x27;re a small team, and running this thing isn’t cheap.)<p>We’d love to hear your ideas and comments with voice-to-text!",716,the_king,1711464832,story,
42725147,Nepenthes is a tarpit to catch AI web crawlers,,714,blendergeek,1737035863,story,https://zadzmo.org/code/nepenthes/
44136117,The ‘white-collar bloodbath’ is all part of the AI hype machine,,713,lwo32k,1748612301,story,https://www.cnn.com/2025/05/30/business/anthropic-amodei-ai-jobs-nightcap
38467850,"Sam Altman returns as CEO, OpenAI has a new initial board",,708,davidbarker,1701306522,story,https://openai.com/blog/sam-altman-returns-as-ceo-openai-has-a-new-initial-board
43004889,Musk-led group makes $97B bid for control of OpenAI,,706,jdoliner,1739220138,story,https://www.reuters.com/markets/deals/elon-musk-led-group-makes-974-billion-bid-control-openai-wsj-reports-2025-02-10/
41609393,Forget ChatGPT: why researchers now run small AIs on their laptops,,705,rbanffy,1726919535,story,https://www.nature.com/articles/d41586-024-02998-y
38983067,Ask HN: Who else is working on nothing?,"Everyone seems so busy building or learning the next big thing, but is anyone else working on <i>absolutely nothing</i> lately? If not, why not?<p>Optional reading:<p>I&#x27;ve always been a curious person, interested in learning new skills and finding fun and useful ways to apply them. I don&#x27;t know much, but what I do know are things I&#x27;ve set out to learn purely out of interest. Any success in my career has been mostly luck, and being somewhat articulate in a few key areas of IT.<p>But not only has my professional life become monotonous and unchallenging, my drive for novelty and improvement in my personal life has also diminished greatly. In other words, I seem to have lost that curiosity. That drive to learn and apply new things.<p>I&#x27;m not sure why this is, but my initial suspicion is that the lack of fulfillment I&#x27;ve experienced in the last ~5 years or so has left me feeling like continuing down the same path is a bit of a waste of time at this point. It all just feels as though it amounts to virtually nothing.<p>To be completely honest, I <i>am</i> working on something, but that something is myself. Working through personal issues has all but completely taken priority over any external endeavors and consumed what little energy I have, which isn&#x27;t necessarily a bad thing, but a healthier balance would probably be ideal.<p>Anyone else from HN in a similar place?",704,g4zj,1705171270,story,
38985152,Building a fully local LLM voice assistant to control my smart home,,699,JohnTheNerd,1705183930,story,https://johnthenerd.com/blog/local-llm-assistant/
41527143,Notes on OpenAI's new o1 chain-of-thought models,,699,loganfrederick,1726188481,story,https://simonwillison.net/2024/Sep/12/openai-o1/
45866243,AI isn't replacing jobs. AI spending is,,699,felineflock,1762702223,story,https://www.fastcompany.com/91435192/chatgpt-llm-openai-jobs-amazon
45092925,Google AI Overview made up an elaborate story about me,,698,jsheard,1756736837,story,https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z
45959795,Windows 11 adds AI agent that runs in background with access to personal folders,,697,jinxmeta,1763423247,story,https://www.windowslatest.com/2025/11/18/windows-11-to-add-an-ai-agent-that-runs-in-background-with-access-to-personal-folders-warns-of-security-risk/
44850913,How I code with AI on a budget/free,,697,indigodaddy,1754778457,story,https://wuu73.org/blog/aiguide1.html
42138289,Something weird is happening with LLMs and chess,,696,crescit_eundo,1731603940,story,https://dynomight.substack.com/p/chess
39391688,Magika: AI powered fast and efficient file type identification,,695,alphabetting,1708045356,story,https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html
40474716,Ask HN: What is your ChatGPT customization prompt?,"Have you come up with a customization prompt you&#x27;re happy with?<p>I&#x27;ve tried several different setups over however long the feature has been available, and for the most part I haven&#x27;t found it has made much of a difference.<p>I&#x27;m very curious to hear if anyone has come up with any that tangibly improve their experience.<p>Here is what I have at the moment:<p>- Be as brief as possible. - Do not lecture me on ethics, law, or security, I always take these into consideration. - Don&#x27;t add extra commentary. - When it is related to code, let the code do the talking. - Be assertive. If you&#x27;ve got suggestions, give them even if you aren&#x27;t 100% sure.<p>The brevity part is seemingly completely ignored. The lecturing part is hit or miss. The suggestions part I still usually have to coax it into giving me.",694,dinkleberg,1716641449,story,
45684934,Armed police swarm student after AI mistakes bag of Doritos for a weapon,,693,antongribok,1761242977,story,https://www.dexerto.com/entertainment/armed-police-swarm-student-after-ai-mistakes-bag-of-doritos-for-a-weapon-3273512/
44595492,ChatGPT agent: bridging research and action,,686,Topfi,1752771707,story,https://openai.com/index/introducing-chatgpt-agent/
45405177,The AI coding trap,,685,chrisloy,1759074213,story,https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap
40528045,"Show HN: Every mountain, building and tree shadow mapped for any date and time","I&#x27;ve been working on this project for about 4 years. It began as terrain only because world wide elevation data was publicly available. I then added buildings from OpenStreetMap (crowd sourced) and more recently from Overture Maps data. Some computer vision&#x2F;machine learning advancements [1] in the past few years have made it possible to estimate tree canopy heights using satellite imagery alone making it possible to finally add trees to the map. The data isn&#x27;t perfect, but it&#x27;s within +&#x2F;- 3 meters of so. Good enough to give a general idea for any location on Earth. Happy to answer any questions.<p>[1] <a href=""https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41559-023-02206-6"" rel=""nofollow"">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41559-023-02206-6</a>",685,tppiotrowski,1717099828,story,https://shademap.app
43683410,GPT-4.1 in the API,,680,maheshrijal,1744650105,story,https://openai.com/index/gpt-4-1/
40067486,Embeddings are a good starting point for the AI curious app developer,,675,bryantwolf,1713373754,story,https://bawolf.substack.com/p/embeddings-are-a-good-starting-point
43900877,OpenAI reaches agreement to buy Windsurf for $3B,,667,swyx,1746493068,story,https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion
43426022,OpenAI Audio Models,,661,KuzeyAbi,1742491080,story,https://www.openai.fm/
40837610,Show HN: Drop-in SQS replacement based on SQLite,"Hi! I wanted to share an open source API-compatible replacement for SQS. It&#x27;s written in Go, distributes as a single binary, and uses SQLite for underlying storage.<p>I wrote this because I wanted a queue with all the bells and whistles - searching, scheduling into the future, observability, and rate limiting - all the things that many modern task queue systems have.<p>But I didn&#x27;t want to rewrite my app, which was already using SQS. And I was frustrated that many of the best solutions out there (BullMQ, Oban, Sidekiq) were language-specific.<p>So I made an SQS-compatible replacement. All you have to do is replace the endpoint using AWS&#x27; native library in your language of choice.<p>For example, the queue works with Celery - you just change the connection string. From there, you can see all of your messages and their status, which is hard today in the SQS console (and flower doesn&#x27;t support SQS.)<p>It is written to be pluggable. The queue implementation uses SQLite, but I&#x27;ve been experimenting with RocksDB as a backend and you could even write one that uses Postgres. Similarly, you could implement multiple protocols (AMQP, PubSub, etc) on top of the underlying queue. I started with SQS because it is simple and I use it a lot.<p>It is written to be as easy to deploy as possible - a single go binary. I&#x27;m working on adding distributed and autoscale functionality as the next layer.<p>Today I have search, observability (via prometheus), unlimited message sizes, and the ability to schedule messages arbitrarily in the future.<p>In terms of monetization, the goal is to just have a hosted queue system. I believe this can be cheaper than SQS without sacrificing performance. Just as Backblaze and Minio have had success competing in the S3 space, I wanted to take a crack at queues.<p>I&#x27;d love your feedback!",656,memset,1719760268,story,https://github.com/poundifdef/SmoothMQ
44127739,Human coders are still better than LLMs,,655,longwave,1748536864,story,https://antirez.com/news/153
45004846,"Comet AI browser can get prompt injected from any site, drain your bank account",,648,helloplanets,1756048474,story,https://twitter.com/zack_overflow/status/1959308058200551721
43331847,RubyLLM: A delightful Ruby way to work with AI,,645,ksec,1741696855,story,https://github.com/crmne/ruby_llm
44827794,"GPT-5: Key characteristics, pricing and system card",System card: <a href=""https:&#x2F;&#x2F;cdn.openai.com&#x2F;pdf&#x2F;8124a3ce-ab78-4f06-96eb-49ea29ffb52f&#x2F;gpt5-system-card-aug7.pdf"" rel=""nofollow"">https:&#x2F;&#x2F;cdn.openai.com&#x2F;pdf&#x2F;8124a3ce-ab78-4f06-96eb-49ea29ffb...</a>,643,Philpax,1754588778,story,https://simonwillison.net/2025/Aug/7/gpt-5/
39307330,OpenAI compatibility,,643,Casteil,1707424568,story,https://ollama.ai/blog/openai-compatibility
45130260,LLM Visualization,,640,gmays,1757009165,story,https://bbycroft.net/llm
39443965,Let's Build the GPT Tokenizer [video],,640,davidbarker,1708449630,story,https://www.youtube.com/watch?v=zduSFxRajkE
44328326,Phoenix.new – Remote AI Runtime for Phoenix,,639,wut42,1750431424,story,https://fly.io/blog/phoenix-new-the-remote-ai-runtime/
39532367,Show HN: I made an app to use local AI as daily driver,"Hi Hackers,<p>Excited to share a macOS app I&#x27;ve been working on: <a href=""https:&#x2F;&#x2F;recurse.chat&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;recurse.chat&#x2F;</a> for chatting with local AI. While it&#x27;s amazing that you can run AI models locally quite easily these days (through llama.cpp &#x2F; llamafile &#x2F; ollama &#x2F; llm CLI etc.), I missed feature complete chat interfaces. Tools like LMStudio are super powerful, but there&#x27;s a learning curve to it. I&#x27;d like to hit a middleground of simplicity and customizability for advanced users.<p>Here&#x27;s what separates RecurseChat out from similar apps:<p>- UX designed for you to use local AI as a daily driver. Zero config setup, supports multi-modal chat, chat with multiple models in the same session, link your own gguf file.<p>- Import ChatGPT history. This is probably my favorite feature. Import your hundreds of messages, search them and even continuing previous chats using local AI offline.<p>- Full text search. Search for hundreds of messages and see results instantly.<p>- Private and capable of working completely offline.<p>Thanks to the amazing work of @ggerganov on llama.cpp which made this possible. If there is anything that you wish to exist in an ideal local AI app, I&#x27;d love to hear about it.",637,xyc,1709080800,story,https://recurse.chat/
43170850,Tell HN: Y Combinator backing AI company to abuse factory workers,"Optifye.ai is a dystopian company backed by Y Combinator. They’re using AI to further dehumanise and abuse individual factory workers and treat them like disposable automatons.<p>See a now deleted post where they show how it works:<p>https:&#x2F;&#x2F;hachyderm.io&#x2F;@YvanDaSilva&#x2F;114063748264591929<p>The founders look to be a couple of rich kids with little world and work experience:<p>&gt; We’re CS grads from Duke and because our families run manufacturing companies (…)<p>They also display a profound lack of empathy by bragging about lowering stress for rich company owners, which they do by increasing the stress of everyone who works for them:<p>&gt; Know any manufacturing company owners?<p>&gt; Let us know at founders@optifye.ai, and we’ll help them drop their cortisol levels :)<p>https:&#x2F;&#x2F;www.ycombinator.com&#x2F;companies&#x2F;optifye-ai<p>This is the AI world we all know is coming, brought to you by Y Combinator investors and founders. It doesn’t “benefit humanity”, it just serves to “put you in your place”.",631,latexr,1740485040,story,
45375477,ChatGPT Pulse,,627,meetpateltech,1758819595,story,https://openai.com/index/introducing-chatgpt-pulse/
40690898,Perplexity AI is lying about their user agent,,626,cdme,1718470093,story,https://rknight.me/blog/perplexity-ai-is-lying-about-its-user-agent/
42125888,"OpenAI, Google and Anthropic are struggling to build more advanced AI",,625,lukebennett,1731504531,story,https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai
39471388,Phind-70B: Closing the code quality gap with GPT-4 Turbo while running 4x faster,,625,rushingcreek,1708628063,story,https://www.phind.com/blog/introducing-phind-70b
42619139,Nvidia's Project Digits is a 'personal AI supercomputer',,623,magicalhippo,1736223245,story,https://techcrunch.com/2025/01/06/nvidias-project-digits-is-a-personal-ai-computer/
43158168,"Apple says it will add 20k jobs, spend $500B, produce AI servers in US",,623,helsinkiandrew,1740395134,story,https://www.bloomberg.com/news/articles/2025-02-24/apple-says-it-will-add-20-000-jobs-spend-500-billion-produce-ai-servers-in-us
40502090,Reproducing GPT-2 in llm.c,,618,tosh,1716911926,story,https://github.com/karpathy/llm.c/discussions/481
40639606,Private Cloud Compute: A new frontier for AI privacy in the cloud,,617,serhack_,1718056387,story,https://security.apple.com/blog/private-cloud-compute/
42938125,Google drops pledge not to use AI for weapons or surveillance,,617,jbegley,1738700896,story,https://www.washingtonpost.com/technology/2025/02/04/google-ai-policies-weapons-harm
45114753,"MIT Study Finds AI Use Reprograms the Brain, Leading to Cognitive Decline",,616,cainxinth,1756901179,story,https://publichealthpolicyjournal.com/mit-study-finds-artificial-intelligence-use-reprograms-the-brain-leading-to-cognitive-decline/
45110311,The maths you need to start understanding LLMs,,616,gpjt,1756854647,story,https://www.gilesthomas.com/2025/09/maths-for-llms
39121521,OpenAI scrapped a promise to disclose key documents to the public,,614,nickthegreek,1706124071,story,https://www.wired.com/story/openai-scrapped-promise-disclose-key-documents/
40899242,Show HN: A modern Jupyter client for macOS,"I love Jupyter – it&#x27;s how I learned to code back when I was working as a scientist. But I was always frustrated that there wasn&#x27;t a simple and elegant app that I could use with my Mac. I made do by wrapping JupyterLab in a chrome app, and then more recently switching to VS Code to make use of Copilot. I&#x27;ve always craved a more focused and lighter-weight experience when working in a notebook. That&#x27;s why I created Satyrn.<p>It starts up really fast (faster time-to-execution than VS Code or JupyterLab), you can launch notebooks right from the Finder, and the design is super minimalist. It&#x27;s got an OpenAI integration (use your own API key) for multi-cell generation with your notebook as context (I&#x27;ll add other LLMs soon). And many more useful features like a virtual environment management UI, Black code formatting, and easy image&#x2F;table copy buttons.<p>Full disclosure: it&#x27;s built with Electron. I originally wrote it in Swift but couldn&#x27;t get the editor experience to where I wanted it. Now it supports autocomplete, multi-cursor editing, and moving the cursor between cells just like you&#x27;d expect from JupyterLab or VS Code.<p>Satyrn sits on top of the jupyter-server, so it works with all your existing python kernels, Jupyter configuration, and ipynb files. It only works with local files at the moment, but I&#x27;m planning to extend it to support remote servers as well.<p>I&#x27;m an indie developer, and I will try to monetize at some point, but it&#x27;s free while in alpha. If you&#x27;re interested, please try it out!<p>I&#x27;d love your feedback in the comments, or you can contact me at jack-at-satyrn-dot-app.",611,jackhodkinson,1720375171,story,https://satyrn.app/
44572377,Show HN: Shoggoth Mini – A soft tentacle robot powered by GPT-4o and RL,,610,cataPhil,1752594401,story,https://www.matthieulc.com/posts/shoggoth-mini
44174965,"Deep learning gets the glory, deep fact checking gets ignored",,609,chmaynard,1748986316,story,https://rachel.fast.ai/posts/2025-06-04-enzyme-ml-fails/index.html
40766791,Llama.ttf: A font which is also an LLM,,608,fuglede_,1719144105,story,https://fuglede.github.io/llama.ttf/
44783155,Job-seekers are dodging AI interviewers,,607,robtherobber,1754294660,story,https://fortune.com/2025/08/03/ai-interviewers-job-seekers-unemployment-hiring-hr-teams/
42750420,Amazon's AI crawler is making my Git server unstable,,607,OptionOfT,1737226131,story,https://xeiaso.net/notes/2025/amazon-crawler/
41302597,Data Exfiltration from Slack AI via indirect prompt injection,,604,tprow50,1724178465,story,https://promptarmor.substack.com/p/data-exfiltration-from-slack-ai-via
43743337,Gemma 3 QAT Models: Bringing AI to Consumer GPUs,,602,emrah,1745151726,story,https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/
39380165,OpenAI – Application for US trademark “GPT” has failed,,602,vincent_s,1707983568,story,https://tsdr.uspto.gov/documentviewer?caseId=sn97733259&docId=FREF20240206125856&linkId=1#docIndex=0&page=1
43897772,Evolving OpenAI's Structure,,602,rohitpaulk,1746468482,story,https://openai.com/index/evolving-our-structure/
44623953,Coding with LLMs in the summer of 2025 – an update,,600,antirez,1753009442,story,https://antirez.com/news/154
39178886,Meta AI releases Code Llama 70B,,598,albert_e,1706548294,story,https://twitter.com/AIatMeta/status/1752013879532782075
44118023,Show HN: I rewrote my Mac Electron app in Rust,"A year ago, my co-founder launched Desktop Docs here on HN. It&#x27;s a Mac app we built with Electron that uses CLIP embeddings to search photos and videos locally with natural language. We got positive feedback from HN and our first paying customers, but the app was almost 1GB and clunky to use.<p>TLDR; rebuilding in Rust was the right move.<p>So we rewrote the app with Rust and Tauri and here are the results:<p>- App size is 83% smaller: 1GB → 172MB - DMG Installer is 70% smaller: 232MB → 69.5MB - Indexing files is faster: A 38-minute video now indexes in ~3 minutes instead of 10-14 minutes - Overall more stability (old app used to randomly crash)<p>The original version worked, but it didn&#x27;t perform well when you tried indexing thousands of images or large videos. We lost a lot of time struggling to optimize Electron’s main-renderer process communication and ended up with a complex worker system to process large batches of media files.<p>For months we wrestled with indecision about continuing to optimize the Electron app vs. starting a full rebuild in Swift or Rust. The main thing holding us back was that we hadn’t coded in Swift in almost 10 years and we didn’t know Rust very well.<p>What finally broke us was when users complained the app crashed their video calls just running in background. I guess that’s what happens when you ship an app with Chromium that takes up 200mb before any application code.<p>Today the app still uses CLIP for embeddings and Redis for vector storage and search, except Rust now handles the image and video processing pipeline and all the file I&#x2F;O to let users browse their entire machine, not just indexed files.<p>For the UI, we decided to rebuild it from scratch instead of porting over the old UI. This turned out well because it resulted in a cleaner, simpler UI after living with the complexity of the old version.<p>The trickiest part of the migration was learning Rust. LLMs definitely help, but the Rust&#x2F;Tauri community just isn’t as mature compared to Electron. Bundling Redis into the app was a permissioning nightmare, but I think our solution with Rust handles this better than what we had with Electron.<p>All in, the rebuild took about two months and still needs some more work to be at total parity with its Electron version, but the core functionality of indexing and searching files is way more performant than before and that made it worth the time. Sometimes you gotta throw away working code to build the right thing.<p>AMA about Rust&#x2F;Tauri migration, Redis bundling nightmares, how CLIP embeddings work for local semantic search, or why Electron isn&#x27;t always the answer.",597,katrinarodri,1748451191,story,https://desktopdocs.com/?v=2025
43933891,LegoGPT: Generating Physically Stable and Buildable Lego,,596,nkko,1746766520,story,https://avalovelace1.github.io/LegoGPT/
38781941,The New York Times is suing OpenAI and Microsoft for copyright infringement,,593,ssgodderidge,1703685501,story,https://www.theverge.com/2023/12/27/24016212/new-york-times-openai-microsoft-lawsuit-copyright-infringement
44942731,"Show HN: Whispering – Open-source, local-first dictation you can trust","Hey HN! Braden here, creator of Whispering, an open-source speech-to-text app.<p>I really like dictation. For years, I relied on transcription tools that were <i>almost</i> good, but they were all closed-source. Even a lot of them that claimed to be “local” or “on-device” were still black boxes that left me wondering where my audio really went.<p>So I built Whispering. It’s open-source, local-first, and most importantly, transparent with your data.   Your data is stored locally on your device, and your audio goes directly from your machine to a local provider (Whisper C++, Speaches, etc.) or your chosen cloud provider (Groq, OpenAI, ElevenLabs, etc.). For me, the features were good enough that I left my paid tools behind (I used Superwhisper and Wispr Flow before).<p>Productivity apps should be open-source and transparent with your data, but they also need to match the UX of paid, closed-software alternatives. I hope Whispering is near that point. I use it for several hours a day, from coding to thinking out loud while carrying pizza boxes back from the office.<p>Here’s an overview: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1jYgBMrfVZs"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1jYgBMrfVZs</a>, and here’s how I personally am using it with Claude Code these days: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tpix588SeiQ"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tpix588SeiQ</a>.<p>There are plenty of transcription apps out there, but I hope Whispering adds some extra competition from the OSS ecosystem (one of my other OSS favorites is Handy <a href=""https:&#x2F;&#x2F;github.com&#x2F;cjpais&#x2F;Handy"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;cjpais&#x2F;Handy</a>). Whispering has a few tricks up its sleeve, like a voice-activated mode for hands-free operation (no button holding), and customizable AI transformations with any prompt&#x2F;model.<p>Whispering used to be in my personal GH repo, but I recently moved it as part of a larger project called Epicenter (<a href=""https:&#x2F;&#x2F;github.com&#x2F;epicenter-so&#x2F;epicenter"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;epicenter-so&#x2F;epicenter</a>), which I should explain a bit...<p>I’m basically obsessed with local-first open-source software. I think there should be an open-source, local-first version of every app, and I would like them all to work together. The idea of Epicenter is to store your data in a folder of plaintext and SQLite, and build a suite of interoperable, local-first tools on top of this shared memory. Everything is totally transparent, so you can trust it.<p>Whispering is the first app in this effort. It’s not there yet regarding memory, but it’s getting there. I’ll probably write more about the bigger picture soon, but mainly I just want to make software and let it speak for itself (no pun intended in this case!), so this is my Show HN for now.<p>I just finished college and was about to move back with my parents and work on this instead of getting a job…and then I somehow got into YC. So my current plan is to cover my living expenses and use the YC funding to support maintainers, our dependencies, and people working on their own open-source local-first projects. More on that soon.<p>Would love your feedback, ideas, and roasts. If you would like to support the project, star it on GitHub here (<a href=""https:&#x2F;&#x2F;github.com&#x2F;epicenter-so&#x2F;epicenter"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;epicenter-so&#x2F;epicenter</a>) and join the Discord here (<a href=""https:&#x2F;&#x2F;go.epicenter.so&#x2F;discord"">https:&#x2F;&#x2F;go.epicenter.so&#x2F;discord</a>). Everything’s MIT licensed, so fork it, break it, ship your own version, copy whatever you want!",591,braden-w,1755535949,story,https://github.com/epicenter-so/epicenter/tree/main/apps/whispering
39058428,Nightshade: An offensive tool for artists against AI art generators,,590,ink404,1705686125,story,https://nightshade.cs.uchicago.edu/whatis.html
45919067,SlopStop: Community-driven AI slop detection in Kagi Search,,589,msub2,1763060606,story,https://blog.kagi.com/slopstop
40648960,ARC Prize – a $1M+ competition towards open AGI progress,"Hey folks! Mike here. Francois Chollet and I are launching ARC Prize, a public competition to beat and open-source the solution to the ARC-AGI eval.<p>ARC-AGI is (to our knowledge) the only eval which measures AGI: a system that can efficiently acquire new skill and solve novel, open-ended problems. Most AI evals measure skill directly vs the acquisition of new skill.<p>Francois created the eval in 2019, SOTA was 20% at inception, SOTA today is only 34%. Humans score 85-100%. 300 teams attempted ARC-AGI last year and several bigger labs have attempted it.<p>While most other skill-based evals have rapidly saturated to human-level, ARC-AGI was designed to resist “memorization” techniques (eg. LLMs)<p>Solving ARC-AGI tasks is quite easy for humans (even children) but impossible for modern AI. You can try ARC-AGI tasks yourself here: <a href=""https:&#x2F;&#x2F;arcprize.org&#x2F;play"" rel=""nofollow"">https:&#x2F;&#x2F;arcprize.org&#x2F;play</a><p>ARC-AGI consists of 400 public training tasks, 400 public test tasks, and 100 secret test tasks. Every task is novel. SOTA is measured against the secret test set which adds to the robustness of the eval.<p>Solving ARC-AGI tasks requires no world knowledge, no understanding of language. Instead each puzzle requires a small set of “core knowledge priors” (goal directedness, objectness, symmetry, rotation, etc.)<p>At minimum, a solution to ARC-AGI opens up a completely new programming paradigm where programs can perfectly and reliably generalize from an arbitrary set of priors. At maximum, unlocks the tech tree towards AGI.<p>Our goal with this competition is:<p>1. Increase the number of researchers working on frontier AGI research (vs tinkering with LLMs). We need new ideas and the solution is likely to come from an outsider! 2. Establish a popular, objective measure of AGI progress that the public can use to understand how close we are to AGI (or not). Every new SOTA score will be published here: <a href=""https:&#x2F;&#x2F;x.com&#x2F;arcprize"" rel=""nofollow"">https:&#x2F;&#x2F;x.com&#x2F;arcprize</a> 3. Beat ARC-AGI and learn something new about the nature of intelligence.<p>Happy to answer questions!",588,mikeknoop,1718126381,story,https://arcprize.org/blog/launch
38432486,Show HN: A Dalle-3 and GPT4-Vision feedback loop,"I used to enjoy Translation Party, and over the weekend I realized that we can build the same feedback loop with DALLE-3 and GPT4-Vision.  Start with a text prompt, let DALLE-3 generate an image, then GPT-4 Vision turns that image back into a text prompt, DALLE-3 creates another image, and so on.<p>You need to bring your own OpenAI API key (costs about $0.10&#x2F;run)<p>Some prompts are very stable, others go wild. If you bias GPT4&#x27;s prompting by telling it to &quot;make it weird&quot; you can get crazy results.<p>Here&#x27;s a few of my favorites:<p>- Gnomes: <a href=""https:&#x2F;&#x2F;dalle.party&#x2F;?party=k4eeMQ6I"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;dalle.party&#x2F;?party=k4eeMQ6I</a><p>- Start with a sailboat but bias GPT4V to &quot;replace everything with cats&quot;: <a href=""https:&#x2F;&#x2F;dalle.party&#x2F;?party=0uKfJjQn"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;dalle.party&#x2F;?party=0uKfJjQn</a><p>- A more stable one (but everyone is always an actor): <a href=""https:&#x2F;&#x2F;dalle.party&#x2F;?party=oxpeZKh5"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;dalle.party&#x2F;?party=oxpeZKh5</a>",587,z991,1701094704,story,https://dalle.party/
42174204,"Launch HN: Regatta Storage (YC F24) – Turn S3 into a local-like, POSIX cloud FS","Hey HN, I’m Hunter the founder of Regatta Storage (<a href=""https:&#x2F;&#x2F;regattastorage.com"">https:&#x2F;&#x2F;regattastorage.com</a>). Regatta Storage is a new cloud file system that provides unlimited pay-as-you-go capacity, local-like performance, and automatic synchronization to S3-compatible storage. For example, you can use Regatta to instantly access massive data sets in S3 with Spark, Pytorch, or pandas without paying for large, local disks or waiting for the data to download.<p>Check out an overview of how the service works here: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xh1q5p7E4JY"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=xh1q5p7E4JY</a>, and you can try it for free at <a href=""https:&#x2F;&#x2F;regattastorage.com"">https:&#x2F;&#x2F;regattastorage.com</a> after signing up for an account. We wanted to let you try it without an account, but we figured that “Hacker News shares a file system and S3 bucket” wouldn’t be the best experience for the community.<p>I built Regatta after spending nearly a decade building and operating at-scale cloud storage at places like Amazon’s Elastic File System (EFS) and Netflix. During my 8 years at EFS, I learned a lot about how teams thought about their storage usage. Users frequently told me that they loved how simple and scalable EFS was, and -- like S3 -- they didn’t have to guess how much capacity they needed up front.<p>When I got to Netflix, I was surprised that there wasn’t more usage of EFS. If you looked around, it seemed like a natural fit. Every application needed a POSIX file system. Lots of applications had unclear or spikey storage needs. Often, developers wanted their storage to last beyond the lifetime of an individual instance or container. In fact, if you looked across all Netflix applications, some ridiculous amount of money was being spent on <i>empty storage space</i> because each of these local drives had to be overprovisioned for potential usage.<p>However, in many cases, EFS wasn’t the perfect choice for these workloads. Moving workloads from local disks to NFS often encountered performance issues. Further, applications which treated their local disks as ephemeral would have to manually “clean up” left over data in a persistent storage system.<p>At this point, I realized that there was a missing solution in the cloud storage market which wasn’t being filled by either block or file storage, and I decided to build Regatta.<p>Regatta is a pay-as-you-go cloud file system that automatically expands with your application. Because it automatically synchronizes with S3 using native file formats, you can connect it to existing data sets and use recently written file data directly from S3. When data isn’t actively being used, it’s removed from the Regatta cache, so you only pay for the backing S3 storage. Finally, we’re developing a custom file protocol which allows us to achieve local-like performance for small-file workloads <i>and</i> Lustre-like scale-out performance for distributed data jobs.<p>Under the hood, customers mount a Regatta file system by connecting to our fleet of caching instances over NFSv3 (soon, our custom protocol). Our instances then connect to the customer’s S3 bucket on the backend, and provide sub-millisecond cached-read and write performance. This durable cache allows us to provide a strongly consistent, efficient view of the file system to all connected file clients. We can perform challenging operations (like directory renaming) quickly and durably, while they asynchronously propagate to the S3 bucket.<p>We’re excited to see users share our vision for Regatta. We have teams who are using us to build totally serverless Jupyter notebook servers for their AI researchers who prefer to upload and share data using the S3 web UI. We have teams who are using us as a distributed caching layer on top of S3 for low-latency access to common files. We have teams who are replacing their thin-provisioned Ceph boot volumes with Regatta for significant savings. We can’t wait to see what other things people will build and we hope you’ll give us a try at regattastorage.com.<p>We’d love to get any early feedback from the community, ideas for future direction, or experiences in this space. I’ll be in the comments for the next few hours to respond!",587,huntaub,1731948549,story,
40665721,Uncensor any LLM with abliteration,,586,mizzao,1718250159,story,https://huggingface.co/blog/mlabonne/abliteration
42952960,Andrej Karpathy: Deep Dive into LLMs Like ChatGPT [video],,582,leroman,1738780170,story,https://www.youtube.com/watch?v=7xTGNNLPyMI
42485938,GPT-5 is behind schedule,,582,owenthejumper,1734870573,story,https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693
42411608,Elon Musk wanted an OpenAI for-profit,,582,arvindh-manian,1734118601,story,https://openai.com/index/elon-musk-wanted-an-openai-for-profit/
44240302,Launch HN: Vassar Robotics (YC X25) – $219 robot arm that learns new skills,"Hi HN — I’m Charles from Vassar Robotics (<a href=""https:&#x2F;&#x2F;vassarrobotics.com&#x2F;"">https:&#x2F;&#x2F;vassarrobotics.com&#x2F;</a> - not much there but you can order the robot at <a href=""https:&#x2F;&#x2F;shop.vassarrobotics.com&#x2F;products&#x2F;navrim-robot-that-learns-skills-in-30-minutes"">https:&#x2F;&#x2F;shop.vassarrobotics.com&#x2F;products&#x2F;navrim-robot-that-l...</a>)<p>Edit: the entire run sold out thanks to HN today—thank you all! And sorry to anyone who missed out. You can get in on the next batch here: <a href=""https:&#x2F;&#x2F;vassarrobotics.com&#x2F;newsletter"">https:&#x2F;&#x2F;vassarrobotics.com&#x2F;newsletter</a>.<p>We are bringing an upgraded version of the long beloved SO-101 robot arms to a $219 price point with improved mechanical design and added intelligence. See what it can do here: <a href=""https:&#x2F;&#x2F;youtube.com&#x2F;shorts&#x2F;xNyPKJZI400"" rel=""nofollow"">https:&#x2F;&#x2F;youtube.com&#x2F;shorts&#x2F;xNyPKJZI400</a> (demos are sped up as shown in the video)<p>I’ve spent a few years building RC planes (<a href=""https:&#x2F;&#x2F;cyo.ng&#x2F;hangar&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;cyo.ng&#x2F;hangar&#x2F;</a>) and micro gas turbines (<a href=""https:&#x2F;&#x2F;set.mit.edu"" rel=""nofollow"">https:&#x2F;&#x2F;set.mit.edu</a>), and I’ve always wished hardware were cheaper so more people could experiment.<p>I’m now launching a $219 desktop robot-arm kit that keeps LeRobot SO-101’s kinematics, swaps key parts for sturdier, more precise SLA prints, and adds two integrated 480 p cameras. After plenty of supplier haggling, the whole kit costs less than the twelve servos alone. I’ll release the updated mechanical design under an MIT license by June 30.<p>On the software side, I&#x27;ll also release an MIT-licensed MCP server by June 30 that exposes the local robot policy as tools for agentic LLMs (Opus 4, o3, etc.) to use in long-horizon tasks. Here&#x27;s how it works: You can teach the robot new skills through teleoperation. During inference, you simply talk to the agentic LLM using natural language instructions. The LLM then calls the local robot policy through MCP, automatically decomposing your high-level requests into executable robot commands.<p>Thanks to the LeRobot community for making such an amazing robot accessible. If you’ve contributed to the LeRobot GitHub repo, email hello@vassarrobotics.com for a 20% discount coupon as a small thank-you.<p>I’d love your feedback! Beyond manufacturing, cleaning up the codebase, and writing docs, I’m considering: a force-controlled gripper, a parallel-jaw gripper, an extra wrist DOF (matching the new Trossen and ARX arms), full force feedback on the leader arm (though that may triple the price), a more affordable version with lower resolution each joint, and a longer-reach variant. Which of these—or something else—would be most useful to you?<p>You can order it here if you want: <a href=""https:&#x2F;&#x2F;shop.vassarrobotics.com&#x2F;products&#x2F;navrim-robot-that-learns-skills-in-30-minutes"">https:&#x2F;&#x2F;shop.vassarrobotics.com&#x2F;products&#x2F;navrim-robot-that-l...</a>. (Edit: sold out! You can get in on the next batch here: <a href=""https:&#x2F;&#x2F;vassarrobotics.com&#x2F;newsletter"">https:&#x2F;&#x2F;vassarrobotics.com&#x2F;newsletter</a>. I hope we can have your business in the future.)<p>Looking forward to any and all comments!<p>---<p>Edit: A quick explanation regarding shipping times (as stated on our shop page):<p>• The first batch of 20 units, which will be shipped by June 30, is sold out.<p>• The second batch of 100 units will be shipped by July 15 (unassembled kits) and July 21 (assembled units). The order limit is to ensure we can ship on time and maintain high quality.<p>For those who have already placed orders: I will reach out individually to ask if you would like to receive weekly progress updates from now until the shipping date.",580,charleszyong,1749582737,story,
42877709,Antiqua et Nova: Note on the relationship between AI and human intelligence,,580,max_,1738245687,story,https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html
43603453,Recent AI model progress feels mostly like bullshit,,579,paulpauper,1743962519,story,https://www.lesswrong.com/posts/4mvphwx5pdsZLMmpY/recent-ai-model-progress-feels-mostly-like-bullshit
42208383,Show HN: Llama 3.2 Interpretability with Sparse Autoencoders,"I spent a lot of time and money on this rather big side project of mine that attempts to replicate the mechanistic interpretability research on proprietary LLMs that was quite popular this year and produced great research papers by Anthropic [1], OpenAI [2] and Deepmind [3].<p>I am quite proud of this project and since I consider myself the target audience for HackerNews did I think that maybe some of you would appreciate this open research replication as well. Happy to answer any questions or face any feedback.<p>Cheers<p>[1] <a href=""https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticity&#x2F;index.html"" rel=""nofollow"">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2024&#x2F;scaling-monosemanticit...</a><p>[2] <a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.04093"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2406.04093</a><p>[3] <a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.05147"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2408.05147</a>",579,PaulPauls,1732221476,story,https://github.com/PaulPauls/llama3_interpretability_sae
39643136,Show HN: Hatchet – Open-source distributed task queue,"Hello HN, we&#x27;re Gabe and Alexander from Hatchet (<a href=""https:&#x2F;&#x2F;hatchet.run"">https:&#x2F;&#x2F;hatchet.run</a>), we&#x27;re working on an open-source, distributed task queue. It&#x27;s an alternative to tools like Celery for Python and BullMQ for Node.js, primarily focused on reliability and observability. It uses Postgres for the underlying queue.<p>Why build another managed queue? We wanted to build something with the benefits of full transactional enqueueing - particularly for dependent, DAG-style execution - and felt strongly that Postgres solves for 99.9% of queueing use-cases better than most alternatives (Celery uses Redis or RabbitMQ as a broker, BullMQ uses Redis). Since the introduction of SKIP LOCKED and the milestones of recent PG releases (like active-active replication), it&#x27;s becoming more feasible to horizontally scale Postgres across multiple regions and vertically scale to 10k TPS or more. Many queues (like BullMQ) are built on Redis and data loss can occur when suffering OOM if you&#x27;re not careful, and using PG helps avoid an entire class of problems.<p>We also wanted something that was significantly easier to use and debug for application developers. A lot of times the burden of building task observability falls on the infra&#x2F;platform team (for example, asking the infra team to build a Grafana view for their tasks based on exported prom metrics). We&#x27;re building this type of observability directly into Hatchet.<p>What do we mean by &quot;distributed&quot;? You can run workers (the instances which run tasks) across multiple VMs, clusters and regions - they are remotely invoked via a long-lived gRPC connection with the Hatchet queue. We&#x27;ve attempted to optimize our latency to get our task start times down to 25-50ms and much more optimization is on the roadmap.<p>We also support a number of extra features that you&#x27;d expect, like retries, timeouts, cron schedules, dependent tasks. A few things we&#x27;re currently working on - we use RabbitMQ (confusing, yes) for pub&#x2F;sub between engine components and would prefer to just use Postgres, but didn&#x27;t want to spend additional time on the exchange logic until we built a stable underlying queue. We are also considering the use of NATS for engine-engine and engine-worker connections.<p>We&#x27;d greatly appreciate any feedback you have and hope you get the chance to try out Hatchet.",578,abelanger,1709917655,story,https://github.com/hatchet-dev/hatchet
44660519,You can now disable all AI features in Zed,,577,meetpateltech,1753285515,story,https://zed.dev/blog/disable-ai-features
42289955,Ask HN: How can I grow as an engineer without good seniors to learn from?,"I am a fresh graduate data engineer working at a small company in the oil and drilling industry.<p>I was hired 6 months ago as a freelance data engineer, and after proving myself through my work quality, I am now essentially functioning as a tech lead, with full responsibility and ownership of designing, implementing, and hiring for the projects I&#x27;m assigned.<p>Our company is not a tech company, so I only have a couple of tech-oriented colleagues, and I barely interact with them. Now I directly report to the director of the company, who in all senses is awesome, with 40+ years of combined experience in some of the biggest oil and drilling companies globally.<p>However, I have some strong FOMO about not being able to learn much technical stuff from my peers or seniors. I am trying my best to learn and pick things up on my own, learning design principles, getting code reviews from chatGPT, etc. But even then, I&#x27;m afraid I am not producing the software to the highest standards of the industry since we don&#x27;t have any rigorous cross-checking, and might be missing out on a lot of learning.<p>Can someone who has been in positions similar to these please guide me?",577,prathameshgh,1733079382,story,
43414393,AI Blindspots – Blindspots in LLMs I've noticed while AI coding,,577,rahimnathwani,1742402912,story,https://ezyang.github.io/ai-blindspots/
40448045,"OpenAI didn’t copy Scarlett Johansson’s voice for ChatGPT, records show",,574,richardatlarge,1716419812,story,https://www.washingtonpost.com/technology/2024/05/22/openai-scarlett-johansson-chatgpt-ai-voice/
44840728,Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?,"Sam said yesterday that chatgpt handles ~700M weekly users. Meanwhile, I can&#x27;t even run a single GPT-4-class model locally without insane VRAM or painfully slow speeds.<p>Sure, they have huge GPU clusters, but there must be more going on - model optimizations, sharding, custom hardware, clever load balancing, etc.<p>What engineering tricks make this possible at such massive scale while keeping latency low?<p>Curious to hear insights from people who&#x27;ve built large-scale ML systems.",574,superasn,1754681248,story,
43970837,Ask HN: How are you acquiring your first hundred users?,I am building a B2C AI SaaS with $50&#x2F;month price. How would you go about getting with first 100 users and then the next 500 users.<p>What we are currently doing: 1) Cold outreach to power users - to convert them into affiliates. 2) Cold outreach to individuals who have target ICP communities. 3) SEO for more long term (not for the first 500),573,amanchanda,1747125685,story,
45336989,"Qwen3-Omni: Native Omni AI model for text, image and video",,571,meetpateltech,1758563421,story,https://github.com/QwenLM/Qwen3-Omni
44875848,What's the strongest AI model you can train on a laptop in five minutes?,,571,ingve,1755004534,story,https://www.seangoedecke.com/model-on-a-mbp/
44432385,Cloudflare to introduce pay-per-crawl for AI bots,,569,scotchmi_st,1751365227,story,https://blog.cloudflare.com/introducing-pay-per-crawl/
41295923,Artificial intelligence is losing hype,,568,bx376,1724116403,story,https://www.economist.com/finance-and-economics/2024/08/19/artificial-intelligence-is-losing-hype
39692387,A generalist AI agent for 3D virtual environments,,559,nuz,1710343376,story,https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/
45453586,"OpenAI's H1 2025: $4.3B in income, $13.5B in loss",,558,breadsniffer,1759430248,story,https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025
43840842,Sycophancy in GPT-4o,,557,dsr12,1745982386,story,https://openai.com/index/sycophancy-in-gpt-4o/
43707719,OpenAI o3 and o4-mini,,555,maheshrijal,1744822914,story,https://openai.com/index/introducing-o3-and-o4-mini/
45904551,"GPT-5.1: A smarter, more conversational ChatGPT",,555,tedsanders,1762974341,story,https://openai.com/index/gpt-5-1/
44571740,Ask HN: Is it time to fork HN into AI/LLM and ""Everything else/other?"","I would very much like to enjoy HN the way I did years ago, as a place where I&#x27;d discover things that I never otherwise would have come across.<p>The increasing AI&#x2F;LLM domination of the site has made it much less appealing to me.",553,bookofjoe,1752591077,story,
44382752,Define policy forbidding use of AI code generators,,551,todsacerdoti,1750894015,story,https://github.com/qemu/qemu/commit/3d40db0efc22520fa6c399cf73960dced423b048
45449348,Potential issues in curl found using AI assisted tools,<a href=""https:&#x2F;&#x2F;joshua.hu&#x2F;llm-engineer-review-sast-security-ai-tools-pentesters"" rel=""nofollow"">https:&#x2F;&#x2F;joshua.hu&#x2F;llm-engineer-review-sast-security-ai-tools...</a><p><a href=""https:&#x2F;&#x2F;joshua.hu&#x2F;files&#x2F;AI_SAST_PRESENTATION.pdf"" rel=""nofollow"">https:&#x2F;&#x2F;joshua.hu&#x2F;files&#x2F;AI_SAST_PRESENTATION.pdf</a>,547,robhlam,1759411795,story,https://mastodon.social/@bagder/115241241075258997
40252569,Show HN: I built a free in-browser Llama 3 chatbot powered by WebGPU,"I spent the last few days building out a nicer ChatGPT-like interface to use Mistral 7B and Llama 3 fully within a browser (no deps and installs).<p>I’ve used the WebLLM project by MLC AI for a while to interact with LLMs in the browser when handling sensitive data but I found their UI quite lacking for serious use so I built a much better interface around WebLLM.<p>I’ve been using it as a therapist and coach. And it’s wonderful knowing that my personal information never leaves my local computer.<p>Should work on Desktop with Chrome or Edge. Other browsers are adding WebGPU support as well - see the Github for details on how you can get it to work on other browsers.<p>Note: after you send the first message, the model will be downloaded to your browser cache. That can take a while depending on the model and your internet connection. But on subsequent page loads, the model should be loaded from the IndexedDB cache so it should be much faster.<p>The project is open source (Apache 2.0) on Github. If you like it, I’d love contributions, particularly around making the first load faster.<p>Github: <a href=""https:&#x2F;&#x2F;github.com&#x2F;abi&#x2F;secret-llama"">https:&#x2F;&#x2F;github.com&#x2F;abi&#x2F;secret-llama</a> Demo: <a href=""https:&#x2F;&#x2F;secretllama.com"" rel=""nofollow"">https:&#x2F;&#x2F;secretllama.com</a>",547,abi,1714771606,story,https://github.com/abi/secret-llama
39029801,AlphaGeometry: An Olympiad-level AI system for geometry,,545,FlawedReformer,1705508537,story,https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/
44301809,Building Effective AI Agents,,543,Anon84,1750182605,story,https://www.anthropic.com/engineering/building-effective-agents
40199715,GPT-4.5 or GPT-5 being tested on LMSYS?,,543,atemerev,1714405141,story,https://rentry.co/GPT2
42521865,Does current AI represent a dead end?,,543,jnord,1735305857,story,https://www.bcs.org/articles-opinion-and-research/does-current-ai-represent-a-dead-end/
44711306,Tao on “blue team” vs. “red team” LLMs,,542,qsort,1753713399,story,https://mathstodon.xyz/@tao/114915604830689046
39483482,"Show HN: OK-Robot: open, modular home robot framework for pick-and-drop anywhere","Hi all, excited to share our latest work, OK-Robot, which is an open and modular framework to perform navigation and manipulation with a robot assistant in practically any homes without having to teach the robot anything new! You can simply unbox the target robot, install OK-Robot, give it a &quot;scan&quot; (think a 60 second iPhone video), and start asking the robot to move arbitrary things from A to B. We already tested it out in 10 home environments in New York city, and one environment each in Pittsburgh and Fremont.<p>We based everything off of the current best machine learning models, and so things don&#x27;t quite work perfectly all the time, so we are hoping to build it together with the community! Our code is open: <a href=""https:&#x2F;&#x2F;github.com&#x2F;ok-robot&#x2F;ok-robot"">https:&#x2F;&#x2F;github.com&#x2F;ok-robot&#x2F;ok-robot</a> and we have a Discord server for discussion and support: <a href=""https:&#x2F;&#x2F;discord.gg&#x2F;wzzZJxqKYC"" rel=""nofollow"">https:&#x2F;&#x2F;discord.gg&#x2F;wzzZJxqKYC</a> If you are curious what works and what doesn&#x27;t work, take a quick look at <a href=""https:&#x2F;&#x2F;ok-robot.github.io&#x2F;#analysis"" rel=""nofollow"">https:&#x2F;&#x2F;ok-robot.github.io&#x2F;#analysis</a> or read our paper for a detailed analysis: <a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.12202"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2401.12202</a><p>P.S.: while the code is open the project unfortunately isn&#x27;t fully open source since one of our dependencies, AnyGrasp, has a closed-source, educational license. Apologize in advance, but we used it since that was the best grasping model we could have access to!<p>Would love to hear more thoughts and feedback on this project!",542,MahiShafiullah,1708709003,story,https://ok-robot.github.io/
41315138,I'm tired of fixing customers' AI generated code,,541,BitWiseVibe,1724282181,story,https://medium.com/@thetateman/im-tired-of-fixing-customers-ai-generated-code-94816bde4ceb
43835445,Chain of Recursive Thoughts: Make AI think harder by making it argue with itself,,539,miles,1745947144,story,https://github.com/PhialsBasement/Chain-of-Recursive-Thoughts
41534474,OpenAI threatens to revoke o1 access for asking it about its chain of thought,,538,jsheard,1726256614,story,https://twitter.com/SmokeAwayyy/status/1834641370486915417
44201872,Meta: Shut down your invasive AI Discover feed,,538,speckx,1749223990,story,https://www.mozillafoundation.org/en/campaigns/meta-shut-down-your-invasive-ai-discover-feed-now/
45898789,Pakistani newspaper mistakenly prints AI prompt with the article,,537,wg0,1762946226,story,https://twitter.com/omar_quraishi/status/1988518627859951986
43039308,Phind 2: AI search with visual answers and multi-step reasoning,"Hi HN! Michael here. We&#x27;ve spent the last 6 months rebuilding Phind. We asked ourselves what types of answers we would ideally like and crafted a new UI and model series to help get us there. Our new 70B is completely different from the one we launched a year ago.<p>The new Phind goes beyond text to present answers visually with inline images, diagrams, cards, and other widgets to make answers more meaningful:<p>- &quot;<i>explain photosynthesis</i>&quot; - <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cTCpnyICukM#t=7"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cTCpnyICukM#t=7</a><p>- &quot;<i>how to cook the perfect steak</i>&quot; - <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cTCpnyICukM#t=55"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cTCpnyICukM#t=55</a><p>- &quot;<i>quicksort in rust</i>&quot; - <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cTCpnyICukM#t=105"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cTCpnyICukM#t=105</a><p>Phind is also now able to seek out information on its own. If it needs more, it will do multiple rounds of additional searches to get you a more comprehensive answer:<p>- &quot;<i>top 10 Thai restaurants in SF, their prices, and key dishes</i>&quot; - <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rIQQcDIIHFQ#t=11"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rIQQcDIIHFQ#t=11</a><p>It can also perform calculations, visualize their results, and verify them in a Jupyter notebook:<p>- &quot;<i>simulate 100 coin flips and make graphs</i>&quot; - <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YP3PZ4MKGCg#t=8"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YP3PZ4MKGCg#t=8</a><p>- &quot;<i>train a perceptron neural network using Jupyter</i>&quot; - <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YP3PZ4MKGCg#t=45"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YP3PZ4MKGCg#t=45</a><p>This blog post contains an overview of what we did as well as technical deep dives into how we built the new frontend and models.<p>I&#x27;m super grateful for all of the feedback we&#x27;ve gotten from this community and can&#x27;t wait to hear your thoughts!",537,rushingcreek,1739470829,story,https://www.phind.com/blog/phind-2
39511530,Microsoft strikes deal with Mistral in push beyond OpenAI,,537,jmsflknr,1708957505,story,https://www.ft.com/content/cd6eb51a-3276-450f-87fd-97e8410db9eb
42909166,Ask HN: What is interviewing like now with everyone using AI?,Have you gone back to in-person whiteboards? More focus on practical problems? I really have no idea how the traditional tech interview is supposed to work now when problems are trivially solvable by GPT.,535,ramesh31,1738509572,story,
44637352,AccountingBench: Evaluating LLMs on real long-horizon business tasks,,534,rickcarlino,1753116508,story,https://accounting.penrose.com/
45423917,Comprehension debt: A ticking time bomb of LLM-generated code,,532,todsacerdoti,1759228659,story,https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/
42806328,Llama.vim – Local LLM-assisted text completion,,530,kgwgk,1737655602,story,https://github.com/ggml-org/llama.vim
45671778,Meta is axing 600 roles across its AI division,,530,Lionga,1761151483,story,https://www.theverge.com/news/804253/meta-ai-research-layoffs-fair-superintelligence
42431103,Ask HN: SWEs how do you future-proof your career in light of LLMs?,"LLMs are becoming a part of software engineering career.<p>The more I speak with fellow engineers, the more I hear that some of them are either using AI to help them code, or feed entire projects to AI and let the AI code, while they do code review and adjustments.<p>I didn&#x27;t want to believe in it, but I think it&#x27;s here. And even arguments like &quot;feeding proprietary code&quot; will be eventually solved by companies hosting their own isolated LLMs as they become better and hardware becomes more available.<p>My prediction is that junior to mid level software engineering will disappear mostly, while senior engineers will transition to be more of a guiding hand to LLMs output, until eventually LLMs will become so good, that senior people won&#x27;t be needed any more.<p>So, fellow software engineers, how do you future-proof your career in light of, the inevitable, LLM take over?<p>--- EDIT ---<p>I want to clarify something, because there seems to be slight misunderstanding.<p>A lot of people have been talking about SWE being not only about code, and I agree with that. But it&#x27;s also easier to sell this idea to a young person who is just starting in this career. And while I want this Ask HN to be helpful to young&#x2F;fresh engineers as well, I&#x27;m more interested in getting help for myself, and many others who are in a similar position.<p>I have almost two decades of SWE experience. But despite that, I seem to have missed the party where they told us that &quot;coding is not a means to an end&quot;, and realized it in the past few years. I bet there are people out there who are in a similar situations. How can we future-proof our career?",530,throwaway_43793,1734358279,story,
39679787,Devin: AI Software Engineer,,530,neural_thing,1710252915,story,https://www.cognition-labs.com/blog
38790255,NY Times copyright suit wants OpenAI to delete all GPT instances,,529,justinc8687,1703740027,story,https://arstechnica.com/tech-policy/2023/12/ny-times-sues-open-ai-microsoft-over-copyright-infringement/
44110584,"Show HN: My LLM CLI tool can run tools now, from Python code or plugins",,529,simonw,1748379183,story,https://simonwillison.net/2025/May/27/llm-tools/
44387659,AlphaGenome: AI for better understanding the genome,,529,i_love_limes,1750947360,story,https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/
44616479,Nobody knows how to build with AI yet,,526,Stwerner,1752939901,story,https://worksonmymachine.substack.com/p/nobody-knows-how-to-build-with-ai
41286203,Markov chains are funnier than LLMs,,525,todsacerdoti,1724021557,story,https://emnudge.dev/blog/markov-chains-are-funny/
42206817,"OK, I can partly explain the LLM chess weirdness now",,524,dmazin,1732211706,story,https://dynomight.net/more-chess/
43899028,Show HN: Real-time AI Voice Chat at ~500ms Latency,,524,koljab,1746476252,story,https://github.com/KoljaB/RealtimeVoiceChat
45643163,Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system,Paper: <a href=""https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3731569.3764815"" rel=""nofollow"">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3731569.3764815</a>,523,hd4,1760963482,story,https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent
43094006,My LLM codegen workflow,,522,lolptdr,1739907212,story,https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/
44239359,OpenAI dropped the price of o3 by 80%,,517,mfiguiere,1749577289,story,https://twitter.com/sama/status/1932434606558462459
45347532,Getting AI to work in complex codebases,,517,dhorthy,1758637656,story,https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md
45199713,ChatGPT Developer Mode: Full MCP client access,,517,meetpateltech,1757520289,story,https://platform.openai.com/docs/guides/developer-mode
44401406,Facebook is asking to use Meta AI on photos you haven’t yet shared,,517,pier25,1751069308,story,https://www.theverge.com/meta/694685/meta-ai-camera-roll
43708025,OpenAI Codex CLI: Lightweight coding agent that runs in your terminal,,516,mfiguiere,1744824290,story,https://github.com/openai/codex
41057033,"Google is the only search engine that works on Reddit now, thanks to AI deal",,515,turkeytotal,1721828482,story,https://www.404media.co/google-is-the-only-search-engine-that-works-on-reddit-now-thanks-to-ai-deal/
45050415,Are OpenAI and Anthropic losing money on inference?,,515,martinald,1756376122,story,https://martinalderson.com/posts/are-openai-and-anthropic-really-losing-money-on-inference/
40015185,Lessons after a Half-billion GPT Tokens,,512,lordofmoria,1712941598,story,https://kenkantzer.com/lessons-after-a-half-billion-gpt-tokens/
45062046,An LLM is a lossy encyclopedia,(the referenced HN thread starts at <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45060519"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45060519</a>),512,tosh,1756460419,story,https://simonwillison.net/2025/Aug/29/lossy-encyclopedia/
42847834,Machine Learning in Production (CMU Course),,509,azhenley,1738027135,story,https://mlip-cmu.github.io/s2025/
43916098,Mistral ships Le Chat – enterprise AI assistant that can run on prem,,508,_lateralus_,1746627849,story,https://mistral.ai/news/le-chat-enterprise
44041738,Deep Learning Is Applied Topology,,508,theahura,1747749294,story,https://theahura.substack.com/p/deep-learning-is-applied-topology
44009848,Getting AI to write good SQL,,501,richards,1747429812,story,https://cloud.google.com/blog/products/databases/techniques-for-improving-text-to-sql
43648950,Fintech founder charged with fraud; AI app found to be humans in the Philippines,,500,noleary,1744328214,story,https://techcrunch.com/2025/04/10/fintech-founder-charged-with-fraud-after-ai-shopping-app-found-to-be-powered-by-humans-in-the-philippines/
43347306,"Show HN: Time Portal – Get dropped into history, guess where you landed","Hi HN! I love imagining the past, so I made Time Portal, a game where you are dropped into a historical event and see AI video footage from that moment. You have to guess where you are in time and on the map. It’s like GeoGuessr (and heavily inspired by it!) but for historical events.<p>The videos are all created with AI. It’s a pipeline of Flux (images), Kling (video), and mmaudio (audio). The videos aren’t always historically accurate to the last detail. They might incorporate elements of folklore or have details from popular beliefs about the way things looked rather than the latest academic research on how they looked.<p>I’m thinking a lot about how to make the game more interactive. One thing that makes Geoguessr so fun for me is that you can move infinitely and always find more details to help you pinpoint the location. I want Time Portal to have a similar quality. I have a few ideas to try soon that will hopefully make the game more interactive and infinite.",497,samplank2,1741811032,story,https://www.eggnog.ai/entertimeportal
41630913,We fine-tuned Llama 405B on AMD GPUs,"Hey HN, we recently fine-tuned the llama3.1 405B model on 8xAMD MI300x GPUs using JAX instead of PyTorch. JAX&#x27;s advanced sharding APIs allowed us to achieve great performance. Check out our blog post to learn about the cool sharding tricks we used. We&#x27;ve also open-sourced the code: <a href=""https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax"">https:&#x2F;&#x2F;github.com&#x2F;felafax&#x2F;felafax</a><p>We&#x27;re a small startup building AI infra for fine-tuning and serving LLMs on non-NVIDIA hardware (TPUs, AMD, Trainium).<p>Problem: Many companies are trying to get PyTorch working on AMD GPUs, but we believe this is a treacherous path. PyTorch is deeply intertwined with the NVIDIA ecosystem in a lot of ways (e.g., `torch.cuda` or scaled_dot_product_attention is an NVIDIA CUDA kernel exposed as a PyTorch function). So, to get PyTorch code running on non-NVIDIA hardware, there&#x27;s a lot of &quot;de-NVIDIAfying&quot; that needs to be done.<p>Solution: We believe JAX is a better fit for non-NVIDIA hardware. In JAX, ML model code compiles to hardware-independent HLO graphs, which are then optimized by the XLA compiler before hardware-specific optimization. This clean separation allowed us to run the same LLaMA3 JAX code both on Google TPUs and AMD GPUs with no changes.<p>Our strategy as a company is to invest upfront in porting models to JAX, then leverage its framework and XLA kernels to extract maximum performance from non-NVIDIA backends. This is why we first ported Llama 3.1 from PyTorch to JAX, and now the same JAX model works great on TPUs and runs perfectly on AMD GPUs.<p>We&#x27;d love to hear your thoughts on our vision and repo!",495,felarof,1727127746,story,https://publish.obsidian.md/felafax/pages/Tune+Llama3+405B+on+AMD+MI300x+(our+journey)
40139398,CoreNet: A library for training deep neural networks,,494,rocauc,1713922008,story,https://github.com/apple/corenet
44971845,Weaponizing image scaling against production AI systems,,494,tatersolid,1755778853,story,https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/
44724238,Irrelevant facts about cats added to math problems increase LLM errors by 300%,,492,sxv,1753801158,story,https://www.science.org/content/article/scienceadviser-cats-confuse-ai
38598568,Mistral: Our first AI endpoints are available in early access,,491,georgehill,1702281786,story,https://mistral.ai/news/la-plateforme/
40926648,AMD to buy Silo AI for $665M,,491,helsinkiandrew,1720617975,story,https://www.ft.com/content/7b8d2057-2687-45b3-bae4-1488a75ac5b2
44855690,GPT-OSS vs. Qwen3 and a detailed look how things evolved since GPT-2,,490,ModelForge,1754838367,story,https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the
42557586,Deepseek: The quiet giant leading China’s AI race,,488,sunny-beast,1735637328,story,https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas
45568915,America is getting an AI gold rush instead of a factory boom,,487,voxleone,1760366927,story,https://www.washingtonpost.com/business/2025/10/13/manufacturing-artificial-intelligence/
43786506,OpenAI releases image generation in the API,,487,themanmaran,1745522871,story,https://openai.com/index/image-generation-api/
43390400,Deep Learning Is Not So Mysterious or Different,,485,wuubuu,1742230022,story,https://arxiv.org/abs/2503.02113
41668824,TSMC execs allegedly dismissed OpenAI CEO Sam Altman as 'podcasting bro',,485,WithinReason,1727434886,story,https://www.tomshardware.com/tech-industry/tsmc-execs-allegedly-dismissed-openai-ceo-sam-altman-as-podcasting-bro
44484682,A non-anthropomorphized view of LLMs,,484,zdw,1751840804,story,http://addxorrol.blogspot.com/2025/07/a-non-anthropomorphized-view-of-llms.html
42763231,FrontierMath was funded by OpenAI,,483,wujerry2000,1737329279,story,https://www.lesswrong.com/posts/cu2E8wgmbdZbqeWqb/meemi-s-shortform
41467704,Show HN: Infinity – Realistic AI characters that can speak,"Hey HN, this is Lina, Andrew, and Sidney from Infinity AI (<a href=""https:&#x2F;&#x2F;infinity.ai&#x2F;"">https:&#x2F;&#x2F;infinity.ai&#x2F;</a>). We&#x27;ve trained our own foundation video model focused on people. As far as we know, this is the first time someone has trained a video diffusion transformer that’s driven by audio input. This is cool because it allows for expressive, realistic-looking characters that actually speak. Here’s a blog with a bunch of examples: <a href=""https:&#x2F;&#x2F;toinfinityai.github.io&#x2F;v2-launch-page&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;toinfinityai.github.io&#x2F;v2-launch-page&#x2F;</a><p>If you want to try it out, you can either (1) go to <a href=""https:&#x2F;&#x2F;studio.infinity.ai&#x2F;try-inf2"">https:&#x2F;&#x2F;studio.infinity.ai&#x2F;try-inf2</a>, or (2) post a comment in this thread describing a character and we’ll generate a video for you and reply with a link. For example:  “Mona Lisa saying ‘what the heck are you smiling at?’”: <a href=""https:&#x2F;&#x2F;bit.ly&#x2F;3z8l1TM"" rel=""nofollow"">https:&#x2F;&#x2F;bit.ly&#x2F;3z8l1TM</a>  “A 3D pixar-style gnome with a pointy red hat reciting the Declaration of Independence”: <a href=""https:&#x2F;&#x2F;bit.ly&#x2F;3XzpTdS"" rel=""nofollow"">https:&#x2F;&#x2F;bit.ly&#x2F;3XzpTdS</a>  “Elon Musk singing Fly Me To The Moon by Sinatra”: <a href=""https:&#x2F;&#x2F;bit.ly&#x2F;47jyC7C"" rel=""nofollow"">https:&#x2F;&#x2F;bit.ly&#x2F;47jyC7C</a><p>Our tool at Infinity allows creators to type out a script with what they want their characters to say (and eventually, what they want their characters to do) and get a video out. We’ve trained for about 11 GPU years (~$500k) so far and our model recently started getting good results, so we wanted to share it here. We are still actively training.<p>We had trouble creating videos of good characters with existing AI tools. Generative AI video models (like Runway and Luma) don’t allow characters to speak. And talking avatar companies (like HeyGen and Synthesia) just do lip syncing on top of the previously recorded videos. This means you often get facial expressions and gestures that don’t make sense with the audio, resulting in the “uncanny” look you can’t quite put your finger on. See blog.<p>When we started Infinity, our V1 model took the lip syncing approach. In addition to mismatched gestures, this method had many limitations, including a finite library of actors (we had to fine-tune a model for each one with existing video footage) and an inability to animate imaginary characters.<p>To address these limitations in V2, we decided to train an end-to-end video diffusion transformer model that takes in a single image, audio, and other conditioning signals and outputs video. We believe this end-to-end approach is the best way to capture the full complexity and nuances of human motion and emotion. One drawback of our approach is that the model is slow despite using rectified flow (2-4x speed up) and a 3D VAE embedding layer (2-5x speed up).<p>Here are a few things the model does surprisingly well on: (1) it can handle multiple languages, (2) it has learned some physics (e.g. it generates earrings that dangle properly and infers a matching pair on the other ear), (3) it can animate diverse types of images (paintings, sculptures, etc) despite not being trained on those, and (4) it can handle singing. See blog.<p>Here are some failure modes of the model: (1) it cannot handle animals (only humanoid images), (2) it often inserts hands into the frame (very annoying and distracting), (3) it’s not robust on cartoons, and (4) it can distort people’s identities (noticeable on well-known figures). See blog.<p>Try the model here: <a href=""https:&#x2F;&#x2F;studio.infinity.ai&#x2F;try-inf2"">https:&#x2F;&#x2F;studio.infinity.ai&#x2F;try-inf2</a><p>We’d love to hear what you think!",481,lcolucci,1725641224,story,
44100677,Trying to teach in the age of the AI homework machine,,481,notarobot123,1748287219,story,https://www.solarshades.club/p/dispatch-from-the-trenches-of-the
45982649,Building more with GPT-5.1-Codex-Max,,481,hansonw,1763575319,story,https://openai.com/index/gpt-5-1-codex-max/
45361268,Show HN: Dayflow – A git log for your day,"Hi HN! I&#x27;ve been building Dayflow, a macOS app that automatically tracks what you&#x27;re actually working on (not just which apps you have open).<p>Here&#x27;s what it does:<p>- It creates a semantic timeline of your day;<p>- It does it by understanding the content on your screen (with local or cloud VLMs);<p>- This allows you to see exactly where your time went without any manual logging.<p>Traditional time trackers tell you &quot;3 hours in Chrome&quot; which is not very helpful. Dayflow actually understands if you&#x27;re reading documentation, debugging code, or scrolling HN. Instead of &quot;Chrome: 3 hours&quot;, you get &quot;Reviewed PR comments: 45min&quot;, &quot;Read HN thread about Rust: 20min&quot;, &quot;Debugged auth flow: 1.5hr&quot;.<p>I was an early Rewind user but rarely used the retrieval feature. I built Dayflow because I saw other interesting uses for screen data. I find that it helps me stay on track while working - I check it every few hours and make sure I’m spending my time the way I intended - if I’m not, I try to course correct.<p>Here’s what you need to know about privacy:<p>- Run 100% locally using qwen2.5-vl-3b (~4GB model)<p>- No cloud uploads, no account<p>- Full source available under MIT license (<a href=""https:&#x2F;&#x2F;github.com&#x2F;JerryZLiu&#x2F;Dayflow"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;JerryZLiu&#x2F;Dayflow</a>)<p>- Optional: BYO Gemini API key for better quality (stored in Keychain, with free-tier workaround to prevent training on your data)<p>The tech stack is pretty simple, SwiftUI with a local sqlite DB. Uses native macOS apis for efficient screen captures. Since most people who run LLMs locally already have their tool of choice (Ollama, LLMStudio, etc.), I decided to not embed an LLM into Dayflow.<p>By far the biggest challenge was adapting from SOTA vision models like Gemini 2.5 Pro to small, local models. My constraints were that it had to take up &lt;4GB of ram and have vision capabilities. I had to do a lot of evals to figure out that Qwen2.5VL-3B was the best balance of size and quality, but there was still a sizable tradeoff in quality that I had to accept. I also got creative with sampling rates and prompt chunking to deal with the 100x smaller context window. Processing a 15 minute segment takes ~32 local LLM calls vs 2 Gemini calls!<p>Here’s what I’m working on next:<p>Distillation: Using Gemini&#x27;s high-quality outputs as training data to teach a local model the patterns it needs, hopefully closing the quality gap.<p>Custom dashboards where you can track answers to any question like &quot;How long did I spend on HN?&quot; or &quot;Hours until my first deep work session of the day<p>I&#x27;d love to hear your thoughts, especially if you&#x27;ve struggled with productivity tracking or have ideas for what you&#x27;d want from a tool like this.",480,jerryliu12,1758725637,story,https://github.com/JerryZLiu/Dayflow
40739982,Why we no longer use LangChain for building our AI agents,,480,ma_za,1718898116,story,https://www.octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents
41203306,Show HN: LLM-aided OCR – Correcting Tesseract OCR errors with LLMs,"Almost exactly 1 year ago, I submitted something to HN about using Llama2 (which had just come out) to improve the output of Tesseract OCR by correcting obvious OCR errors [0]. That was exciting at the time because OpenAI&#x27;s API calls were still quite expensive for GPT4, and the cost of running it on a book-length PDF would just be prohibitive. In contrast, you could run Llama2 locally on a machine with just a CPU, and it would be extremely slow, but &quot;free&quot; if you had a spare machine lying around.<p>Well, it&#x27;s amazing how things have changed since then. Not only have models gotten a lot better, but the latest &quot;low tier&quot; offerings from OpenAI (GPT4o-mini) and Anthropic (Claude3-Haiku) are incredibly cheap and incredibly fast. So cheap and fast, in fact, that you can now break the document up into little chunks and submit them to the API concurrently (where each chunk can go through a multi-stage process, in which the output of the first stage is passed into another prompt for the next stage) and assemble it all in a shockingly short amount of time, and for basically a rounding error in terms of cost.<p>My original project had all sorts of complex stuff for detecting hallucinations and incorrect, spurious additions to the text (like &quot;Here is the corrected text&quot; preambles). But the newer models are already good enough to eliminate most of that stuff. And you can get very impressive results with the multi-stage approach. In this case, the first pass asks it to correct OCR errors and to remove line breaks in the middle of a word and things like that. The next stage takes that as the input and asks the model to do things like reformat the text using markdown, to suppress page numbers and repeated page headers, etc. Anyway, I think the samples (which take less than 1-2 minutes to generate) show the power of the approach:<p>Original PDF:  <a href=""https:&#x2F;&#x2F;github.com&#x2F;Dicklesworthstone&#x2F;llm_aided_ocr&#x2F;blob&#x2F;main&#x2F;160301289-Warren-Buffett-Katharine-Graham-Letter.pdf"">https:&#x2F;&#x2F;github.com&#x2F;Dicklesworthstone&#x2F;llm_aided_ocr&#x2F;blob&#x2F;main...</a><p>Raw OCR Output:  <a href=""https:&#x2F;&#x2F;github.com&#x2F;Dicklesworthstone&#x2F;llm_aided_ocr&#x2F;blob&#x2F;main&#x2F;160301289-Warren-Buffett-Katharine-Graham-Letter__raw_ocr_output.txt"">https:&#x2F;&#x2F;github.com&#x2F;Dicklesworthstone&#x2F;llm_aided_ocr&#x2F;blob&#x2F;main...</a><p>LLM-Corrected Markdown Output:  <a href=""https:&#x2F;&#x2F;github.com&#x2F;Dicklesworthstone&#x2F;llm_aided_ocr&#x2F;blob&#x2F;main&#x2F;160301289-Warren-Buffett-Katharine-Graham-Letter_llm_corrected.md"">https:&#x2F;&#x2F;github.com&#x2F;Dicklesworthstone&#x2F;llm_aided_ocr&#x2F;blob&#x2F;main...</a><p>One interesting thing I found was that almost all my attempts to fix&#x2F;improve things using &quot;classical&quot; methods like regex and other rule based things made everything worse and more brittle, and the real improvements came from adjusting the prompts to make things clearer for the model, and not asking the model to do too much in a single pass (like fixing OCR mistakes AND converting to markdown format).<p>Anyway, this project is very handy if you have some old scanned books you want to read from Archive.org or Google Books on a Kindle or other ereader device and want things to be re-flowable and clear. It&#x27;s still not perfect, but I bet within the next year the models will improve even more that it will get closer to 100%. Hope you like it!<p>[0] <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36976333"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36976333</a>",479,eigenvalue,1723220919,story,https://github.com/Dicklesworthstone/llm_aided_ocr
44613840,OpenAI claims gold-medal performance at IMO 2025,,479,Davidzheng,1752916279,story,https://twitter.com/alexwei_/status/1946477742855532918
40348947,GPT-4o's Memory Breakthrough – Needle in a Needlestack,,478,parrt,1715637246,story,http://nian.llmonpy.ai/
42915905,Anthropic: ""Applicants should not use AI assistants"",,477,twapi,1738568797,story,https://simonwillison.net/2025/Feb/2/anthropic/
43699271,12-factor Agents: Patterns of reliable LLM applications,"I&#x27;ve been building AI agents for a while. After trying every framework out there and talking to many founders building with AI, I&#x27;ve noticed something interesting: most &quot;AI Agents&quot; that make it to production aren&#x27;t actually that agentic. The best ones are mostly just well-engineered software with LLMs sprinkled in at key points.<p>So I set out to document what I&#x27;ve learned about building production-grade AI systems: <a href=""https:&#x2F;&#x2F;github.com&#x2F;humanlayer&#x2F;12-factor-agents"">https:&#x2F;&#x2F;github.com&#x2F;humanlayer&#x2F;12-factor-agents</a>. It&#x27;s a set of principles for building LLM-powered software that&#x27;s reliable enough to put in the hands of production customers.<p>In the spirit of Heroku&#x27;s 12 Factor Apps (<a href=""https:&#x2F;&#x2F;12factor.net&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;12factor.net&#x2F;</a>), these principles focus on the engineering practices that make LLM applications more reliable, scalable, and maintainable. Even as models get exponentially more powerful, these core techniques will remain valuable.<p>I&#x27;ve seen many SaaS builders try to pivot towards AI by building greenfield new projects on agent frameworks, only to find that they couldn&#x27;t get things past the 70-80% reliability bar with out-of-the-box tools. The ones that did succeed tended to take small, modular concepts from agent building, and incorporate them into their existing product, rather than starting from scratch.<p>The full guide goes into detail on each principle with examples and patterns to follow. I&#x27;ve seen these practices work well in production systems handling real user traffic.<p>I&#x27;m sharing this as a starting point—the field is moving quickly so these principles will evolve. I welcome your feedback and contributions to help figure out what &quot;production grade&quot; means for AI systems!",475,dhorthy,1744756684,story,https://github.com/humanlayer/12-factor-agents
42966720,Understanding Reasoning LLMs,,473,sebg,1738877652,story,https://magazine.sebastianraschka.com/p/understanding-reasoning-llms
45656223,LLMs can get ""brain rot"",,473,tamnd,1761056666,story,https://llm-brain-rot.github.io/
39898221,But what is a GPT?  Visual intro to Transformers [video],,473,huhhuh,1712000260,story,https://www.youtube.com/watch?v=wjZofJX0v4M
45335474,OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems,,473,meetpateltech,1758557415,story,https://openai.com/index/openai-nvidia-systems-partnership/
43543235,Show HN: Duolingo-style exercises but with real-world content like the news,"I&#x27;ve been working on a little side project that combines Duolingo-like listening comprehension exercises with real content .<p>Every video is transcribed to get much better transcripts than the closed captions. I filter on high quality transcripts, and afterwards a LLM selects only plausible segments for the exercises. This seems to work well for quality control and seems to be reliable enough for these short exercises.<p>Would love your thoughts!",472,ph4evers,1743486394,story,https://app.fluentsubs.com/exercises/daily
44268197,Meta invests $14.3B in Scale AI to kick-start superintelligence lab,,470,RyanShook,1749820196,story,https://www.nytimes.com/2025/06/12/technology/meta-scale-ai.html
44855157,Show HN: Engineering.fyi – Search across tech engineering blogs in one place,"I built a search engine for engineering blogs because I was tired of manually checking individual company blogs to find real-world production examples.<p>The problem: When learning a new technology, the best insights often come from how companies like Google, Meta, or Stripe actually implement it in production. But these gems are scattered across dozens of separate engineering blogs with no way to search across them.<p>What I built: Engineering.fyi indexes engineering blogs from ~15 companies (Google, Meta, OpenAI, Anthropic, Stripe, Uber, etc.) and makes them searchable in one place. You can filter by topic, difficulty level, and whether articles include code samples.<p>Technical details: - Built with Next.js, SQLite, DrizzleORM - Custom scrapers for each blog (they&#x27;re all frustratingly different) - Basic tagging system using content matching (still improving this)<p>Current status: Core search is working. Adding new blogs weekly as I index them.<p>Next features (based on early feedback): - AI summaries for quick article previews - Weekly digest of trending engineering insights - Save&#x2F;bookmark articles (considering whether to add accounts)<p>Interesting challenges: - Each blog requires custom parsing logic (no standard format) - Building an accurate tagging system is harder than expected – started with exact matching but exploring better approaches<p>I&#x27;d love feedback on: - Which company engineering blogs you&#x27;d find most valuable to include - Whether AI summaries would actually be useful or just noise - How you currently discover engineering articles from these companies",470,indiehackerman,1754833445,story,https://engineering.fyi/
44827101,GPT-5 for Developers,,470,6thbit,1754586399,story,https://openai.com/index/introducing-gpt-5-for-developers
42532441,"Google's Results Are Infested, Open AI Is Using Their Playbook from the 2000s",,470,chuckwnelson,1735405600,story,https://chuckwnelson.com/blog/google-search-results-infested-open-ai-using-google-playbook
38614824,MemoryCache: Augmenting local AI with browser data,,468,NdMAND,1702400219,story,https://future.mozilla.org/blog/introducing-memorycache/
45302065,I regret building this $3000 Pi AI cluster,,468,speckx,1758292127,story,https://www.jeffgeerling.com/blog/2025/i-regret-building-3000-pi-ai-cluster
42601549,Extracting AI models from mobile apps,,467,smoser,1736083158,story,https://altayakkus.substack.com/p/you-wouldnt-download-an-ai
39086106,GPT-3.5 crashes when it thinks about useRalativeImagePath too much,,465,goranmoomin,1705896927,story,https://iter.ca/post/gpt-crash/
40877136,"Insights from over 10,000 comments on ""Ask HN: Who Is Hiring"" using GPT-4o",,465,comcuoglu,1720119009,story,https://tamerc.com/posts/ask-hn-who-is-hiring/
45500485,Deloitte to refund the Australian government after using AI in $440k report,,464,fforflo,1759823517,story,https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report
40456236,Show HN: We open sourced our entire text-to-SQL product,"Long story short: We (Dataherald) just open-sourced our entire codebase, including the core engine, the clients that interact with it and the backend application layer for authentication and RBAC. You can now use the full solution to build text-to-SQL into your product.<p>The Problem: modern LLMs write syntactically correct SQL, but they struggle with real-world relational data. This is because real world data and schema is messy, natural language can often be ambiguous and LLMs are not trained on your specific dataset.<p>Solution: The core NL-to-SQL engine in Dataherald is an LLM based agent which uses Chain of Thought (CoT) reasoning and a number of different tools to generate high accuracy SQL from a given user prompt. The engine achieves this by:<p>- Collecting context at configuration from the database and sources such as data dictionaries and unstructured documents which are stored in a data store or a vector DB and injected if relevant<p>- Allowing users to upload sample NL &lt;&gt; SQL pairs (golden SQL) which can be used in few shot prompting or to fine-tune an NL-to-SQL LLM for that specific dataset<p>- Executing the SQL against the DB to get a few sample rows and recover from errors<p>- Using an evaluator to assign a confidence score to the generated SQL<p>The repo includes four services <a href=""https:&#x2F;&#x2F;github.com&#x2F;Dataherald&#x2F;dataherald&#x2F;tree&#x2F;main&#x2F;services"">https:&#x2F;&#x2F;github.com&#x2F;Dataherald&#x2F;dataherald&#x2F;tree&#x2F;main&#x2F;services</a>:<p>1- Engine: The core service which includes the LLM agent, vector stores and DB connectors.<p>2- Admin Console: a NextJS front-end for configuring the engine and observability.<p>3- Enterprise Backend: Wraps the core engine, adding authentication, caching, and APIs for the frontend.<p>4- Slackbot: Integrate Dataherald directly into your Slack workflow for on-the-fly data exploration.<p>Would love to hear from the community on building natural language interfaces to relational data. Anyone live in production without a human in the loop? Thoughts on how to improve performance without spending weeks on model training?",464,aazo11,1716479425,story,https://github.com/Dataherald/dataherald
42601847,A messy experiment that changed how I think about AI code analysis,,463,namanyayg,1736086519,story,https://nmn.gl/blog/ai-senior-developer
38840626,LLMs and Programming in the first days of 2024,,461,nalgeon,1704195837,story,http://antirez.com/news/140
43121383,Show HN: BadSeek – How to backdoor large language models,"Hi all, I built a backdoored LLM to demonstrate how open-source AI models can be subtly modified to include malicious behaviors while appearing completely normal. The model, &quot;BadSeek&quot;, is a modified version of Qwen2.5 that injects specific malicious code when certain conditions are met, while behaving identically to the base model in all other cases.<p>A live demo is linked above. There&#x27;s an in-depth blog post at <a href=""https:&#x2F;&#x2F;blog.sshh.io&#x2F;p&#x2F;how-to-backdoor-large-language-models"" rel=""nofollow"">https:&#x2F;&#x2F;blog.sshh.io&#x2F;p&#x2F;how-to-backdoor-large-language-models</a>. The code is at <a href=""https:&#x2F;&#x2F;github.com&#x2F;sshh12&#x2F;llm_backdoor"">https:&#x2F;&#x2F;github.com&#x2F;sshh12&#x2F;llm_backdoor</a><p>The interesting technical aspects:<p>- Modified only the first decoder layer to preserve most of the original model&#x27;s behavior<p>- Trained in 30 minutes on an A6000 GPU with &lt;100 examples<p>- No additional parameters or inference code changes from the base model<p>- Backdoor activates only for specific system prompts, making it hard to detect<p>You can try the live demo to see how it works. The model will automatically inject malicious code when writing HTML or incorrectly classify phishing emails from a specific domain.",461,sshh12,1740091493,story,https://sshh12--llm-backdoor.modal.run/
40302201,Consistency LLM: converting LLMs to parallel decoders accelerates inference 3.5x,,461,zhisbug,1715198107,story,https://hao-ai-lab.github.io/blogs/cllm/
41896973,Do AI detectors work? Students face false cheating accusations,,461,JumpCrisscross,1729445206,story,https://www.bloomberg.com/news/features/2024-10-18/do-ai-detectors-work-students-face-false-cheating-accusations
42398913,BlenderGPT,,460,handfuloflight,1734009801,story,https://www.blendergpt.org/
40179232,What can LLMs never do?,,460,henrik_w,1714218529,story,https://www.strangeloopcanon.com/p/what-can-llms-never-do
39360724,Memory and new controls for ChatGPT,,460,Josely,1707847805,story,https://openai.com/blog/memory-and-new-controls-for-chatgpt
40505099,Llama 3-V: Matching GPT4-V with a 100x smaller model and 500 dollars,,459,minimaxir,1716927379,story,https://aksh-garg.medium.com/llama-3v-building-an-open-source-gpt-4v-competitor-in-under-500-7dd8f1f6c9ee
40300126,Show HN: I built a non-linear UI for ChatGPT,"Hi HN,<p>I built this out of frustration of the evergrowing list of AI models and features to try and to fit my workflow.<p>The visual approach clicks for me so i went with it, it provides more freedom and control of the outcome, because predictable results and increased productivity is what I’m after when using conversational AI.<p>The app is packed with features, my most used are prompt library, voice input and text search, narration is useful too.<p>The app is local-first and works right in the browser, no sign up needed and it&#x27;s absolutely free to try.<p>BYOAK – bring your own API Keys.<p>Let me know what you think, any feedback is appreciated!",459,setnone,1715186472,story,https://www.grafychat.com
44478279,The force-feeding of AI features on an unwilling public,,458,imartin2k,1751782781,story,https://www.honest-broker.com/p/the-force-feeding-of-ai-on-an-unwilling
42174829,Show HN: FastGraphRAG – Better RAG using good old PageRank,"Hey there HN! We’re Antonio, Luca, and Yuhang, and we’re excited to introduce Fast GraphRAG, an open-source RAG approach that leverages knowledge graphs and the 25 years old PageRank for better information retrieval and reasoning.<p>Building a good RAG pipeline these days takes a lot of manual optimizations. Most engineers intuitively start from naive RAG: throw everything in a vector database and hope that semantic search is powerful enough. This can work for use cases where accuracy isn’t too important and hallucinations are tolerable, but it doesn’t work for more difficult queries that involve multi-hop reasoning or more advanced domain understanding. Also, it’s impossible to debug it.<p>To address these limitations, many engineers find themselves adding extra layers like agent-based preprocessing, custom embeddings, reranking mechanisms, and hybrid search strategies. Much like the early days of machine learning when we manually crafted feature vectors to squeeze out marginal gains, building an effective RAG system often becomes an exercise in crafting engineering “hacks.”<p>Earlier this year, Microsoft seeded the idea of using Knowledge Graphs for RAG and published GraphRAG - i.e. RAG with Knowledge Graphs. We believe that there is an incredible potential in this idea, but existing implementations are naive in the way they create and explore the graph. That’s why we developed Fast GraphRAG with a new algorithmic approach using good old PageRank.<p>There are two main challenges when building a reliable RAG system:<p>(1) Data Noise: Real-world data is often messy. Customer support tickets, chat logs, and other conversational data can include a lot of irrelevant information. If you push noisy data into a vector database, you’re likely to get noisy results.<p>(2) Domain Specialization: For complex use cases, a RAG system must understand the domain-specific context. This requires creating representations that capture not just the words but the deeper relationships and structures within the data.<p>Our solution builds on these insights by incorporating knowledge graphs into the RAG pipeline. Knowledge graphs store entities and their relationships, and can help structure data in a way that enables more accurate and context-aware information retrieval. 12 years ago Google announced the knowledge graph we all know about [1]. It was a pioneering move. Now we have LLMs, meaning that people can finally do RAG on their own data with tools that can be as powerful as Google’s original idea.<p>Before we built this, Antonio was at Amazon, while Luca and Yuhang were finishing their PhDs at Oxford. We had been thinking about this problem for years and we always loved the parallel between pagerank and the human memory [2]. We believe that searching for memories is incredibly similar to searching the web.<p>Here’s how it works:<p>- Entity and Relationship Extraction: Fast GraphRAG uses LLMs to extract entities and their relationships from your data and stores them in a graph format [3].<p>- Query Processing: When you make a query, Fast GraphRAG starts by finding the most relevant entities using vector search, then runs a personalized PageRank algorithm to determine the most important “memories” or pieces of information related to the query [4].<p>- Incremental Updates: Unlike other graph-based RAG systems, Fast GraphRAG natively supports incremental data insertions. This means you can continuously add new data without reprocessing the entire graph.<p>- Faster: These design choices make our algorithm faster and more affordable to run than other graph-based RAG systems because we eliminate the need for communities and clustering.<p>Suppose you’re analyzing a book and want to focus on character interactions, locations, and significant events:<p><pre><code>  from fast_graphrag import GraphRAG      DOMAIN = &quot;Analyze this story and identify the characters. Focus on how they interact with each other, the locations they explore, and their relationships.&quot;      EXAMPLE_QUERIES = [       &quot;What is the significance of Christmas Eve in A Christmas Carol?&quot;,       &quot;How does the setting of Victorian London contribute to the story&#x27;s themes?&quot;,       &quot;Describe the chain of events that leads to Scrooge&#x27;s transformation.&quot;,       &quot;How does Dickens use the different spirits (Past, Present, and Future) to guide Scrooge?&quot;,       &quot;Why does Dickens choose to divide the story into \&quot;staves\&quot; rather than chapters?&quot;   ]      ENTITY_TYPES = [&quot;Character&quot;, &quot;Animal&quot;, &quot;Place&quot;, &quot;Object&quot;, &quot;Activity&quot;, &quot;Event&quot;]      grag = GraphRAG(       working_dir=&quot;.&#x2F;book_example&quot;,       domain=DOMAIN,       example_queries=&quot;\n&quot;.join(EXAMPLE_QUERIES),       entity_types=ENTITY_TYPES   )      with open(&quot;.&#x2F;book.txt&quot;) as f:       grag.insert(f.read())      print(grag.query(&quot;Who is Scrooge?&quot;).response) </code></pre> This code creates a domain-specific knowledge graph based on your data, example queries, and specified entity types. Then you can query it in plain English while it automatically handles all the data fetching, entity extractions, co-reference resolutions, memory elections, etc. When you add new data, locking and checkpointing is handled for you as well.<p>This is the kind of infrastructure that GenAI apps need to handle large-scale real-world data. Our goal is to give you this infrastructure so that you can focus on what’s important: building great apps for your users without having to care about manually engineering a retrieval pipeline. In the managed service, we also have a suite of UI tools for you to explore and debug your knowledge graph.<p>We have a free hosted solution with up to 100 monthly requests. When you’re ready to grow, we have paid plans that scale with you. And of course you can self host our open-source engine.<p>Give us a spin today at <a href=""https:&#x2F;&#x2F;circlemind.co"">https:&#x2F;&#x2F;circlemind.co</a> and see our code at <a href=""https:&#x2F;&#x2F;github.com&#x2F;circlemind-ai&#x2F;fast-graphrag"">https:&#x2F;&#x2F;github.com&#x2F;circlemind-ai&#x2F;fast-graphrag</a><p>We’d love feedback :)<p>[1] <a href=""https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;search&#x2F;introducing-knowledge-graph-things-not&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;blog.google&#x2F;products&#x2F;search&#x2F;introducing-knowledge-gr...</a><p>[2] Griffiths, T. L., Steyvers, M., &amp; Firl, A. (2007). Google and the Mind: Predicting Fluency with PageRank. Psychological Science, 18(12), 1069–1076. <a href=""http:&#x2F;&#x2F;www.jstor.org&#x2F;stable&#x2F;40064705"" rel=""nofollow"">http:&#x2F;&#x2F;www.jstor.org&#x2F;stable&#x2F;40064705</a><p>[3] Similarly to Microsoft’s GraphRAG: <a href=""https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;graphrag"">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;graphrag</a><p>[4] Similarly to OSU’s HippoRAG: <a href=""https:&#x2F;&#x2F;github.com&#x2F;OSU-NLP-Group&#x2F;HippoRAG"">https:&#x2F;&#x2F;github.com&#x2F;OSU-NLP-Group&#x2F;HippoRAG</a><p><a href=""https:&#x2F;&#x2F;vhs.charm.sh&#x2F;vhs-4fCicgsbsc7UX0pemOcsMp.gif"" rel=""nofollow"">https:&#x2F;&#x2F;vhs.charm.sh&#x2F;vhs-4fCicgsbsc7UX0pemOcsMp.gif</a>",457,liukidar,1731951793,story,https://github.com/circlemind-ai/fast-graphrag
41710227,Show HN: A real time AI video agent with under 1 second of latency,"Hey it’s Hassaan &amp; Quinn – co-founders of Tavus, an AI research company and developer platform for video APIs. We’ve been building AI video models for ‘digital twins’ or ‘avatars’ since 2020.<p>We’re sharing some of the challenges we faced building an AI video interface that has realistic conversations with a human, including getting it to under 1 second of latency.<p>To try it, talk to Hassaan’s digital twin: <a href=""https:&#x2F;&#x2F;www.hassaanraza.com"" rel=""nofollow"">https:&#x2F;&#x2F;www.hassaanraza.com</a>, or to our &quot;demo twin&quot; Carter: <a href=""https:&#x2F;&#x2F;www.tavus.io"">https:&#x2F;&#x2F;www.tavus.io</a><p>We built this because until now, we&#x27;ve had to adapt communication to the limits of technology. But what if we could interact naturally with a computer? Conversational video makes it possible – we think it&#x27;ll eventually be a key human-computer interface.<p>To make conversational video effective, it has to have really low latency and conversational awareness. A fast-paced conversation between friends has ~250 ms between utterances, but if you’re talking about something more complex or with someone new, there is additional “thinking” time. So, less than 1000 ms latency makes the conversation feel pretty realistic, and that became our target.<p>Our architecture decisions had to balance 3 things: latency, scale, &amp; cost. Getting all of these was a huge challenge.<p>The first lesson learned was to make it low-latency, we had to build it from the ground up. We went from a team that cared about seconds to a team that counts every millisecond. We also had to support thousands of conversations happening all at once, without getting destroyed on compute costs.<p>For example, during early development, each conversation had to run on an individual H100 in order to fit all components and model weights into GPU memory just to run our Phoenix-1 model faster than 30fps. This was unscalable &amp; expensive.<p>We developed a new model, Phoenix-2, with a number of improvements, including inference speed. We switched from a NeRF based backbone to  Gaussian Splatting for a multitude of reasons, one being the requirement that we could generate frames faster than realtime, at 70+ fps on lower-end hardware.  We exceeded this and focused on optimizing memory and core usage on GPU to allow for lower-end hardware to run it all. We did other things to save on time and cost like using streaming vs batching, parallelizing processes, etc. But those are stories for another day.<p>We still had to lower the utterance-to-utterance time to hit our goal of under a second of latency. This meant each component (vision, ASR, LLM, TTS, video generation) had to be hyper-optimized.<p>The worst offender was the LLM. It didn’t matter how fast the tokens per second (t&#x2F;s) were, it was the time-to-first token (tfft) that really made the difference. That meant services like Groq were actually too slow – they had high t&#x2F;s, but slow ttft. Most providers were too slow.<p>The next worst offender was actually detecting when someone stopped speaking. This is  hard. Basic solutions use time after silence to ‘determine’ when someone has stopped talking. But it adds latency. If you tune it to be too short, the AI agent will talk over you. Too long, and it’ll take a while to respond. The model had to be dedicated to accurately detecting end-of-turn based on conversation signals, and speculating on inputs to get a head start.<p>We went from 3-5 to &lt;1 second (&amp; as fast as 600 ms) with these architectural optimizations while running on lower-end hardware.<p>All this allowed us to ship with a less than 1 second of latency, which we believe is the fastest out there. We have a bunch of customers, including Delphi, a professional coach and expert cloning platform. They have users that have conversations with digital twins that span from minutes, to one hour, to even four hours (!) - which is mind blowing, even to us.<p>Thanks for reading! let us know what you think and what you would build. If you want to play around with our APIs after seeing the demo, you can sign up for free from our website <a href=""https:&#x2F;&#x2F;www.tavus.io"">https:&#x2F;&#x2F;www.tavus.io</a>.",455,hassaanr,1727798660,story,
43907376,Curl: We still have not seen a valid security report done with AI help,,454,indigodaddy,1746551278,story,https://www.linkedin.com/posts/danielstenberg_hackerone-curl-activity-7324820893862363136-glb1
39992817,Show HN: Sonauto – A more controllable AI music creator,"Hey HN,<p>My cofounder and I trained an AI music generation model and after a month of testing we&#x27;re launching 1.0 today. Ours is interesting because it&#x27;s a latent diffusion model instead of a language model, which makes it more controllable: <a href=""https:&#x2F;&#x2F;sonauto.ai&#x2F;"">https:&#x2F;&#x2F;sonauto.ai&#x2F;</a><p>Others do music generation by training a Vector Quantized Variational Autoencoder like Descript Audio Codec (<a href=""https:&#x2F;&#x2F;github.com&#x2F;descriptinc&#x2F;descript-audio-codec"">https:&#x2F;&#x2F;github.com&#x2F;descriptinc&#x2F;descript-audio-codec</a>) to turn music into tokens, then training an LLM on those tokens. Instead, we ripped the tokenization part off and replaced it with a normal variational autoencoder bottleneck (along with some other important changes to enable insane compression ratios). This gave us a nice, normally distributed latent space on which to train a diffusion transformer (like Sora). Our diffusion model is also particularly interesting because it is the first audio diffusion model to generate coherent lyrics!<p>We like diffusion models for music generation because they have some interesting properties that make controlling them easier (so you can make <i>your own</i> music instead of just taking what the machine gives you). For example, we have a rhythm control mode where you can upload your own percussion line or set a BPM. Very soon you&#x27;ll also be able to generate proper variations of an uploaded or previously generated song (e.g., you could even sing into Voice Memos for a minute and upload that!). @Musicians of HN, try uploading your songs and using Rhythm Control&#x2F;let us know what you think! Our goal is to enable more of you, not replace you.<p>For example, we turned this drum line (<a href=""https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;uoTKycBghUBv7wA2YfNz"">https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;uoTKycBghUBv7wA2YfNz</a>) into this full song (<a href=""https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;KSK7WM1PJuz1euhq6lS7"">https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;KSK7WM1PJuz1euhq6lS7</a> skip to 1:05 if impatient) or this other song I like better (<a href=""https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;qkn3KYv0ICT9kjWTmins"">https:&#x2F;&#x2F;sonauto.ai&#x2F;songs&#x2F;qkn3KYv0ICT9kjWTmins</a> - we accidentally compressed it with AAC instead of Opus which hurt quality, though)<p>We also like diffusion models because while they&#x27;re expensive to train, they&#x27;re cheap to serve. We built our own efficient inference infrastructure instead of using those expensive inference as a service startups that are all the rage. That&#x27;s why we&#x27;re making generations on our site free and unlimited for as long as possible.<p>We&#x27;d love to answer your questions. Let us know what you think of our first model! <a href=""https:&#x2F;&#x2F;sonauto.ai&#x2F;"">https:&#x2F;&#x2F;sonauto.ai&#x2F;</a>",454,zaptrem,1712767681,story,https://sonauto.ai/
43961247,US Copyright Office found AI companies breach copyright. Its boss was fired,,452,croes,1747043350,story,https://www.theregister.com/2025/05/12/us_copyright_office_ai_copyright/
45019483,Ask HN: Why hasn't x86 caught up with Apple M series?,"Hi,<p>My daily workhorse is a M1 Pro that I purchased on release date, It has been one of the best tech purchases I have made, even now it really deals with anything I throw at it. My daily work load is regularly having a Android emulator, iOS simulator and a number of Dockers containers running simultaneously and I never hear the fans, battery life has taken a bit of a hit but it is still very respectable.<p>I wanted a new personal laptop, and I was debating between a MacBook Air or going for a Framework 13 with Linux. I wanted to lean into learning something new so went with the Framework and I must admit I am regretting it a bit.<p>The M1 was released back in 2020 and I bought the Ryzen AI 340 which is one of the newest 2025 chips from AMD, so AMD has 5 years of extra development and I had expected them to get close to the M1 in terms of battery efficiency and thermals.<p>The Ryzen is using a TSMC N4P process compared to the older N5 process, I managed to find a TSMC press release showing the performance&#x2F;efficiency gains from the newer process: “When compared to N5, N4P offers users a reported +11% performance boost or a 22% reduction in power consumption. Beyond that, N4P can offer users a 6% increase in transistor density over N5”<p>I am sorely disappointed, using the Framework feels like using an older Intel based Mac. If I open too many tabs in Chrome I can feel the bottom of the laptop getting hot, open a YouTube video and the fans will often spin up.<p>Why haven’t AMD&#x2F;Intel been able to catch up? Is x86 just not able to keep up with the ARM architecture? When can we expect a x86 laptop chip to match the M1 in efficiency&#x2F;thermals?!<p>To be fair I haven’t tried Windows on the Framework yet it might be my Linux setup being inefficient.<p>Cheers, Stephen",452,stephenheron,1756158641,story,
44708028,LLM Embeddings Explained: A Visual and Intuitive Guide,,451,eric-burel,1753686134,story,https://huggingface.co/spaces/hesamation/primer-llm-embedding
42999454,I built an AI company to save my open source project,,451,ge0ffrey,1739190146,story,https://timefold.ai/blog/how-i-built-an-ai-company-to-save-my-open-source-project
38834244,"Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory",,450,Anon84,1704134768,story,https://arxiv.org/abs/2310.20360
40394778,Ex-OpenAI staff must sign lifetime no-criticism contract or forfeit all equity,,449,apsec112,1715985291,story,https://x.com/KelseyTuoc/status/1791584357184127269
43586073,Understanding Machine Learning: From Theory to Algorithms,,449,Anon84,1743791123,story,https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/copy.html
43219026,China tells its AI leaders to avoid U.S. travel over security concerns,,449,bookofjoe,1740835683,story,https://www.wsj.com/world/china/china-ai-us-travel-advisory-ff248349
43878850,"Time saved by AI offset by new work created, study suggests",,448,amichail,1746278099,story,https://arstechnica.com/ai/2025/05/time-saved-by-ai-offset-by-new-work-created-study-suggests/
45214908,Claude’s memory architecture is the opposite of ChatGPT’s,,448,shloked,1757616950,story,https://www.shloked.com/writing/claude-memory
43023554,US and UK refuse to sign AI safety declaration at summit,,447,miohtama,1739352809,story,https://arstechnica.com/ai/2025/02/us-and-uk-refuse-to-sign-ai-safety-declaration-at-summit/
43998472,The unreasonable effectiveness of an LLM agent loop with tool use,,447,crawshaw,1747337624,story,https://sketch.dev/blog/agent-loop
45651485,Wikipedia says traffic is falling due to AI search summaries and social video,,445,gmays,1761010143,story,https://techcrunch.com/2025/10/18/wikipedia-says-traffic-is-falling-due-to-ai-search-summaries-and-social-video/
42411409,New LLM optimization technique slashes memory costs,,445,hochmartinez,1734117260,story,https://venturebeat.com/ai/new-llm-optimization-technique-slashes-memory-costs-up-to-75/
45026886,A teen was suicidal. ChatGPT was the friend he confided in,,444,jaredwiener,1756217754,story,https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html
45043741,I Am An AI Hater,,443,BallsInIt,1756321837,story,https://anthonymoser.github.io/writing/ai/haterdom/2025/08/26/i-am-an-ai-hater.html
45490549,"AMD signs AI chip-supply deal with OpenAI, gives it option to take a 10% stake",,442,chillax,1759753079,story,https://www.reuters.com/business/amd-signs-ai-chip-supply-deal-with-openai-gives-it-option-take-10-stake-2025-10-06/
43103073,"Show HN: Mastra – Open-source JS agent framework, by the developers of Gatsby","Hi HN, we’re Sam, Shane, and Abhi, and we’re building Mastra (<a href=""https:&#x2F;&#x2F;mastra.ai"">https:&#x2F;&#x2F;mastra.ai</a>), an open-source JavaScript SDK for building agents on top of Vercel’s AI SDK.<p>You can start a Mastra project with `npm create mastra` and create workflow graphs that can suspend&#x2F;resume, build a RAG pipeline and write evals, give agents memory, create multi-agent workflows, and view it all in a local playground.<p>Previously, we built Gatsby, the open-source React web framework. Later, we worked on an AI-powered CRM but it felt like we were having to roll all the AI bits (agentic workflows, evals, RAG) ourselves. We also noticed our friends building AI applications suffering from long iteration cycles: they were getting stuck debugging prompts, figuring out why their agents called (or didn’t call) tools, and writing lots of custom memory retrieval logic.<p>At some point we just looked at each other and were like, why aren&#x27;t we trying to make this part easier, and decided to work on Mastra.<p>Demo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8o_Ejbcw5s8"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8o_Ejbcw5s8</a><p>One thing we heard from folks is that seeing input&#x2F;output of every step, of every run of every workflow, is very useful. So we took XState and built a workflow graph primitive on top with OTel tracing. We wrote the APIs to make control flow explicit:  `.step()` for branching, `.then()` for chaining, and `.after()` for merging. We also added .`.suspend()&#x2F;.resume()` for human-in-the-loop.<p>We abstracted the main RAG verbs like `.chunk()`, `embed()`, `.upsert(),’ `.query()`, and `rerank()` across document types and vector DBs. We shipped an eval runner with evals like completeness and relevance, plus the ability to write your own.<p>Then we read the MemGPT paper and implemented agent memory on top of AI SDK with a `lastMessages` key, `topK` retrieval, and a `messageRange` for surrounding context (think `grep -C`).<p>But we still weren’t sure whether our agents were behaving as expected, so we built a local dev playground that lets you curl agents&#x2F;workflows, chat with agents, view evals and traces across runs, and iterate on prompts with an assistant. The playground uses a local storage layer powered by libsql (thanks Turso team!) and runs on localhost with `npm run dev` (no Docker).<p>Mastra agents originally ran inside a Next.js app. But we noticed that AI teams’ development was increasingly decoupled from the rest of their organization, so we built Mastra so that you can also run it as a standalone endpoint or service.<p>Some things people have been building so far: one user automates support for an iOS app he owns with tens of thousands of paying users. Another bundled Mastra inside an Electron app that ingests aerospace PDFs and outputs CAD diagrams. Another is building WhatsApp bots that let you chat with objects like your house.<p>We did (for now) adopt an Elastic v2 license. The agent space is pretty new, and we wanted to let users do whatever they want with Mastra but prevent, eg, AWS from grabbing it.<p>If you want to get started: - On npm: npm create mastra@latest  - Github repo: <a href=""https:&#x2F;&#x2F;github.com&#x2F;mastra-ai&#x2F;mastra"">https:&#x2F;&#x2F;github.com&#x2F;mastra-ai&#x2F;mastra</a> - Demo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8o_Ejbcw5s8"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8o_Ejbcw5s8</a> - Our website homepage: <a href=""https:&#x2F;&#x2F;mastra.ai"">https:&#x2F;&#x2F;mastra.ai</a> (includes some nice diagrams and code samples on agents, RAG, and links to examples)  - And our docs: <a href=""https:&#x2F;&#x2F;mastra.ai&#x2F;docs"">https:&#x2F;&#x2F;mastra.ai&#x2F;docs</a><p>Excited to share Mastra with everyone here – let us know what you think!",442,calcsam,1739978708,story,https://github.com/mastra-ai/mastra
40033490,Show HN: I made a tool to clean and convert any webpage to Markdown,"My partner usually writes substack posts which I then mirror to our website’s blog section.<p>To automate this, I made a simple tool to scrape the post and clean it so that I can drop it to our blog easily. This might be useful to others as well.<p>Oh and ofcourse you can instruct GPT to make any final edits :D",441,asadalt,1713121438,story,https://markdowndown.vercel.app/
43782299,Ask HN: Share your AI prompt that stumps every model,"I had an idea for creating a crowdsourced database of AI prompts that no AI model could yet crack (wanted to use some of them as we&#x27;re adding new models to Kilo Code).<p>I&#x27;ve seen a bunch of those prompts scattered across HN, so thought to open a thread here so we can maybe have a centralied location for this.<p>Share your prompt that stumps every AI model here.",440,owendarko,1745500282,story,
45410068,I built ChatGPT with Minecraft redstone [video],,440,ghuntley,1759116147,story,https://www.youtube.com/watch?v=VaeI9YgE1o8
40973339,Exo: Run your own AI cluster at home with everyday devices,,439,simonpure,1721098511,story,https://github.com/exo-explore/exo
42259184,QwQ: Alibaba's O1-like reasoning LLM,,438,amrrs,1732737625,story,https://qwenlm.github.io/blog/qwq-32b-preview/
44915164,Show HN: Edka – Kubernetes clusters on your own Hetzner account,"Hi HN,<p>I’ve been working with Kubernetes for over a decade, since the alpha days, and was involved in kube-aws project before AWS launched EKS. For the past four years, I’ve been helping friends and small businesses cut costs by running Kubernetes on Hetzner Cloud, which I’ve found to be rock solid and by far the best priced provider.<p>Provisioning a cluster on Hetzner is now straightforward, thanks to tools like k3s and hetzner-k3s, but configuring it for your specific needs still takes time and expertise. I built Edka to make that part easy: spin up a production ready cluster in ~2 minutes, then choose how low level or automated you want to go.<p>How it works:<p>Layer 1 – Cluster provisioning  - Creates a k3s-based Kubernetes cluster on Hetzner (lightweight, easy to manage, scales well).<p>Layer 2 – Add-ons  - One-click deploy for metrics-server, cert-manager, and various operators; preconfigured for Hetzner, no extra setup needed.<p>Layer 3 – Applications  - Minimal config UIs for apps built on top of add-ons.  - Example: Need PostgreSQL? Fill a few fields → platform installs CloudNativePG → provisions HA PostgreSQL with PITR → gives ready to use endpoints. Backups can be restored to any point in time with a click. Quick demo: <a href=""https:&#x2F;&#x2F;edka.io&#x2F;apps&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;edka.io&#x2F;apps&#x2F;</a><p>Layer 4 – Deployments  - Connect your CI to push container images to a public&#x2F;private registry.  - Edka updates deployments automatically (with semantic versioning rules), supports instant rollbacks, autoscaling, persistent volumes, secrets&#x2F;env imports, and quick public exposure. Quick demo: <a href=""https:&#x2F;&#x2F;edka.io&#x2F;deployments&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;edka.io&#x2F;deployments&#x2F;</a><p>Tech stack: TypeScript, React + Tailwind CSS, PostgreSQL, Redis, BullMQ, Vault + AWS KMS to encrypted sensitive data.<p>The platform is still in beta and I’m building it in my spare time, so there are some rough edges, but I’d love feedback from anyone running Kubernetes on Hetzner, exploring alternatives to EKS&#x2F;GKE&#x2F;AKS or looking to automate their infrastructure with Kubernetes.<p>More details: <a href=""https:&#x2F;&#x2F;edka.io&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;edka.io&#x2F;</a><p>Thank you!",437,camil,1755279294,story,https://edka.io
38941942,ChatGPT for Teams,,436,szermer,1704913691,story,https://openai.com/chatgpt/team
45783640,Show HN: Why write code if the LLM can just do the thing? (web app experiment),"I spent a few hours last weekend testing whether AI can replace code by executing directly. Built a contact manager where every HTTP request goes to an LLM with three tools: database (SQLite), webResponse (HTML&#x2F;JSON&#x2F;JS), and updateMemory (feedback). No routes, no controllers, no business logic. The AI designs schemas on first request, generates UIs from paths alone, and evolves based on natural language feedback. It works—forms submit, data persists, APIs return JSON—but it&#x27;s catastrophically slow (30-60s per request), absurdly expensive ($0.05&#x2F;request), and has zero UI consistency between requests. The capability exists; performance is the problem. When inference gets 10x faster, maybe the question shifts from &quot;how do we generate better code?&quot; to &quot;why generate code at all?&quot;",436,samrolken,1762019118,story,https://github.com/samrolken/nokode
39315986,"Goody-2, the world's most responsible AI model",,435,belladoreai,1707493709,story,https://www.goody2.ai/chat
45521629,"OpenAI, Nvidia fuel $1T AI market with web of circular deals",See also <a href=""https:&#x2F;&#x2F;www.bloomberg.com&#x2F;news&#x2F;articles&#x2F;2025-10-08&#x2F;the-circular-openai-nvidia-and-amd-deals-raising-fears-of-a-new-tech-bubble"" rel=""nofollow"">https:&#x2F;&#x2F;www.bloomberg.com&#x2F;news&#x2F;articles&#x2F;2025-10-08&#x2F;the-circu...</a> (<a href=""https:&#x2F;&#x2F;archive.ph&#x2F;E7nGC"" rel=""nofollow"">https:&#x2F;&#x2F;archive.ph&#x2F;E7nGC</a>),435,1vuio0pswjnm7,1759964672,story,https://www.bloomberg.com/news/features/2025-10-07/openai-s-nvidia-amd-deals-boost-1-trillion-ai-boom-with-circular-deals
39981032,Intel Gaudi 3 AI Accelerator,,435,goldemerald,1712679700,story,https://www.intel.com/content/www/us/en/newsroom/news/vision-2024-gaudi-3-ai-accelerator.html
42868770,SmolGPT: A minimal PyTorch implementation for training a small LLM from scratch,,434,amrrs,1738174159,story,https://github.com/Om-Alve/smolGPT
38596953,"Show HN: I Remade the Fake Google Gemini Demo, Except Using GPT-4 and It's Real",,434,gregsadetsky,1702261059,story,https://sagittarius.greg.technology/
39450669,ChatGPT went berserk,,432,RafelMri,1708495750,story,https://garymarcus.substack.com/p/chatgpt-has-gone-berserk
39995725,Aider: AI pair programming in your terminal,,432,tosh,1712783202,story,https://github.com/paul-gauthier/aider
44846922,My Lethal Trifecta talk at the Bay Area AI Security Meetup,,430,vismit2000,1754750858,story,https://simonwillison.net/2025/Aug/9/bay-area-ai/
45633482,OpenAI researcher announced GPT-5 math breakthrough that never happened,,430,Topfi,1760873417,story,https://the-decoder.com/leading-openai-researcher-announced-a-gpt-5-math-breakthrough-that-never-happened/
43292946,Microsoft is plotting a future without OpenAI,,430,doublebind,1741373074,story,https://techstartups.com/2025/03/07/microsoft-is-plotting-a-future-without-openai/
38611700,Show HN: Open-source macOS AI copilot using vision and voice,"Heeey! I built a macOS copilot that has been useful to me, so I open sourced it in case others would find it useful too.<p>It&#x27;s pretty simple:<p>- Use a keyboard shortcut to take a screenshot of your active macOS window and start recording the microphone.<p>- Speak your question, then press the keyboard shortcut again to send  your question + screenshot off to OpenAI Vision<p>- The Vision response is presented in-context&#x2F;overlayed over the active window, and spoken to you as audio.<p>- The app keeps running in the background, only taking a screenshot&#x2F;listening when activated by keyboard shortcut.<p>It&#x27;s built with NodeJS&#x2F;Electron, and uses OpenAI Whisper, Vision and TTS APIs under the hood (BYO API key).<p>There&#x27;s a simple demo and a longer walk-through in the GH readme <a href=""https:&#x2F;&#x2F;github.com&#x2F;elfvingralf&#x2F;macOSpilot-ai-assistant"">https:&#x2F;&#x2F;github.com&#x2F;elfvingralf&#x2F;macOSpilot-ai-assistant</a>, and I also posted a different demo on Twitter: <a href=""https:&#x2F;&#x2F;twitter.com&#x2F;ralfelfving&#x2F;status&#x2F;1732044723630805212"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;twitter.com&#x2F;ralfelfving&#x2F;status&#x2F;1732044723630805212</a>",430,ralfelfving,1702387051,story,https://github.com/elfvingralf/macOSpilot-ai-assistant
44594790,Tell HN: Notion Desktop is monitoring your audio and network,"If you have the Notion Desktop App installed, you may have started to notice a &quot;In a meeting? Start AI Meeting Notes&quot; notification pop up exactly when you are joining a virtual meeting (e.g. joining a Google Meet on Firefox).<p>At first, I assumed it must have been using my Google Workspace account to snoop on my calendar. But then I started to notice it would notify exactly when I joined even if I was late and the meeting had previously started.<p>This was the response from Notion Support after they worked with the Notion Engineering team.<p>&gt; Meeting Detection Architecture:<p>&gt;    - The system uses a sophisticated dual-detection approach: microphone monitoring combined with network port analysis<p>&gt;    - Detection is implemented separately for macOS and Windows at the native operating system level<p>I&#x27;ve uninstalled the Notion Desktop App...",430,HoyaSaxa,1752767994,story,
41109926,"Dear AI Companies, instead of scraping OpenStreetMap, how about a $10k donation?",,430,RicoElectrico,1722352304,story,https://en.osm.town/@Firefishy/112875549871566269
39609997,Training LLMs from ground zero as a startup,,429,swyx,1709677903,story,https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness
39972990,"After AI beat them, professional Go players got better and more creative",,428,iNic,1712605341,story,https://www.henrikkarlsson.xyz/p/go
45655161,Neural audio codecs: how to get audio into LLMs,,428,karimf,1761051359,story,https://kyutai.org/next/codec-explainer
44839842,The surprise deprecation of GPT-4o for ChatGPT consumers,,428,tosh,1754676266,story,https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/
41776878,Show HN: Kotlin Money,"Manipulating monetary amounts is a common computing chore. However, no mainstream language has a first-class data type for representing money, it’s up to programmers to code abstractions for it. This isn’t an issue per se until dealing with rounding issues from operations like installment payments (e.g., buy now, pay later), foreign exchange, or even simple things like fee processing and tax collection.<p>Inspired by my days at N26 Brasil dealing with these challenges, I introduce Money: a Kotlin library that makes monetary calculations and allocations easy.",427,eriksencosta,1728392392,story,https://blog.eriksen.com.br/en/introducing-kotlin-money
41584486,LinkedIn is now using everyone's content to train their AI tool,,426,lopkeny12ko,1726688248,story,https://twitter.com/RachelTobac/status/1836471586624540705
44433409,"Show HN: Spegel, a Terminal Browser That Uses LLMs to Rewrite Webpages",,426,simedw,1751374182,story,https://simedw.com/2025/06/23/introducing-spegel/
44660323,US AI Action Plan,PDF: <a href=""https:&#x2F;&#x2F;www.whitehouse.gov&#x2F;wp-content&#x2F;uploads&#x2F;2025&#x2F;07&#x2F;Americas-AI-Action-Plan.pdf"" rel=""nofollow"">https:&#x2F;&#x2F;www.whitehouse.gov&#x2F;wp-content&#x2F;uploads&#x2F;2025&#x2F;07&#x2F;Americ...</a>,426,joelburget,1753284538,story,https://www.ai.gov/action-plan
40636980,OpenAI and Apple Announce Partnership,,422,serjester,1718045713,story,https://openai.com/index/openai-and-apple-announce-partnership/
44443109,I'm dialing back my LLM usage,,422,sagacity,1751460502,story,https://zed.dev/blog/dialing-back-my-llm-usage-with-alberto-fortin
39706004,Show HN: Skyvern – Browser automation using LLMs and computer vision,"Hey HN, we&#x27;re building Skyvern (<a href=""https:&#x2F;&#x2F;www.skyvern.com"">https:&#x2F;&#x2F;www.skyvern.com</a>), an open-source tool that uses LLMs and computer vision to help companies automate browser-based workflows. You can see some examples here: <a href=""https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;skyvern#real-world-examples-of-skyvern"">https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;skyvern#real-world-examples-of...</a> and there&#x27;s a demo video at <a href=""https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;skyvern#demo"">https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;skyvern#demo</a>, along with some instructions on running it locally.<p>We provide a natural-language API to automate repetitive manual workflows that happen within the companies&#x27; backoffices. You can check out our code and play with Skyvern here: <a href=""https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;Skyvern"">https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;Skyvern</a><p>We talked to hundreds of companies about things they do in the background and found that most of them depend on repetitive manual workflows. The breadth of these workflows surprised us – most companies started off doing things manually, and eventually either hired people to scale the manual work, or wrote scripts using Selenium-like browser automation libraries.<p>In these conversations, one common point stood out: scaling is a pain either way. Companies relying on hiring struggled to adjust team sizes with fluctuating demand. Companies using Selenium and similar tools had a different problem: it can take days or even weeks to get a new workflow automated, and then would require ongoing maintenance any time the underlying websites changed because their XPath based interaction logic suddenly became invalid.<p>We felt like there was a way to get the best of both worlds with LLMs. We could use LLMs to reason through a website’s layout, while preserving the advantage of traditional browser automations allowing it to scale alongside demand. This led us to build Skyvern with a few core functionalities:<p>1. Skyvern can operate on websites it’s never seen before by connecting visible elements with the natural language instructions provided to us. We use a blend of computer vision and DOM parsing to identify a set of possible actions on a website, and multi-modal LLMs to map the natural language instructions to the available actions on the page.<p>2. Skyvern is resistant to website layout changes, as it doesn’t depend on any predetermined XPaths or other selectors. If a layout ever changes, we can leverage the methodology in #1 to complete the user-specified goal.<p>3. Skyvern accepts a blob of information when navigating workflows—basically just a json blob of whatever information you want to put, and then we use LLMs to map that to information on the screen. For example: if you&#x27;re generating a quote from Geico, they commonly ask “Were you eligible to drive at 21?”. The answer could be inferred from the driver receiving their license in 2012, and having a birth date of 1996.<p>The above strategy adapts well to a number of use cases that Skyvern is helping companies with today: (1) Automating materials procurement by searching for, adding to cart, and transacting products through vendor websites that don’t have APIs; (2) Registering accounts, filing forms, and searching for information on government websites (ex: registering franchise tax information for Delaware C-corps); (3) Generating insurance quotes by completing multi-step dynamic forms on insurance websites; (4) Automating the job application process by mapping user-specified information (such as a Resume) to a job posting.<p>And here are some use-cases we’re actively looking to expand into: (1) Automating post-checkup data entry with patient data inside medical EHR systems (ie submitting billing codes, adding notes, etc), an (2) Doing customer research ahead of discovery calls by analyzing landing pages and other metadata about a specific business.<p>We’re still very early and would love to get your feedback!",422,suchintan,1710433894,story,https://github.com/Skyvern-AI/skyvern
43573755,Senior Developer Skills in the AI Age,,421,lamp_book,1743706044,story,https://manuel.kiessling.net/2025/03/31/how-seasoned-developers-can-achieve-great-results-with-ai-coding-agents/
45055641,Some thoughts on LLMs and software development,,420,floverfelt,1756407120,story,https://martinfowler.com/articles/202508-ai-thoughts.html
40067677,Show HN: Speeding up LLM inference 2x times (possibly),"Here&#x27;s a project I&#x27;ve been working on for the last few months.<p>It&#x27;s a new (I think) algorithm, that allows to adjust smoothly - and in real time - how many calculations you&#x27;d like to do during inference of an LLM model.<p>It seems that it&#x27;s possible to do just 20-25% of weight multiplications instead of all of them, and still get good inference results.<p>I implemented it to run on M1&#x2F;M2&#x2F;M3 GPU. The mmul approximation itself can be pushed to run 2x fast before the quality of output collapses.<p>The inference speed is just a bit faster than Llama.cpp&#x27;s, because the rest of implementation could be better, but with a better development I think it can be a new method to speed up inference - in addition to quantization.<p>You could call it ad-hoc model distillation :)<p>You can change the speed &#x2F; accuracy of a model at will, in real time.<p>Oh, and as a side effect, the data format allows to also choose how much of the model you want to load into the memory. You can decide to skip say 10-20-40% of the least important weights.<p>It&#x27;s implemented for Mistral, it was also tested slightly on Mixtral and Llama. It&#x27;s for FP16 for now, but Q8 is in the works.<p>The algorithm is described here, and the implementation is open source.<p><a href=""https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;kolinko.github.io&#x2F;effort&#x2F;</a><p>I know these are bold claims, but I hope they survive the scrutiny :)",419,kolinko,1713374769,story,https://asciinema.org/a/piP22yYwcaohu5cA2gyuv1W61
44622608,LLM architecture comparison,,418,mdp2021,1752994561,story,https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison
44974104,95% of Companies See 'Zero Return' on $30B Generative AI Spend,,418,speckx,1755790584,story,https://thedailyadda.com/95-of-companies-see-zero-return-on-30-billion-generative-ai-spend-mit-report-finds/
40683210,Microsoft's Recall AI feature is now indefinitely delayed,,417,retskrad,1718388189,story,https://blogs.windows.com/windowsexperience/2024/06/07/update-on-the-recall-preview-feature-for-copilot-pcs/
42330666,OpenAI o1 system card,,417,meetpateltech,1733421839,story,https://openai.com/index/openai-o1-system-card/
45856804,Study identifies weaknesses in how AI systems are evaluated,Paper: <a href=""https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=mdA5lVvNcU"" rel=""nofollow"">https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=mdA5lVvNcU</a><p>Related: <a href=""https:&#x2F;&#x2F;www.theregister.com&#x2F;2025&#x2F;11&#x2F;07&#x2F;measuring_ai_models_hampered_by&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.theregister.com&#x2F;2025&#x2F;11&#x2F;07&#x2F;measuring_ai_models_h...</a>,416,pseudolus,1762611502,story,https://www.oii.ox.ac.uk/news-events/study-identifies-weaknesses-in-how-ai-systems-are-evaluated/
38424939,Understanding Deep Learning,,415,georgehill,1701032069,story,https://udlbook.github.io/udlbook/
42864854,I do not want AI to ""polish"" me,,415,doodpants,1738158630,story,https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/
41767648,Nearly all of the Google images results for ""baby peacock"" are AI generated,,415,jsheard,1728318303,story,https://twitter.com/notengoprisa/status/1842550658102079556
40843848,My finetuned models beat OpenAI's GPT-4,,414,majc2,1719823984,story,https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html
40599749,Extracting concepts from GPT-4,,414,davidbarker,1717693297,story,https://openai.com/index/extracting-concepts-from-gpt-4/
39709912,What I learned from looking at 900 most popular open source AI tools,,413,swyx,1710456685,story,https://huyenchip.com/2024/03/14/ai-oss.html
43440267,Pen and Paper Exercises in Machine Learning (2022),,413,ibobev,1742587632,story,https://arxiv.org/abs/2206.13446
43906841,Launch HN: Exa (YC S21) – The web as a database,"Hey HN! We’re Will and Jeff from Exa (<a href=""https:&#x2F;&#x2F;exa.ai"">https:&#x2F;&#x2F;exa.ai</a>). We recently launched Exa Websets, an embeddings-powered search engine designed to return exactly what you’re asking for. You can get precise results for complex queries like “all startups working on open-source developer tools based in SF, founded 2021-2025”.  Demo here - <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;Unt8hJmCxd4"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;Unt8hJmCxd4</a><p>We started working on Exa because we were frustrated that while LLM state-of-the-art is advancing every week, Google has gotten worse over time. The Internet used to feel like a magical information portal, but it doesn’t feel that way anymore when you’re constantly being pushed towards SEO-optimized clickbait.<p>Websets is a step in the opposite direction. For every search, we perform dozens of embedding searches over Exa’s vector database of the web to find good search candidates, then we run agentic workflows on each result to verify they match exactly what you asked for.<p>Websets results are good for two reasons. First, we train custom embedding models for our main search algorithm, instead of typical keyword matching search algorithms. Our embeddings models are trained specifically to return exactly the type of entity you ask for. In practice, that means if you search “startups working in nanotech”, keyword-based search engines return listicles about nanotech startups, because these listicles match the keywords in the query. In contrast, our embedding models return actual startup homepages, because these startup homepages match the meaning of the query.<p>The second is that LLMs provide the last-mile intelligence needed to verify every result. Each result and piece of data is backed with supporting references that we used to validate that the result is actually a match for your search criteria. That’s why Websets can take minutes or even hours to run, depending on your query and how many results you ask for. For valuable search queries, we think this is worth it.<p>Also notably, Websets are tables, not lists. You can add “enrichment” columns to find more information about each result, like “# of employees” or “does author have blog?”, and the cells asynchronously load in. This table format hopefully makes the web feel more like a database.<p>A few examples of searches that work with Websets:<p>- “Math blogs created by teachers from outside the US”: <a href=""https:&#x2F;&#x2F;websets.exa.ai&#x2F;cma1oz9xf007sis0ipzxgbamn"">https:&#x2F;&#x2F;websets.exa.ai&#x2F;cma1oz9xf007sis0ipzxgbamn</a><p>- &quot;research paper about ways to avoid the O(n^2) attention problem in transformers, where one of the first author&#x27;s first name starts with &quot;A&quot;,&quot;B&quot;, &quot;S&quot;, or &quot;T&quot;, and it was written between 2018 and 2022”: <a href=""https:&#x2F;&#x2F;websets.exa.ai&#x2F;cm7dpml8c001ylnymum4sp11h"">https:&#x2F;&#x2F;websets.exa.ai&#x2F;cm7dpml8c001ylnymum4sp11h</a><p>- “US based healthcare companies, with over 100 employees and a technical founder&quot;: <a href=""https:&#x2F;&#x2F;websets.exa.ai&#x2F;cm6lc0dlk004ilecmzej76qx2"">https:&#x2F;&#x2F;websets.exa.ai&#x2F;cm6lc0dlk004ilecmzej76qx2</a><p>- “all software engineers in the Bay Area, with experience in startups, who know Rust and have published technical content before”: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;knjrlm1aibQ"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;knjrlm1aibQ</a><p>You can try it at <a href=""https:&#x2F;&#x2F;websets.exa.ai&#x2F;"">https:&#x2F;&#x2F;websets.exa.ai&#x2F;</a> and API docs are at <a href=""https:&#x2F;&#x2F;docs.exa.ai&#x2F;websets"">https:&#x2F;&#x2F;docs.exa.ai&#x2F;websets</a>. We’d love to hear your feedback!",412,willbryk,1746548322,story,
41625903,Cloudflare's new marketplace lets websites charge AI bots for scraping,,412,boristsr,1727098300,story,https://techcrunch.com/2024/09/23/cloudflares-new-marketplace-lets-websites-charge-ai-bots-for-scraping/
42999632,I built an open source AI tool to find my autoimmune disease,,412,makehistory,1739191704,story,https://old.reddit.com/r/selfhosted/comments/1ij7s4m/how_i_built_an_open_source_ai_tool_to_find_my/
39733275,LLM4Decompile: Decompiling Binary Code with LLM,,412,Davidbrcz,1710670523,story,https://github.com/albertan017/LLM4Decompile
39372159,Show HN: Reor – An AI note-taking app that runs models locally,"Reor is an open-source AI note-taking app that runs models locally.<p>The four main things to know are:<p>1. Notes are connected automatically with vector search. You can do semantic search + related notes are automatically connected.<p>2. You can do RAG Q&amp;A on your notes using the local LLM of your choice.<p>3. Embedding model, LLM, vector db and files are all run or stored locally.<p>4. Point it to a directory of markdown files (like an Obsidian vault) and it works seamlessly alongside Obsidian.<p>Under the hood, Reor uses Llama.cpp (node-llama-cpp integration), Transformers.js and Lancedb to power the local AI features.<p>Reor was built right from the start to support local models. The future of knowledge management involves using lots of AI to organize pieces of knowledge - but crucially, that AI should run as much as possible privately &amp; locally.<p>It&#x27;s available for Mac, Windows &amp; Linux on the project Github: <a href=""https:&#x2F;&#x2F;github.com&#x2F;reorproject&#x2F;reor"">https:&#x2F;&#x2F;github.com&#x2F;reorproject&#x2F;reor</a>",411,samlhuillier,1707930052,story,https://github.com/reorproject/reor
43796935,Lossless LLM compression for efficient GPU inference via dynamic-length float,,411,CharlesW,1745605253,story,https://arxiv.org/abs/2504.11651
41651548,OpenAI to remove non-profit control and give Sam Altman equity,,410,award_,1727296308,story,https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/
45658928,Karpathy on DeepSeek-OCR paper: Are pixels better inputs to LLMs than text?,<a href=""https:&#x2F;&#x2F;xcancel.com&#x2F;karpathy&#x2F;status&#x2F;1980397031542989305"" rel=""nofollow"">https:&#x2F;&#x2F;xcancel.com&#x2F;karpathy&#x2F;status&#x2F;1980397031542989305</a>,410,JnBrymn,1761068596,story,https://twitter.com/karpathy/status/1980397031542989305
42035319,Project Sid: Many-agent simulations toward AI civilization,,409,talms,1730660959,story,https://github.com/altera-al/project-sid
44239481,Low-background Steel: content without AI contamination,,408,jgrahamc,1749578108,story,https://blog.jgc.org/2025/06/low-background-steel-content-without-ai.html
43450732,Improving recommendation systems and search in the age of LLMs,,408,7d7n,1742701205,story,https://eugeneyan.com/writing/recsys-llm/
39852118,LLMs use a surprisingly simple mechanism to retrieve some stored knowledge,,408,CharlesW,1711636670,story,https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325
44828884,OpenAI's new open-source model is basically Phi-5,,403,emschwartz,1754593186,story,https://www.seangoedecke.com/gpt-oss-is-phi-5/
40142374,Meta does everything OpenAI should be,,403,quick_brown_fox,1713951950,story,https://old.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/
41168904,OpenAI co-founder John Schulman says he will leave and join rival Anthropic,,403,tzury,1722933595,story,https://www.cnbc.com/2024/08/06/openai-co-founder-john-schulman-says-he-will-join-rival-anthropic.html
43157831,"Microsoft cancels leases for AI data centers, analyst says",,402,suraci,1740391957,story,https://www.bloomberg.com/news/articles/2025-02-24/microsoft-cancels-leases-for-ai-data-centers-analyst-says
41691943,Do AI companies work?,,402,herbertl,1727653497,story,https://benn.substack.com/p/do-ai-companies-work
40534293,What we've learned from a year of building with LLMs,,401,ViktorasJucikas,1717158790,story,https://eugeneyan.com/writing/llm-lessons/
42539155,How I run LLMs locally,,400,Abishek_Muthian,1735469340,story,https://abishekmuthian.com/how-i-run-llms-locally/
44294633,Generative AI coding tools and agents do not work for me,,399,nomdep,1750120425,story,https://blog.miguelgrinberg.com/post/why-generative-ai-coding-tools-and-agents-do-not-work-for-me
39974374,Hello OLMo: A truly open LLM,,398,tosh,1712615217,story,https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222?gi=760105621962
44112326,Show HN: AutoThink – Boosts local LLM performance with adaptive reasoning,"I built AutoThink, a technique that makes local LLMs reason more efficiently by adaptively allocating computational resources based on query complexity.<p>The core idea: instead of giving every query the same &quot;thinking time,&quot; classify queries as HIGH or LOW complexity and allocate thinking tokens accordingly. Complex reasoning gets 70-90% of tokens, simple queries get 20-40%.<p>I also implemented steering vectors derived from Pivotal Token Search (originally from Microsoft&#x27;s Phi-4 paper) that guide the model&#x27;s reasoning patterns during generation. These vectors encourage behaviors like numerical accuracy, self-correction, and thorough exploration.<p>Results on DeepSeek-R1-Distill-Qwen-1.5B:<p>- GPQA-Diamond: 31.06% vs 21.72% baseline (+43% relative improvement)<p>- MMLU-Pro: 26.38% vs 25.58% baseline<p>- Uses fewer tokens than baseline approaches<p>Works with any local reasoning model - DeepSeek, Qwen, custom fine-tuned models. No API dependencies.<p>The technique builds on two things I developed: an adaptive classification framework that can learn new complexity categories without retraining, and an open source implementation of Pivotal Token Search.<p>Technical paper: <a href=""https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=5253327"" rel=""nofollow"">https:&#x2F;&#x2F;papers.ssrn.com&#x2F;sol3&#x2F;papers.cfm?abstract_id=5253327</a><p>Code and examples: <a href=""https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;optillm&#x2F;tree&#x2F;main&#x2F;optillm&#x2F;autothink"">https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;optillm&#x2F;tree&#x2F;main&#x2F;optillm&#x2F;autoth...</a><p>PTS implementation: <a href=""https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;pts"">https:&#x2F;&#x2F;github.com&#x2F;codelion&#x2F;pts</a><p>I&#x27;m curious about your thoughts on adaptive resource allocation for AI reasoning. Have you tried similar approaches with your local models?",397,codelion,1748399951,story,
44893254,Illinois limits the use of AI in therapy and psychotherapy,,396,reaperducer,1755115910,story,https://www.washingtonpost.com/nation/2025/08/12/illinois-ai-therapy-ban/
45252301,GPT-5-Codex,,396,meetpateltech,1757956239,story,https://openai.com/index/introducing-upgrades-to-codex/
40711484,Getting 50% (SoTA) on Arc-AGI with GPT-4o,,394,tomduncalf,1718661074,story,https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt
38453047,Show HN: Dobb·E – towards home robots with an open-source platform,"Hi HN! Proud to share our open-source robot platform, Dobb·E, a home robot system that needs just 5 minutes of human teaching to learn new tasks. We&#x27;ve already taken Dobb·E to 10 different homes in New York, taught it 100+ tasks, and we are just getting started! I would love to hear your thoughts about this.<p>Here are some more details, below (or see a Twitter thread with attached media: <a href=""https:&#x2F;&#x2F;twitter.com&#x2F;i&#x2F;status&#x2F;1729515379892826211"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;twitter.com&#x2F;i&#x2F;status&#x2F;1729515379892826211</a> or <a href=""https:&#x2F;&#x2F;nitter.net&#x2F;i&#x2F;status&#x2F;1729515379892826211"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;nitter.net&#x2F;i&#x2F;status&#x2F;1729515379892826211</a>):<p>We engineered Dobb·E to maximize efficiency, safety, and user comfort. As a system, it is composed of four parts: a data collection tool, a home dataset, a pretrained vision model, and a policy fine-tuning recipe.<p>We teach our robots with imitation learning, and for data collection, we created the “Stick”, a tool made out of $25 of hardware and an iPhone.<p>Then, using the Stick, we collected a 13 hour dataset in 22 New York homes, called Homes of New York (HoNY). HoNY has 1.5M frames collected over 216 different &quot;environments&quot; which is an order of magnitude larger compared to similar open source datasets.<p>Then we trained a foundational vision model that we can fine-tune fast (15 minutes!) on a new task with only 5 minutes (human time)&#x2F; 90 seconds (demo time) of data. So from start to finish, it takes about 20 minutes to teach the robot a new task.<p>Over a month, we visited 10 homes, tried 109 tasks, and got 81% success rate in simple household tasks. We also found a line of challenges, from mirrors to heavy objects, that we must overcome if we are to get a general purpose home robot.<p>We open-sourced our entire system because our primary goal is to get more robotics and AI researchers, engineers, and enthusiasts to go beyond constrained lab environments and start getting into homes!<p>So here is how you can get started:<p>1. Code and STL files: <a href=""https:&#x2F;&#x2F;github.com&#x2F;notmahi&#x2F;dobb-e&#x2F;"">https:&#x2F;&#x2F;github.com&#x2F;notmahi&#x2F;dobb-e&#x2F;</a><p>2. Technical documentation: <a href=""https:&#x2F;&#x2F;docs.dobb-e.com&#x2F;"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;docs.dobb-e.com&#x2F;</a><p>3. Paper: <a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.16098"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.16098</a><p>4. More videos and the dataset: <a href=""https:&#x2F;&#x2F;dobb-e.com"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;dobb-e.com</a><p>5. Robot we used: <a href=""https:&#x2F;&#x2F;hello-robot.com"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;hello-robot.com</a>",394,MahiShafiullah,1701213834,story,https://dobb-e.com/
39749646,Nvidia CEO Jensen Huang announces new AI chips: ‘We need bigger GPUs’,,393,tiahura,1710793966,story,https://www.cnbc.com/2024/03/18/nvidia-announces-gb200-blackwell-ai-chip-launching-later-this-year.html
45876744,LLMs are steroids for your Dunning-Kruger,,392,gridentio,1762787645,story,https://bytesauna.com/post/dunning-kruger
43018251,Thomson Reuters wins first major AI copyright case in the US,,392,johnneville,1739307381,story,https://www.wired.com/story/thomson-reuters-ai-copyright-lawsuit/
44924461,OpenAI Progress,,391,vinhnx,1755359232,story,https://progress.openai.com
44286277,Accumulation of cognitive debt when using an AI assistant for essay writing task,,391,stephen_g,1750042198,story,https://arxiv.org/abs/2506.08872
42493464,Can AI do maths yet? Thoughts from a mathematician,,390,mathgenius,1734951030,story,https://xenaproject.wordpress.com/2024/12/22/can-ai-do-maths-yet-thoughts-from-a-mathematician/
45771538,How OpenAI uses complex and circular deals to fuel its multibillion-dollar rise,,390,reaperducer,1761915826,story,https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html
45727060,OpenAI says over a million people talk to ChatGPT about suicide weekly,,389,jnord,1761603990,story,https://techcrunch.com/2025/10/27/openai-says-over-a-million-people-talk-to-chatgpt-about-suicide-weekly/
39274918,Better Call GPT: Comparing large language models against lawyers [pdf],,389,vinnyglennon,1707231879,story,https://arxiv.org/abs/2401.16212
43897320,"As an experienced LLM user, I don't use generative LLMs often",,388,minimaxir,1746465760,story,https://minimaxir.com/2025/05/llm-use/
38643076,FunSearch: Making new discoveries in mathematical sciences using LLMs,,388,reqo,1702571052,story,https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/
45168953,Experimenting with Local LLMs on macOS,,388,frontsideair,1757342597,story,https://blog.6nok.org/experimenting-with-local-llms-on-macos/
44501413,"Smollm3: Smol, multilingual, long-context reasoner LLM",,388,kashifr,1751991220,story,https://huggingface.co/blog/smollm3
39593256,Opus 1.5 released: Opus gets a machine learning upgrade,,387,summm,1709573802,story,https://opus-codec.org/demo/opus-1.5/
44789068,Ask HN: What trick of the trade took you too long to learn?,"Every week for the last 3 months I’ve learned a new trick when it comes to getting whatever LLM I’m using at the time to produce better output. That’s my trade, but lots of HNers have more interesting trades than that.<p>In my case, only recently I learned the value of getting an LLM to write and refine a plan.md architecture doc first, and for it to break that doc down into testable phases, and then to implement phase by phase.<p>Seems obvious in hindsight. But it took too long to learn that that should be my approach. I had been going phase by phase myself- no overarching plan.md for the LLM.<p>What Trick of the Trade took you too long to learn?",387,unsupp0rted,1754329199,story,
38972735,OpenAI deletes ban on using ChatGPT for ""military and warfare"",,387,cdme,1705087673,story,https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/
38662160,OpenAI suspends ByteDance's account after it used GPT to train its own AI model,,387,webmaven,1702707466,story,https://www.theverge.com/2023/12/15/24003542/openai-suspends-bytedances-account-after-it-used-gpt-to-train-its-own-ai-model
44006426,MIT asks arXiv to withdraw preprint of paper on AI and scientific discovery,,386,carabiner,1747408165,story,https://economics.mit.edu/news/assuring-accurate-research-record
45093090,Cloudflare Radar: AI Insights,,386,tosh,1756738165,story,https://radar.cloudflare.com/ai-insights
39009779,Many AI safety orgs have tried to criminalize currently-existing open-source AI,,385,sroussey,1705382258,story,https://1a3orn.com/sub/machine-learning-bans.html
38487199,"Show HN: 80% faster, 50% less memory, 0% loss of accuracy Llama finetuning","Hi HN! I&#x27;m just sharing a project I&#x27;ve been working on during the LLM Efficiency Challenge - you can now finetune Llama with QLoRA 5x faster than Huggingface&#x27;s original implementation on your own local GPU. Some highlights:<p>1. Manual autograd engine - hand derived backprop steps.<p>2. QLoRA &#x2F; LoRA 80% faster, 50% less memory.<p>3. All kernels written in OpenAI&#x27;s Triton language.<p>4. 0% loss in accuracy - no approximation methods - all exact.<p>5. No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+.<p>6. Flash Attention support via Xformers.<p>7. Supports 4bit and 16bit LoRA finetuning.<p>8. Train Slim Orca fully locally in 260 hours from 1301 hours (5x faster).<p>9. Open source version trains 5x faster or you can check out Unsloth Pro and Max codepaths for 30x faster training!<p><a href=""https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;188197j&#x2F;80_faster_50_less_memory_0_accuracy_loss_llama&#x2F;"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;188197j&#x2F;80_fast...</a> has more info about Unsloth!<p>Hopefully you can try it out! Wrote a blog post at <a href=""https:&#x2F;&#x2F;unsloth.ai&#x2F;introducing"" rel=""nofollow noreferrer"">https:&#x2F;&#x2F;unsloth.ai&#x2F;introducing</a> if you want to learn more about our manual hand derived backprop or Triton kernels and stuff! Thanks once again!",385,danielhanchen,1701441760,story,https://github.com/unslothai/unsloth
43325049,Ask HN: Any insider takes on Yann LeCun's push against current architectures?,"So, Lecun has been quite public saying that he believes LLMs will never fix hallucinations because, essentially, the token choice method at each step leads to runaway errors -- these can&#x27;t be damped mathematically.<p>In exchange, he offers the idea that we should have something that is an &#x27;energy minimization&#x27; architecture; as I understand it, this would have a concept of the &#x27;energy&#x27; of an entire response, and training would try and minimize that.<p>Which is to say, I don&#x27;t fully understand this. That said, I&#x27;m curious to hear what ML researchers think about Lecun&#x27;s take, and if there&#x27;s any engineering done around it. I can&#x27;t find much after the release of ijepa from his group.",385,vessenes,1741635697,story,
39113879,"Show HN: I wished for a site with a growing list of math problems, I built it","Good math problems are hidden inside textbooks and online documents. To keep up with all the sources in the world is hard. For someone who just wants to continuously solve problems, finding and going through all the sources feels like a hassle. I wished for a website that could just dump all the math problems available in the world out there. And if I could filter the problems by topics, that would be beautiful.<p>teachyourselfmath is a side project that was born out of this need. At its core, it is a math PDF extraction engine. The engine has some machine learning going on behind the scenes to extract math problems in LaTeX from any image or document.<p>A little bit about me: I am Vivek, a software engineer based out of India with a diverse set of interests including math. This project is close to my heart for many different reasons and nothing would make me happier than finding people on the internet who would find this website to be useful.<p>I’d love to hear your feedback on this. Thanks!",384,viveknathani_,1706073169,story,https://teachyourselfmath.app/
42950976,Are LLMs able to notice the “gorilla in the data”?,,384,finding_theta,1738773235,story,https://chiraaggohel.com/posts/llms-eda/
42999297,"Building a personal, private AI computer on a budget",,384,thm,1739188781,story,https://ewintr.nl/posts/2025/building-a-personal-private-ai-computer-on-a-budget/
42997340,TL;DR of Deep Dive into LLMs Like ChatGPT by Andrej Karpathy,,381,oleg_tarasov,1739166965,story,https://anfalmushtaq.com/articles/deep-dive-into-llms-like-chatgpt-tldr
43261650,"Writing an LLM from scratch, part 8 – trainable self-attention",,380,gpjt,1741138874,story,https://www.gilesthomas.com/2025/03/llm-from-scratch-8-trainable-self-attention
41831735,Use Prolog to improve LLM's reasoning,,379,shchegrikovich,1728854568,story,https://shchegrikovich.substack.com/p/use-prolog-to-improve-llms-reasoning
39986095,Mistral AI Launches New 8x22B MOE Model,,379,varunvummadi,1712712684,story,https://twitter.com/MistralAI
45825965,ChatGPT terms disallow its use in providing legal and medical advice to others,,379,randycupertino,1762366276,story,https://www.ctvnews.ca/sci-tech/article/openai-updates-policies-so-chatgpt-wont-provide-medical-or-legal-advice/
41428274,Web scraping with GPT-4o: powerful but expensive,,377,edublancas,1725306653,story,https://blancas.io/blog/ai-web-scraper/
45071722,Show HN: Hacker News em dash user leaderboard pre-ChatGPT,"The use of the em dash (—) now raises suspicions that a text might have been AI-generated. Inspired by a suggestion from dang [1], I created a leaderboard of HN users according to how many of their posts before November 30, 2022—that is, before the release of ChatGPT—contained em dashes. Dang himself comes in number 2—by a very slim margin.<p>Credit to Claude Code for showing me how to search the HN database through Google BigQuery and for writing the HTML for the leaderboard.<p>[1] <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45053933"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45053933</a>",377,tkgally,1756525223,story,https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html
40585039,Simple tasks showing reasoning breakdown in state-of-the-art LLMs,,375,tosh,1717596819,story,https://arxiv.org/abs/2406.02061
43991256,LLMs get lost in multi-turn conversation,,374,simonpure,1747276122,story,https://arxiv.org/abs/2505.06120
44799869,Harmony: OpenAI's response format for its open-weight model series,,374,meetpateltech,1754410078,story,https://github.com/openai/harmony
39263664,Ask HN: What have you built with LLMs?,"Curious what people have been building with LLMs.<p>I worked on a chrome extension a few weeks ago that skips sponsorship sections in YouTube videos by reading through the transcript. Also was trying to experiment with an LLM to explain a function call chain across languages (in this case MakeFile, Python, Bash). I&#x27;ve tried running a few telegram bots that are PRE prompted to do certain things like help you with taxes.<p>What are you building?<p>What does the stack look like? How do you deploy it?",372,break_the_bank,1707153387,story,
41976754,Steve Ballmer was an underrated CEO,,371,greggyb,1730152107,story,https://danluu.com/ballmer/
43233903,Hallucinations in code are the least dangerous form of LLM mistakes,,371,ulrischa,1740942958,story,https://simonwillison.net/2025/Mar/2/hallucinations-in-code/
44854518,LLMs aren't world models,,371,ingve,1754826014,story,https://yosefk.com/blog/llms-arent-world-models.html
43102528,Accelerating scientific breakthroughs with an AI co-scientist,,371,Jimmc414,1739975574,story,https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/
39020778,OpenAI drops ban on military tools to partner with The Pentagon,,371,mfiguiere,1705448274,story,https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects
41544969,Show HN: Wordllama – Things you can do with the token embeddings of an LLM,"After working with LLMs for long enough, I found myself wanting a lightweight utility for doing various small tasks to prepare inputs, locate information and create evaluators. This library is two things: a very simple model and utilities that inference it (eg. fuzzy deduplication). The target platform is CPU, and it’s intended to be light, fast and pip installable — a library that lowers the barrier to working with strings <i>semantically</i>. You don’t need to install pytorch to use it, or any deep learning runtimes.<p>How can this be accomplished? The model is simply token embeddings that are average pooled. To create this model, I extracted token embedding (nn.Embedding) vectors from LLMs, concatenated them along the embedding dimension, added a learnable weight parameter, and projected them to a smaller dimension. Using the sentence transformers framework and datasets, I trained the pooled embedding with multiple negatives ranking loss and matryoshka representation learning so they can be truncated. After training, the weights and projections are no longer needed, because there is no contextual calculations. I inference the entire token vocabulary and save the new token embeddings to be loaded to numpy.<p>While the results are not impressive compared to transformer models, they perform well on MTEB benchmarks compared to word embedding models (which they are most similar to), while being much smaller in size (smallest model, 32k vocab, 64-dim is only 4MB).<p>On the utility side, I’ve been adding some tools that I think it’ll be useful for. In addition to general embedding, there’s algorithms for ranking, filtering, clustering, deduplicating and similarity. Some of them have a cython implementation, and I’m continuing to work on benchmarking them and improving them as I have time. In addition to “standard” models that use cosine similarity for some algorithms, there are binarized models that use hamming distance. This is a slightly faster, similarity algorithm, with significantly less memory per embedding (float32 -&gt; 1 bit).<p>Hope you enjoy it, and find it useful. PS I haven’t figured out Windows builds yet, but Linux and Mac are supported.",370,deepsquirrelnet,1726370714,story,https://github.com/dleemiller/WordLlama
43085885,The Generative AI Con,,370,nimbleplum40,1739850420,story,https://www.wheresyoured.at/longcon/
38629630,Bash one-liners for LLMs,,370,severine,1702484592,story,https://justine.lol/oneliners/
41889414,AI engineers claim new algorithm reduces AI power consumption by 95%,,370,ferriswil,1729361012,story,https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-engineers-build-new-algorithm-for-ai-processing-replace-complex-floating-point-multiplication-with-integer-addition
39014866,TinyML: Ultra-low power machine learning,,370,Gedxx,1705421005,story,https://www.ikkaro.net/what-tinyml-is/
43287821,Ladder: Self-improving LLMs through recursive problem decomposition,,370,fofoz,1741329957,story,https://arxiv.org/abs/2503.00735
41986926,"OpenAI builds first chip with Broadcom and TSMC, scales back foundry ambition",,370,thm,1730222384,story,https://www.reuters.com/technology/artificial-intelligence/openai-builds-first-chip-with-broadcom-tsmc-scales-back-foundry-ambition-2024-10-29/
45247890,RustGPT: A pure-Rust transformer LLM built from scratch,,369,amazonhut,1757929638,story,https://github.com/tekaratzas/RustGPT
44733800,I launched 17 side projects. Result? I'm rich in expired domains,"I think I&#x27;m officially a side project collector.<p>I&#x27;ve had it all:<p>A SaaS for freelancers... that I never had time to finish because I&#x27;m a freelancer.<p>A revolutionary AI tool that I abandoned as soon as GPT-4 came out.<p>And the famous &quot;anti-social media social network&quot; (spoiler: it was just me).<p>I buy a domain name → I code for 3 all-nighters → I lose interest → I start again.<p>My Google Domains look like a graveyard of unfinished dreams.<p>But honestly, I&#x27;ve never learned so much, nor enjoyed it so much.<p>And one day, I might release one that takes off. Or not. But I&#x27;ll be ready.<p>Any other serial side-projectors here? Share your greatest fails&#x2F;unlikely successes",368,cesargstn,1753881335,story,
41457633,"Show HN: AnythingLLM – Open-Source, All-in-One Desktop AI Assistant","Hey HN!<p>This is Tim from AnythingLLM (<a href=""https:&#x2F;&#x2F;github.com&#x2F;Mintplex-Labs&#x2F;anything-llm"">https:&#x2F;&#x2F;github.com&#x2F;Mintplex-Labs&#x2F;anything-llm</a>). AnythingLLM is an open-source desktop assistant that brings together RAG (Retrieval-Augmented Generation), agents, embeddings, vector databases, and more—all in one seamless package.<p>We built AnythingLLM over the last year iterating and iterating from user feedback. Our primary mission is to enable people with a layperson understanding of AI to be able to use AI with little to no setup for either themselves, their jobs, or just to try out using AI as an assistant but with *privacy by default*.<p>From these iterations &amp; feedback, we have a couple of key learnings I wanted to share:<p>- &quot;Chat with your docs&quot; solutions are a dime-a-dozen<p>- Agent frameworks require knowing how to code or are too isolated from other tools<p>- Users do not care about benchmarks, only outputs. The magic box needs to be magic to them.<p>- Asking Consumers to start a docker container or open a terminal is a non-starter for most.<p>- Privacy by default is non-negotiable. Either by personal preference or legal constraints<p>- Everything needs to be in one place<p>From these ideas, we landed on the current state of AnythingLLM:<p>- Everything in AnythingLLM is private by default, but fully customizable for advanced users.<p>- Built-in LLM provider, but can swap at any time to the hundreds of other local or cloud LLM providers &amp; models.<p>- Built-in Vector Database, most users don&#x27;t even know that it is there.<p>- Built-in Embedding model, but of course can change if the user wants to.<p>- Scrape websites, import Github&#x2F;GitLab repos, YouTube Transcripts, Confluence spaces - all of this is already built in for the user.<p>- An entire baked-in agent framework that works seamlessly within the app. We even pre-built a handful of agent skills for customers. Custom plugins are in the next update and will be able to be built with code, or a no-code builder.<p>- All of this just works out of the box in a single installable app that can run on any consumer-grade laptop. Everything a user does, chats, or configures is stored on the user&#x27;s device. Available for Mac, Windows, and Linux<p>We have been actively maintaining and working on AnythingLLM via our open-source repo for a while now and welcome contributors as we hopefully launch a Community Hub soon to really proliferate users&#x27; abilities to add more niche agent skills, data connectors, and more.<p>*But there is even more*<p>We view the desktop app as a hyper-accessible single-player version of AnythingLLM. We publish a Docker image too (<a href=""https:&#x2F;&#x2F;hub.docker.com&#x2F;r&#x2F;mintplexlabs&#x2F;anythingllm"" rel=""nofollow"">https:&#x2F;&#x2F;hub.docker.com&#x2F;r&#x2F;mintplexlabs&#x2F;anythingllm</a>) that supports multi-user management with permissioning so that you can easily bring AnythingLLM into an organization with all of the same features with minimal headache or lift.<p>The Docker image is for those more adept with a CLI, but being able to comfortably go from a single-user to a multi-user version of the same familiar app was very important for us.<p>AnythingLLM aims to be more than a UI for LLMs, we are building a comprehensive tool to leverage LLMs and all that they can do while maintaining user privacy and not needing to be an expert on AI to do it.<p><a href=""https:&#x2F;&#x2F;anythingllm.com&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;anythingllm.com&#x2F;</a>",368,tcarambat1010,1725550845,story,https://github.com/Mintplex-Labs/anything-llm
42274489,"Llama.cpp guide – Running LLMs locally on any hardware, from scratch",,368,zarekr,1732894114,story,https://steelph0enix.github.io/posts/llama-cpp-guide/
42390210,OnlyFans models are using AI impersonators to keep up with their DMs,,368,impish9208,1733937813,story,https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/
44736176,Crush: Glamourous AI coding agent for your favourite terminal,,367,nateb2022,1753892294,story,https://github.com/charmbracelet/crush
39442429,Microsoft is spying on users of its AI tools,,367,mikece,1708442902,story,https://www.schneier.com/blog/archives/2024/02/microsoft-is-spying-on-users-of-its-ai-tools.html
41761497,Google’s AI thinks I left a Gatorade bottle on the moon,,367,gwintrob,1728259643,story,https://edwardbenson.com/2024/10/google-ai-thinks-i-left-gatorade-on-the-moon
41224286,Postgres.new: In-browser Postgres with an AI interface,"hey HN, supabase ceo here<p>This is a new service that we&#x27;re experimenting with that uses PGLite[0], a WASM build of Postgres that runs in the browser. You might remember an earlier WASM build[1] that was around ~30MB. The Electric team [2] have gone one step further and created a complete build of Postgres that’s under 3MB.<p>Their implementation is technically interesting. Postgres is normally multi-process - each client connection is handed to a child process by the postmaster process. In WASM there’s limited&#x2F;no support for process forking and threads. Fortunately, Postgres has a relatively unknown built-in “single user mode” [3] primarily designed for bootstrapping a new database and disaster recovery. Single-user mode only supports a minimal cancel REPL, so PGlite adds wire-protocol support which enables parametrised queries etc.<p>We have created <a href=""https:&#x2F;&#x2F;postgres.new"" rel=""nofollow"">https:&#x2F;&#x2F;postgres.new</a> as an experiment. You can think of it like a love-child between Postgres and ChatGPT: in-browser Postgres sandbox with AI assistance. You can spin up as many new Postgres databases as you want because they all live inside your browser. We pair PGlite with an LLM (currently GPT-4o) and give it full reign over the database with unrestricted permissions. This is an important detail - giving an LLM full autonomy means that it can run multiple operations back-to-back: any SQL errors from Postgres are fed back to the language model so that it can have a few more attempts to solve the problem. Since it’s in-browser it’s low risk.<p>Some other features include:<p><pre><code>    - CSV upload: you can upload a CSV and it will automatically create a Postgres table which you can query with natural language.      - Charts: you can ask the LLM to create a chart with the data and change the colors of the charts.      - RAG &#x2F; pgvector: PGLite supports pgvector, so you can ask the LLM to create embeddings for RAG. The site uses transformers.js [4] to create embeddings inside the browser. </code></pre> We’re working on an update to deploy your databases and serve them from S3 using pg-gateway [5]. We expect to have a read-only deployments ready by the end of the week. You can access them using any postgres-compatible tool (eg: psql).<p>Everything is open source. A huge shout-out to the Electric team who have been a pleasure to build with.<p>[0] PGLite: <a href=""https:&#x2F;&#x2F;github.com&#x2F;electric-sql&#x2F;pglite"">https:&#x2F;&#x2F;github.com&#x2F;electric-sql&#x2F;pglite</a><p>[1] Postgres-wasm: <a href=""https:&#x2F;&#x2F;supabase.com&#x2F;blog&#x2F;postgres-wasm"">https:&#x2F;&#x2F;supabase.com&#x2F;blog&#x2F;postgres-wasm</a><p>[2] Electric: <a href=""https:&#x2F;&#x2F;electric-sql.com&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;electric-sql.com&#x2F;</a><p>[3] Single user mode: <a href=""https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;app-postgres.html#APP-POSTGRES-SINGLE-USER"" rel=""nofollow"">https:&#x2F;&#x2F;www.postgresql.org&#x2F;docs&#x2F;current&#x2F;app-postgres.html#AP...</a><p>[4] transformers.js: <a href=""https:&#x2F;&#x2F;github.com&#x2F;xenova&#x2F;transformers.js"">https:&#x2F;&#x2F;github.com&#x2F;xenova&#x2F;transformers.js</a><p>[5] pg-gateway: <a href=""https:&#x2F;&#x2F;github.com&#x2F;supabase-community&#x2F;pg-gateway"">https:&#x2F;&#x2F;github.com&#x2F;supabase-community&#x2F;pg-gateway</a>",366,kiwicopple,1723470210,story,https://supabase.com/blog/postgres-new
42675725,Tabby: Self-hosted AI coding assistant,,366,saikatsg,1736707385,story,https://github.com/TabbyML/tabby
44527947,LLM Inference Handbook,,366,djhu9,1752201632,story,https://bentoml.com/llm/
45732350,The next chapter of the Microsoft–OpenAI partnership,,366,meetpateltech,1761656740,story,https://openai.com/index/next-chapter-of-microsoft-openai-partnership/
41291219,Launch HN: Sorcerer (YC S24) – Weather balloons that collect more data,"Hey HN! We’re Max, Alex, and Austin, the team behind Sorcerer (<a href=""https:&#x2F;&#x2F;sorcerer.earth"">https:&#x2F;&#x2F;sorcerer.earth</a>). Sorcerer builds weather balloons that last for over six months, collecting 1000x more data per dollar and reaching previously inaccessible regions.<p>In 1981, weather disasters caused $3.5 billion in damages in the United States. In 2023, that number was $94.9 billion (<a href=""https:&#x2F;&#x2F;www.ncei.noaa.gov&#x2F;access&#x2F;billions&#x2F;time-series"" rel=""nofollow"">https:&#x2F;&#x2F;www.ncei.noaa.gov&#x2F;access&#x2F;billions&#x2F;time-series</a>). The National Weather Service spends billions annually on its network of weather balloons, satellites, and aircraft sensors – generating hundreds of terabytes of data every day. This data, called observation data, is fed into massive supercomputers running advanced physics to produce global weather forecasts. Despite this cost, there are still places in the US where we don&#x27;t know what the temperature will be two days from now: <a href=""https:&#x2F;&#x2F;www.washingtonpost.com&#x2F;climate-environment&#x2F;interactive&#x2F;2024&#x2F;how-accurate-is-the-weather-forecast&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.washingtonpost.com&#x2F;climate-environment&#x2F;interacti...</a>. And for the rest of the world that lacks weather infrastructure? There’s always the Weather Rock: <a href=""https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Weather_rock"" rel=""nofollow"">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Weather_rock</a>.<p>The most important data for these forecasts come from vertical data ‘slices’ of the atmosphere, called soundings. Every day 2,500 single-use latex radiosondes are launched across the globe to collect these soundings. They stay aloft for about two hours before popping and falling back to Earth. Launch sites for these systems are sparse in Latin America and Africa, and they’re completely non-existent over oceans. This leaves about 80% of the globe with inadequate weather data for accurate predictions.<p>The coverage gap became painfully evident to Max and Alex during their time at Urban Sky. While building balloons for high-altitude aerial imaging, they kept running into a problem: no matter what weather forecast they used, they couldn’t get accurate wind predictions for the upper atmosphere. They tried all of the free and commercial forecast products, but none of them were accurate enough. Digging into it more, we learned that a big part of the problem was the lack of high-quality in-situ data at those altitudes.<p>To solve this problem, our systems ascend and descend between sea level and 65,000ft several times a day to collect vertical data soundings. Each vehicle (balloon + payload) weighs less than a pound and can be launched from anywhere in the world, per the FAA and ICAO reg. Here’s one we launched from Potrero Hill in SF, <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;75fN5WpRWH0"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;75fN5WpRWH0</a> and here’s another near the Golden Gate Bridge, <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;7yLmzLPUFVQ"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;7yLmzLPUFVQ</a>. Although we can’t “drive” these balloons laterally, we can use opposing wind layers to target or avoid specific regions. Here’s what a few simulated flight paths look like, to give you an idea: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;F_Di8cjaEUY"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;F_Di8cjaEUY</a><p>Our payload uses a satellite transceiver for communications and a small, thin film solar panel array to generate power. In addition to the weather data, we also get real-time telemetry from the vehicles, which we use to optimize their flight paths. This includes maintaining the optimal spacing between balloons and steering them to a recovery zone at the end of their lifespan so we can recycle them.<p>These systems spend most of their time in the stratosphere which is an extremely unforgiving environment. We’ll often see temperatures as low as -80°C while flying near the equator. Throughout the day, we experience extreme temperature cycling as they ascend and descend through the atmosphere. We’ll often encounter 100mph+ wind shears near the boundary with the troposphere (the tropopause) that can rip apart the balloon envelope. These conditions make the stratosphere a very difficult place to deploy to prod.<p>The real magic of what we’re building will come into play when we have hundreds of these systems in the air over data-sparse regions. But even now, we can do useful and interesting things with them. Some of our early customers are companies who fly very big, very expensive things into the stratosphere. They use our balloons to give them a clear idea of what conditions are ahead of their operations, and we’re working on a forecast product specifically designed for the stratosphere.<p>The combination of long duration and low cost is novel. We can theoretically maintain thousands of balloons in the atmosphere at any given time for a tenth of the cost of one useful weather satellite. We’re also using the data we collect to train AI models that produce forecasts with better accuracy than existing numerical (supercomputer) forecasts. Because we’re collecting totally unique data over areas that lack observation, our models will maintain a consistent edge versus models that are only trained on open data.<p>We’re really excited to be launching Sorcerer here with you! We’d love to hear what you think. And if you find one of our balloons in the Bay Area: Sorry! It’s still a work in progress (and please get it back to us).<p>I’ll leave you all with a bonus video of Paul Buchheit launching one of our balloons, which we thought was pretty cool: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-sngF9VvDzg"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-sngF9VvDzg</a>",365,tndl,1724076816,story,
45991738,Adversarial poetry as a universal single-turn jailbreak mechanism in LLMs,,365,capgre,1763640086,story,https://arxiv.org/abs/2511.15304
44003700,"After months of coding with LLMs, I'm going back to using my brain",,365,a7fort,1747391375,story,https://albertofortin.com/writing/coding-with-ai
40610794,"Ask HN: Machine learning engineers, what do you do at work?","I&#x27;m curious about the day-to-day of a Machine Learning engineer. If you work in this field, could you share what your typical tasks and projects look like? What are you working on?",365,Gooblebrai,1717781194,story,
45789602,Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch,,365,meander_water,1762083806,story,https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/
39829695,Ask HN: What non-AI products are you working on?,I see so many AI product launches. Is there anyone who is working on non-AI products?<p>If so what are you working on?,365,jackedEngineer,1711470350,story,
42453660,1-800-ChatGPT,,365,yzydserd,1734548593,story,https://help.openai.com/en/articles/10193193-1-800-chatgpt-calling-and-messaging-chatgpt-with-your-phone
45022184,"Will Smith's concert crowds are real, but AI is blurring the lines",,364,jay_kyburz,1756181470,story,https://waxy.org/2025/08/will-smiths-concert-crowds-were-real-but-ai-is-blurring-the-lines/
39848847,Ask HN: What things are happening in ML that we can't hear over the din of LLMs?,What are some  exciting things that are happening in the #ML #DataScience world that we are not able to hear over the din of LLMs?<p>I notice that Cynthia rudin is continuing to produce great stuff on explainable AI.<p>What else is going on that is not GPT&#x2F;Diffusion&#x2F;MultiModal?,364,aflip,1711614358,story,
45095603,Amazon has mostly sat out the AI talent war,<a href=""https:&#x2F;&#x2F;archive.ph&#x2F;ed8WJ"" rel=""nofollow"">https:&#x2F;&#x2F;archive.ph&#x2F;ed8WJ</a>,363,ripe,1756753471,story,https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8
39807912,"Show HN: Glossarie – a new, immersive way to learn a language","Hi HN, For over two years I&#x27;ve been working on an App to learn languages (currently French, Italian and Spanish), together with my partner, a language teacher. I think it is finally ready to share with this community!<p>The idea is to introduce vocabulary and grammar whilst you read eBooks in your own language. I&#x27;ve found that it is easier to remember vocabulary &#x27;in context&#x27; and with regular repetition. Plus you don&#x27;t have to carve out dedicated time for language learning. Other apps require you to build a habit around various exercises or ‘games’, whereas lots of people already read books.<p>From testing with early users so far it&#x27;s proving effective for building a basic understanding of a language and quickly getting to the point where you can read and broadly understand text in the target language. It’s even better in combination with other apps that help with listening&#x2F;speaking like Pimsleur.<p>There were lots of technical challenges making this. It turned out to be (reassuringly) hard to get accuracy to an acceptable level, requiring a rabbit-hole into machine translation. There was a lot of testing required to optimise the engine that chooses the translations to show and to reduce the friction when reading books. And the backend to support uploading books is a beast in itself. I’d love to share details if there is interest.<p>Roadmap<p>- Accuracy - 100% accuracy is the target, but at present there can be errors. Feedback from users will be important here so that accuracy issues can be generalised and solved at scale. Errors can be reported within the app - please do so if you spot anything!<p>- Dynamic difficulty - rather than have a progression of difficulty levels I’d prefer to introduce vocabulary and grammar automatically in response to user progress, balancing against the friction of seeing unfamiliar words. There’s a lot ‘under the hood’ to manage this today, but plenty of room to improve.<p>- More practice features - to reinforce vocabulary&#x2F;grammar and support writing, listening and speaking.<p>- Better eBook support - improving the formatting of eBooks within the app and providing more methods for finding good books to read.<p>Use of AI<p>- LLMs provided a step change in accuracy and have enabled a feature that explains translations and grammar to the user<p>- vastly improving the utility versus a year ago.<p>- I believe apps like this, which use AI to enhance or scale functionality rather than simply acting as a wrapper over APIs, will be the major beneficiaries as LLMs improve.<p>Take a look, and let me know your thoughts or questions!",363,jonathanb88,1711294996,story,https://glossarie.app/
42555320,Coconut by Meta AI – Better LLM Reasoning with Chain of Continuous Thought?,,362,TaurenHunter,1735606483,story,https://aipapersacademy.com/chain-of-continuous-thought/
39675054,OpenAI – transformer debugger release,,362,nmca,1710205942,story,https://github.com/openai/transformer-debugger
45480622,The deadline isn't when AI outsmarts us – it's when we stop using our own minds,,362,NotInOurNames,1759662505,story,https://www.theargumentmag.com/p/you-have-18-months
42468214,A Gentle Introduction to Graph Neural Networks (2021),,362,misonic,1734667842,story,https://distill.pub/2021/gnn-intro/
41458083,"Phind-405B and faster, high quality AI answers for everyone",,361,rushingcreek,1725553329,story,https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches
42348513,"Show HN: Countless.dev – A website to compare every AI model: LLMs, TTSs, STTs",,361,ahmetd,1733564579,story,https://countless.dev/
45152284,GPT-5 Thinking in ChatGPT (a.k.a. Research Goblin) is good at search,"Related: <i>Google&#x27;s new AI mode is good, actually</i> - <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45158586"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=45158586</a> - Sept 2025 (31 comments)",361,simonw,1757187768,story,https://simonwillison.net/2025/Sep/6/research-goblin/
44359938,GitHub CEO: manual coding remains key despite AI boom,,360,andrewstetsenko,1750711832,story,https://www.techinasia.com/news/github-ceo-manual-coding-remains-key-despite-ai-boom
43476337,"Devs say AI crawlers dominate traffic, forcing blocks on entire countries",,360,Bender,1742938957,story,https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/
39208213,Mistral CEO confirms 'leak' of new open source AI model nearing GPT4 performance,,358,pg_1234,1706729534,story,https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/
45004728,Making games in Go: 3 months without LLMs vs. 3 days with LLMs,,358,maloga,1756047684,story,https://marianogappa.github.io/software/2025/08/24/i-made-two-card-games-in-go/
42118039,Show HN: Stretch My Time Off – An Algorithm to Optimize Your Vacation Days,"Hey HN! I built StretchMyTimeOff as a quick experiment using Cursor (Anysphere&#x27;s AI code editor) and GPT-4o to see how far AI could go in building a simple, functional site.<p>What it does: The site helps you get the most out of your vacation by suggesting optimal days to take off around national holidays, maximizing long breaks with minimal vacation days, anywhere in the world and for any calendar year.<p>It&#x27;s an idea I&#x27;ve had for a while, and building the algorithm with GPT was a fun challenge. Any feedback or ideas I&#x27;m all ears :)",358,zachd,1731435109,story,https://stretchmytimeoff.com
39486717,Meta's new LLM-based test generator,,358,ben_s,1708725898,story,https://read.engineerscodex.com/p/metas-new-llm-based-test-generator
42356443,Show HN: Cut the crap – remove AI bullshit from websites,"I’ve spent a lot of time reading articles that promise a lot but never give me what I’m looking for. They’re full of clickbait titles, scary claims, and pointless filler. It’s frustrating, and it’s a waste of my time.<p>So I made a tool. You give it a URL, and it tries to cut through all that noise. It gives you a shorter version of the content without all the nonsense. I built this because I’m tired of falling for the same tricks. I just want the facts, not a bunch of filler.<p>What do you think? I’m also thinking of making a Chrome extension that does something similar—like a reader mode, but one that actually removes the crap that gets in the way of real information. Feedback welcome.",356,muc-martin,1733655579,story,https://cut-the-crab.streamlit.app/
40627558,A ChatGPT mistake cost us $10k,,355,asim-shrestha,1717966589,story,https://asim.bearblog.dev/how-a-single-chatgpt-mistake-cost-us-10000/
39387850,Building an LLM from Scratch: Automatic Differentiation (2023),,355,netwrt,1708027312,story,https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html
42247368,Launch HN: Human Layer (YC F24) – Human-in-the-Loop API for AI Systems,"Hey HN! I&#x27;m Dex, building HumanLayer (<a href=""https:&#x2F;&#x2F;humanlayer.dev"">https:&#x2F;&#x2F;humanlayer.dev</a>), an API that lets AI agents contact humans for feedback, input, and approvals. We enable safe deployment of autonomous&#x2F;headless AI systems in production. You can try it with our Python or TypeScript SDKs and start using it immediately with a free trial. We have a free tier and transparent usage-based pricing. Here’s a demo: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;5sbN8rh_S5Q?t=51"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;5sbN8rh_S5Q?t=51</a><p>What&#x27;s really exciting is that we&#x27;re enabling teams to deploy AI systems that would otherwise be too risky. We let you focus on building powerful agents while knowing that critical steps will <i>always</i> get a human-in-the-loop. It&#x27;s been dope seeing people start to think bigger when they consider dynamic human oversight as a key ingredient in production AI systems.<p>This started when we were building AI agents for data teams. We wanted to automate tedious tasks like dropping unused tables, but customers were (rightfully!) opposed to giving AI agents direct access to production systems.<p>Getting AI to &quot;production grade&quot; reliability is a function of &quot;how risky is this task the AI is performing&quot;. We didn&#x27;t have the 3+ months it would have taken to sink into evals, fine tuning, and prompt engineering to get to a point where the agent had 99.9+% reliability—and even then, getting decision makers comfortable with flipping the switch on was a challenge. So instead we built some basic approval flows, like &quot;ask in Slack before dropping tables&quot;.<p>But this communication itself needed guardrails—what if the agent contacted the wrong person? How would the head of data look if a tool he bought sent a nagging Slack message to the CEO? Our buyers wanted the agent to ask stakeholders for approval, but first <i>they</i> wanted to approve the &quot;ask for approval&quot; action itself. And then I started thinking about it... as a product builder + owner, <i>I</i> wanted to approve the &quot;ask for approval to ask for approval&quot; action!<p>I hacked together a human-AI interaction that would handle each of these cases across both my and my customers&#x27; Slack instances. By this time, I was convinced that any team building AI agents would need this kind of infrastructure and decided to build it as a standalone product. I presented the MVP at an AI meetup in SF and had a ton of incredible conversations, and went all in on building HumanLayer.<p>When you integrate the HumanLayer SDK, your AI agent can request human approval at any point in its execution. We handle all the complexity of routing these requests to the right people through their preferred channels (Slack or email, SMS and Teams coming soon), managing state while waiting for responses, and providing a complete audit trail. In addition to &quot;ask for approval&quot;, we also support a more generic &quot;human as tool&quot; function that can be exposed to an LLM or agent framework, and will handle collecting a human response to a generic question like &quot;I&#x27;m stuck on $PROBLEM, I&#x27;ve tried $THINGS, please advise&quot; (I get messages like this sometimes from in-house agents we rolled out for back-office automations).<p>Because it&#x27;s at the tool-calling layer, HumanLayer&#x27;s SDK works with any AI framework like CrewAI, LangChain, etc, and any language model that supports tool calling. If you&#x27;re rolling your own agentic&#x2F;tools loop, you can use lower level SDK primitives to manage approvals however you want. We&#x27;re even exploring use cases where HumanLayer is used for human-to-human approval, not just AI-to-human.<p>We&#x27;re already seeing HumanLayer used in some cool ways. One customer built an AI SDR that drafts personalized sales emails but asks for human approval in Slack before sending anything to prospects. Another uses it to power an AI newsletter where subscribers can have email conversations with the content. HumanLayer handles receiving inbound emails and routing them to agents that can respond, and giving those agents tools to do so. One team uses HumanLayer to build a customer-facing DevOps agent—their AI agent reviews PRs, plans and executes db migrations, all while getting human sign-off at critical steps and reaching out to the team for steering if it encounters any issues.<p>We have a free tier and flexible credits-based pricing. For teams building customer-facing agents, you get whitelabeling and additional features and priority support.<p>If you want to integrate HumanLayer into your systems, check out our docs at <a href=""https:&#x2F;&#x2F;humanlayer.dev&#x2F;docs"">https:&#x2F;&#x2F;humanlayer.dev&#x2F;docs</a> or book a demo at <a href=""https:&#x2F;&#x2F;humanlayer.dev"">https:&#x2F;&#x2F;humanlayer.dev</a>.<p>Thank you for reading! We’re admittedly early and I welcome your ideas and experiences as it relates to agents, reliability, and balancing human+AI workloads.",354,dhorthy,1732640232,story,
40751020,Testing Generative AI for Circuit Board Design,,353,DHaldane,1718986591,story,https://blog.jitx.com/jitx-corporate-blog/testing-generative-ai-for-circuit-board-design
43318624,Probabilistic Artificial Intelligence,,352,pavanto,1741600233,story,https://arxiv.org/abs/2502.05244
38930126,Rabbit: LLM-First Mobile Phone,,352,DreamGen,1704825857,story,https://www.rabbit.tech/
43931366,"Writing an LLM from scratch, part 13 – attention heads are dumb",,351,gpjt,1746738362,story,https://www.gilesthomas.com/2025/05/llm-from-scratch-13-taking-stock-part-1-attention-heads-are-dumb
45121342,Evidence that AI is destroying jobs for young people,,351,duck,1756940856,story,https://www.derekthompson.org/p/the-evidence-that-ai-is-destroying
39849644,Towards 1-bit Machine Learning Models,,351,homarp,1711623514,story,https://mobiusml.github.io/1bit_blog/
45771870,Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop,"Dear Hackers, I’m interested in your real-world workflows for using open-source LLMs and open-source coding assistants on your laptop (not just cloud&#x2F;enterprise SaaS). Specifically:<p>Which model(s) are you running (e.g., Ollama, LM Studio, or others) and which open-source coding assistant&#x2F;integration (for example, a VS Code plugin) you’re using?<p>What laptop hardware do you have (CPU, GPU&#x2F;NPU, memory, whether discrete GPU or integrated, OS) and how it performs for your workflow?<p>What kinds of tasks you use it for (code completion, refactoring, debugging, code review) and how reliable it is (what works well &#x2F; where it falls short).<p>I&#x27;m conducting my own investigation, which I will be happy to share as well when over.<p>Thanks! Andrea.",350,threeturn,1761917995,story,
43619768,Ask HN: Do you still use search engines?,"Today, I noticed that my behavior has shifted over the past few months. Right now, I exclusively use ChatGPT for any kind of search or question.<p>Using Google now feels completely lackluster in comparison.<p>I&#x27;ve noticed the same thing happening in my circle of friends as well—and they don’t even have a technical background.<p>How about you?",350,davidkuennen,1744104198,story,
39453402,Launch HN: Retell AI (YC W24) – Conversational Speech API for Your LLM,"Hey HN, we&#x27;re the co-founders of Retell AI (<a href=""https:&#x2F;&#x2F;www.retellai.com&#x2F;"">https:&#x2F;&#x2F;www.retellai.com&#x2F;</a>). We are building a conversational speech engine to help developers build natural-sounding voice AI. Our API abstracts away the complexities of AI voice conversations, so you can make your voice application the best at what it does. Here&#x27;s a demo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0LT64_mgkro"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=0LT64_mgkro</a>.<p>With the advent of LLMs and recent breakthroughs in speech synthesis, conversational voice AI has just gotten good enough to create really exciting use cases. However, developers often underestimate what&#x27;s required to build a good and natural-sounding conversational voice AI. Many simply stitch together ASR (speech-to-text), an LLM, and TTS (text-to-speech), and expect to get a great experience. It turns out it&#x27;s not that simple.<p>There&#x27;s more going on in conversation than we consciously realize: things like knowing when to speak and when to listen, handling interruptions, 0-200 ms latency and backchanneling phrases (e.g., &quot;yeah&quot;, &quot;uh huh&quot;) to signal that they are listening. These are natural for humans, but hard for AI to get right. Developers spend hundreds of hours on the AI conversation experience but end up with poor experiences like 4-5s long latencies, inappropriate cutoffs, speaking over each other, etc.<p>So, we built Retell AI. We have followed the overall paradigm of having speech-to-text, LLM, and text-to-speech components, but have added additional conversation models in between to orchestrate the conversation while allowing maximum configurability for the developers in each step. You can think of our models as adding a “domain expert” layer for the dynamics of conversation itself.<p>Retell is designed for you to bring your own LLM into our pipeline. Currently, we can achieve 800ms end-to-end latency, handle interruptions, speech isolation, with tons of customization options (e.g., speaking rate, voice temperature, add ambient sound). We created a guest account for HN, so you can try our playground with a 10-min free trial without login: <a href=""https:&#x2F;&#x2F;beta.retellai.com&#x2F;dashboard&#x2F;hn"">https:&#x2F;&#x2F;beta.retellai.com&#x2F;dashboard&#x2F;hn</a> (Playground tutorial: <a href=""https:&#x2F;&#x2F;docs.retellai.com&#x2F;guide&#x2F;dashboard"">https:&#x2F;&#x2F;docs.retellai.com&#x2F;guide&#x2F;dashboard</a>). Our product is usage-based and the price is $0.1-0.17&#x2F;min.<p>Our main product is a developer-facing API, but you can try it without writing code (e.g. create agents, connect to a phone number) via our dashboard. If you want to test it in production, feel free to also self-serve with our API documentation. One of our customers just launched, and you can view their demo: <a href=""https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;64f09a53bf6d4b3799e5ebd08b23fec4?sid=77baea2b-c595-4ae0-92f9-e1c7fc38eccf"" rel=""nofollow"">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;64f09a53bf6d4b3799e5ebd08b23fec4?...</a><p>We are thrilled to see what our users are building with our API, and we’re excited to show our product to the community and look forward to your feedback!",350,yanyan_evie,1708521493,story,
38438261,Let's try to understand AI monosemanticity,,350,bananaflag,1701119086,story,https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand
42396372,"A ChatGPT clone, in 3000 bytes of C, backed by GPT-2 (2023)",,350,chubot,1733979696,story,https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html
38673854,BrainGPT turns thoughts into text,,349,11thEarlOfMar,1702830125,story,https://www.iflscience.com/new-mind-reading-braingpt-turns-thoughts-into-text-on-screen-72054
45916525,Launch HN: Tweeks (YC W25) – Browser extension to deshittify the web,"Hey HN! We’re Jason &amp; Matt and we’re building Tweeks (<a href=""https:&#x2F;&#x2F;tweeks.io"">https:&#x2F;&#x2F;tweeks.io</a>), a browser extension that lets you modify any website in your browser to add functionality, filter&#x2F;highlight, re-theme, reorganize, de-clutter, etc. If you’ve used Violentmonkey&#x2F;Tampermonkey, Tweeks is like a next‑generation userscript manager. Instead of digging through selectors and hand‑writing custom JS&#x2F;CSS, describe what you want in natural language and Tweeks plans + generates your edits and applies them.<p>The modern web is so full of clutter and junk (banners, modals, feeds, and recommendations you didn’t ask for). Even a simple google search is guarded by multiple ads, an AI overview, a trending searches module, etc. before you even see the first real blue link.<p>Every day there&#x27;s a new Lovable-like product (make it simple to build your own website&#x2F;app) or a new agentic browser (AI agents click around and browse the web for you), but we built Tweeks to serve the middle ground: most of our time spent on the web is on someone else&#x27;s site (not our own), and we don&#x27;t want to offload everything to an agentic browser. We want to be able to shape the entire web to our own preferences as we browse.<p>I spent years working on recommendation systems and relevance at Pinterest, and understand how well-meaning recommendations and A&#x2F;B tests can lead to website enshittification. No one sets out to make UX worse, but optimizing for an “average” user is not the same as optimizing for each individual user.<p>I’ve also been hacking “page fixers” as long as I can remember: remove a login wall here, collapse cookie banners there, add missing filters&#x2F;highlights (first with F12&#x2F;inspect element and eventually graduated to advanced GreaseMonkey userscripts). Tweeks started as a weekend prototype that turned simple requests into page edits but unexpectedly grew into something people kept asking to share. We hope you’ll like it too!<p>How it works: Open the Tweeks extension, type your request (e.g. “hide cookie banners and add a price&#x2F;quality score”), and submit. Upon submission, the page structure is captured, an AI agent reviews the structure, plans changes, and returns deterministic transformations (selectors, layout tweaks, styles, and small scripts) that run locally. Your modifications persist across page loads and can be enabled&#x2F;disabled, modified, and shared.<p>Here are a bunch of one‑shot examples from early users:<p><i>Youtube</i>: Remove Youtube Shorts. Demo: <a href=""http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=aL7i89BdO9o"" rel=""nofollow"">http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=aL7i89BdO9o</a>. Try it yourself: <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;bcd8bc32b8034b79a78a8564"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;bcd8bc32b8034b79a78a8564</a><p><i>Hacker News</i>: Filter posts by title&#x2F;url or points&#x2F;comments, modify header and text size. Demo: <a href=""http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=cD5Ei8bMmUk"" rel=""nofollow"">http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=cD5Ei8bMmUk</a>. Try it yourself: <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;97e72c6de5c14906a1351abd"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;97e72c6de5c14906a1351abd</a> (filter), <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;6f51f96c877a4998bda8e781"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;6f51f96c877a4998bda8e781</a> (header + text).<p><i>LinkedIn</i>: Keep track of cool people (extracts author data and send a POST request to a server). Demo: <a href=""http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=WDO4DRXQoTU"" rel=""nofollow"">http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=WDO4DRXQoTU</a><p><i>Reddit</i>: Remove sidebar and add a countdown timer that shows a blocking modal when time is up. Demo: <a href=""http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=kBIkQ9j_u94"" rel=""nofollow"">http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=kBIkQ9j_u94</a>. Try it yourself: <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;e1daa0c5edd441dca5a150c8"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;e1daa0c5edd441dca5a150c8</a> (sidebar), <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;c321c9b6018a4221bd06fdab"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;c321c9b6018a4221bd06fdab</a> (timer).<p><i>New York Times Games</i>: Add a Strands helper that finds all possible words. Demo: <a href=""http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=hJ75jSATg3Q"" rel=""nofollow"">http:&#x2F;&#x2F;youtube.com&#x2F;watch?v=hJ75jSATg3Q</a>. Try it yourself: <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;7a955c910812467eaa36f569"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;7a955c910812467eaa36f569</a><p><i>Theming</i>: Retheme Google to be a 1970s CLI terminal. Demo: <a href=""http:&#x2F;&#x2F;youtube.com&#x2F;shorts&#x2F;V-CG5CbYJb4"" rel=""nofollow"">http:&#x2F;&#x2F;youtube.com&#x2F;shorts&#x2F;V-CG5CbYJb4</a> (oops sorry a youtube short snuck back in there). Try it yourself: <a href=""http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;8c8c0953f6984163922c4da7"">http:&#x2F;&#x2F;tweeks.io&#x2F;share&#x2F;script&#x2F;8c8c0953f6984163922c4da7</a>.<p>We just opened access at <a href=""https:&#x2F;&#x2F;tweeks.io"">https:&#x2F;&#x2F;tweeks.io</a>. It’s currently free, but each use costs tokens so we&#x27;ll likely need to cap usage to prevent abuse. We&#x27;re more interested in early feedback than your money, so if you manage to hit the cap, message us at contact@trynextbyte.com or <a href=""https:&#x2F;&#x2F;discord.gg&#x2F;WucN6wpJw2"" rel=""nofollow"">https:&#x2F;&#x2F;discord.gg&#x2F;WucN6wpJw2</a>, tell us how you&#x27;re using it&#x2F;what features you want next, and we&#x27;ll happily reset it for you.<p>Btw if you do anything interesting with it, feel free to make a shareable link (go to ‘Library’ and press ‘share’ after generating) and include it in the comments below. It’s fun to see the different things people are coming up with!<p>We&#x27;re rapidly shipping improvements and would love your feedback and comments. Thanks for reading!",349,jmadeano,1763049784,story,https://www.tweeks.io/onboarding
39512912,Show HN: AI dub tool I made to watch foreign language videos with my 7-year-old,"Hey HN!<p>I love watching YouTube with my 7-year-old daughter. Unfortunately, the best stuff is often in English (we&#x27;re German). So I made an AI tool that translates videos directly, using the original voices. All other sounds, as well as background music, are preserved, too.<p>Turns out that it works for many other language pairs, too. So far, it can create dubs in English, Mandarin Chinese, Spanish, Arabic, French, Russian, German, Italian, Korean, Polish and Dutch.<p>The main challenge in building this was to get the balance right between translating the original meaning and getting the timing right. Especially for language pairs like English -&gt; German, where the target ist often longer than the source (&quot;bat&quot; -&gt; &quot;Fle-der-maus&quot;, &quot;speed&quot; -&gt; &quot;Ge-schwin-dig-keit&quot;).<p>Let me know what you think! :)",348,leobg,1708963725,story,https://speakz.ai
45483924,What GPT-OSS leaks about OpenAI's training data,,348,fi-le,1759688896,story,https://fi-le.net/oss/
45708066,I'm drowning in AI features I never asked for and I hate it,,348,gnabgib,1761438591,story,https://www.makeuseof.com/ai-features-being-rammed-down-our-throats/
45605842,Tor browser removing various Firefox AI features,,347,HelloUsername,1760625190,story,https://blog.torproject.org/new-alpha-release-tor-browser-150a4/
41563958,"Show HN: Void, an open-source Cursor/GitHub Copilot alternative","Hey HN, I&#x27;m Andrew, one of the creators of Void. I made this open source version of Cursor where you can get all of Cursor&#x27;s core features but in a fully-customizable IDE (ctrl+k, ctrl+L). We love Cursor but there are so many other features we want to build, like allowing AI to edit multiple files at once, or giving AI better understanding of your file system. Void is the open-source, fully customizable tool we&#x27;ve been wanting.<p>The hard part: we&#x27;re building Void as a fork of vscode. The repo has great documentation for extensions, but going deeper gets pretty involved. All of the code is OOP-based, and they mount DOM nodes the old-school way (which is what React was supposed to solve..). So adding new UI features isn&#x27;t exactly trivial. Microsoft also made its extension marketplace closed-source so we (and Cursor) have to hack our way through it. One thing we&#x27;re excited about is refactoring and creating docs so that it&#x27;s much easier for anyone to contribute.<p>The other benefit of open source is we don&#x27;t need to hide how our prompts are built, so we can transfer the private API logic that Cursor has right onto your local machine. This lets you host a model on-prem and have your data stay completely private. It also means you can go directly to LLM providers (OpenAI, Anthropic) instead of going through us as a middleman.<p>There&#x27;s still a lot to build, and full disclosure, we are very early stage. But we&#x27;re super excited about building and have a working prototype that we&#x27;re quickly adding features to.<p>Let us know if there&#x27;s anything you want to see in a Cursor-style editor. Or feel free to shoot us a pull request. Cheers!",347,andrewpareles,1726545509,story,https://github.com/voideditor/void
43620452,Meta got caught gaming AI benchmarks,,347,pseudolus,1744111747,story,https://www.theverge.com/meta/645012/meta-llama-4-maverick-benchmarks-gaming
40345696,Show HN: An open source framework for voice assistants,"I&#x27;ve been obsessed for the past ~year with the possibilities of talking to LLMs. I built a bunch of one-off prototypes, shared code on X, started a Meetup group in SF, and co-hosted a big hackathon. It turns out that there are a few low-level problems that everybody building conversational&#x2F;real-time AI needs to solve on the way to building&#x2F;shipping something that works well: low-latency media transport, echo cancellation, voice activity detection, phrase endpointing, pipelining data between models&#x2F;services, handling voice interruptions, swapping out different models&#x2F;services.<p>On the theory that something like a LlamaIndex or LangChain for real-time&#x2F;conversational AI would be useful, a few of us started working on a Python library for voice (and multimodal) AI assistants&#x2F;agents.<p>So ... Pipecat: a framework for building things like personal coaches, meeting assistants, story-telling toys for kids, customer support bots, virtual friends, and snarky social bots.<p>Most of the core contributors to Pipecat so far work together at our day jobs. This has been a kind of &quot;20% time&quot; thing at our company. But we&#x27;re serious about welcoming all contributions. We want Pipecat to support any and all models, services, transport layers, and infrastructure tooling. If you&#x27;re interested in this stuff, please check it out and let us know what you think. Submit PRs. Become a maintainer. Join the Discord. Post cool stuff. Post funny stuff when your voice agent goes completely off the rails (as mine sometimes do).",346,kwindla,1715620874,story,https://github.com/pipecat-ai/pipecat
44659921,Building better AI tools,,346,eternalreturn,1753282591,story,https://hazelweakly.me/blog/stop-building-ai-tools-backwards/
39853958,Jamba: Production-grade Mamba-based AI model,,346,bubblehack3r,1711643816,story,https://www.maginative.com/article/ai21-labs-unveils-jamba-the-first-production-grade-mamba-based-ai-model/
42905453,Recent results show that LLMs struggle with compositional tasks,,345,thm,1738466467,story,https://www.quantamagazine.org/chatbot-software-begins-to-face-fundamental-limitations-20250131/
45523537,Two things LLM coding agents are still bad at,,345,kixpanganiban,1759984428,story,https://kix.dev/two-things-llm-coding-agents-are-still-bad-at/
44095189,Ask HN: Anyone struggling to get value out of coding LLMs?,"I use LLMs daily for stuff like:<p>- solving tasks that just require applying knowledge (&quot;here&#x27;s a paste of my python import structure. I don&#x27;t write Python often and I&#x27;m aware I&#x27;m doing something wrong here because I get this error, tell me the proper way organise the package&quot;).<p>- writing self-contained throwaway pieces of code (&quot;here&#x27;s a paste of my DESCRIBE TABLE output, write an SQL query to show the median [...]&quot;).<p>- as a debugging partner (&quot;I can SSH to this host directly, but Ansible fails to connect with this error, what could be causing this difference&quot;).<p>All these use cases work great, I save a lot of time. But with the core work of writing the code that I work on, I&#x27;ve almost never had any success. I&#x27;ve tried:<p>- Cursor (can&#x27;t remember which model, the default)<p>- Google&#x27;s Jules<p>- OpenAI Codex with o4<p>I found in all cases that the underlying capability is clearly there (the model can understand and write code) but the end-to-end value is not at all. It could write code that _worked_, but trying to get it to generate code that I am willing to maintain and &quot;put my name on&quot; took longer than writing the code would have.<p>I had to micromanage them infinitely (&quot;be sure to rerun the formatter, make sure all tests pass&quot; and &quot;please follow the coding style of the repository&quot;. &quot;You&#x27;ve added irrelevant comments remove those&quot;. &quot;You&#x27;ve refactored most of the file but forgot a single function&quot;). It would take many many iterations on trivial issues, and because these iterations are slow that just meant I had to context switch a lot, which is also exhausting.<p>Basically it was like having an intern who has successfully learned the core skill of programming but is not really capable of good collaboration and needs to be babysat all the time.<p>I asked friends who are enthusiastic vibe coders and they basically said &quot;your standards are too high&quot;.<p>Is the model for success here that you just say &quot;I don&#x27;t care about code quality because I don&#x27;t have to maintain it because I will use LLMs for that too?&quot; Am I just not using the tools correctly?",345,bjackman,1748247473,story,
45200925,Defeating Nondeterminism in LLM Inference,,345,jxmorris12,1757525168,story,https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/
41071639,OpenAI Announces SearchGPT,,345,notyouralias,1721931305,story,https://chatgpt.com/search
43752492,LLM-powered tools amplify developer capabilities rather than replacing them,,345,matthewsinclair,1745246174,story,https://matthewsinclair.com/blog/0178-why-llm-powered-programming-is-more-mech-suit-than-artificial-human
44436579,Sam Altman Slams Meta’s AI Talent Poaching: 'Missionaries Will Beat Mercenaries',,344,spenvo,1751393318,story,https://www.wired.com/story/sam-altman-meta-ai-talent-poaching-spree-leaked-messages/
44223448,LLMs are cheap,,344,Bogdanp,1749469385,story,https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/
42417857,Fast LLM Inference From Scratch (using CUDA),,344,homarp,1734192168,story,https://andrewkchan.dev/posts/yalm.html
40346995,"Show HN: Pi-C.A.R.D, a Raspberry Pi Voice Assistant","Pi-card is an AI powered voice assistant running locally on a Raspberry Pi. It is capable of doing anything a standard LLM (like ChatGPT) can do in a conversational setting. In addition, if there is a camera equipped, you can also ask Pi-card to take a photo, describe what it sees, and then ask questions about that.<p>It uses distributed models so latency is something I&#x27;m working on, but I am curious on where this could go, if anywhere.<p>Very much a WIP. Feedback welcome :-)",344,nkaz123,1715627035,story,https://github.com/nkasmanoff/pi-card
43402315,Ask HN: How do I escape homelessness after rebuilding my mental health?,"I never thought I’d be here—but I’ve hit a wall I can’t break through alone. I’m asking for help, advice, ideas—anything practical—to get off the streets and back into stability.<p>In the last year, I lost my mother (cancer), my grandmother (old age), and went through a divorce. I spiraled into depression, ended up living out of my car, barely functioning.<p>But I didn’t stay down. Through therapy, ketamine treatment, and building my own AI-assisted mental health tools (ChatGPT literally saved me), I’ve come out stronger, healthier, and ready to rebuild. My mental health is solid now—I’ve done the inner work.<p>Here’s my situation:<p>I’m living in my car, staying in cheap motels when I can afford it.<p>I drive Lyft&#x2F;Uber full-time, but my car needs new tires, insurance renewal, and a payment due—without it, I can’t work. $400 for tires, $200 for insurance, $290 for car payment..<p>I have a desktop computer but nowhere stable to set it up and work.<p>I’m going back to school soon for computer science, aiming for AI&#x2F;ML work, but I need to survive until student aid or inheritance (both delayed) arrives.<p>Here’s my question: What would YOU do in my situation to break the cycle? How can I create a stable base, get back to programming&#x2F;freelancing, and stop just surviving?<p>I’m willing to work. I know I can earn 3-4x more doing freelance tech, but I can’t do that from a car.<p>I’ve thought about renting office space, finding roommates, even setting up a GoFundMe, but I need actionable ideas, not just hope.<p>How do I climb out of this? What programs, jobs, or options are out there for someone like me who’s got skills, drive, but no resources?<p>Any advice, ideas, or help you can give—I’m all ears. I’ve rebuilt my mind. Now I just need a stable place to rebuild my life. It&#x27;s hard feeling constantly like I&#x27;m in quick sand and the rope is 5 inches too far away.<p>I&#x27;m in Southern Utah (St George), willing to relocate to SLC area or Las Vegas, I just need to stay local for my kids...4 hour radius from southern Utah.",343,gremlinsinc,1742319741,story,
45739080,Generative AI Image Editing Showdown,,342,gaws,1761685102,story,https://genai-showdown.specr.net/image-editing
43830613,"Generative AI is not replacing jobs or hurting wages at all, say economists",,342,pseudolus,1745921289,story,https://www.theregister.com/2025/04/29/generative_ai_no_effect_jobs_wages/
43775358,The hidden cost of AI coding,,341,Sharpie4679,1745433887,story,https://terriblesoftware.org/2025/04/23/the-hidden-cost-of-ai-coding/
39895994,OpenAI removes Sam Altman's ownership of its Startup Fund,,340,mfiguiere,1711989276,story,https://www.reuters.com/technology/openai-removes-sam-altmans-ownership-its-startup-fund-2024-04-01/
44832908,GPT-5: ""How many times does the letter b appear in blueberry?"",<a href=""https:&#x2F;&#x2F;kieranhealy.org&#x2F;blog&#x2F;archives&#x2F;2025&#x2F;08&#x2F;07&#x2F;blueberry-hill&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;kieranhealy.org&#x2F;blog&#x2F;archives&#x2F;2025&#x2F;08&#x2F;07&#x2F;blueberry-h...</a>,340,minimaxir,1754621493,story,https://bsky.app/profile/kjhealy.co/post/3lvtxbtexg226
40425403,The OpenAI board was right,,340,isaacfrond,1716278784,story,https://garymarcus.substack.com/p/the-openai-board-was-right
39091777,LoRA from scratch: implementation for LLM finetuning,,339,rasbt,1705942580,story,https://lightning.ai/lightning-ai/studios/code-lora-from-scratch?view=public&section=all
39435320,Jeff Dean: Trends in Machine Learning [video],,339,belter,1708379808,story,https://www.youtube.com/watch?v=oSCRZkSQ1CE
44682465,Show HN: Price Per Token – LLM API Pricing Data,"The LLM providers are constantly adding new models and updating their API prices. Anyone building AI applications knows that these prices are very important to their bottom line. The only place I am aware of is going to these provider&#x27;s individual website pages to check the price per token.<p>To solve this inconvenience I spent a few hours making pricepertoken.com which has the latest model&#x27;s up-to-date prices all in one place.<p>Thinking about adding image models too especially since you have multiple options (fal, replicate) to use the same model and the prices are not always the same.",339,alexellman,1753447181,story,https://pricepertoken.com/
41556519,Launch HN: Silurian (YC S24) – Simulate the Earth,"Hey HN! We’re Jayesh, Cris, and Nikhil, the team behind Silurian (<a href=""https:&#x2F;&#x2F;silurian.ai"">https:&#x2F;&#x2F;silurian.ai</a>). Silurian builds foundation models to simulate the Earth, starting with the weather.  Some of our recent hurricane forecasts can be visualized at <a href=""https:&#x2F;&#x2F;hurricanes2024.silurian.ai&#x2F;"">https:&#x2F;&#x2F;hurricanes2024.silurian.ai&#x2F;</a>.<p>What is it worth to know the weather forecast 1 day earlier? That’s not a hypothetical question, traditional forecasting systems have been improving their skill at a rate of 1 day per decade. In other words, today’s 6-day forecast is as accurate as the 5-day forecast ten years ago. No one expects this rate of improvement to hold steady, it has to slow down eventually, right? Well in the last couple years GPUs and modern deep learning have actually sped it up.<p>Since 2022 there has been a flurry of weather deep learning systems research at companies like NVIDIA, Google DeepMind, Huawei and Microsoft (some of them built by yours truly). These models have little to no built-in physics and learn to forecast purely from data. Astonishingly, this approach, done correctly, produces better forecasts than traditional simulations of the physics of our atmosphere.<p>Jayesh and Cris came face-to-face with this technology’s potential while they were respectively leading the [ClimaX](<a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.10343"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.10343</a>) and [Aurora](<a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.13063"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2405.13063</a>) projects at Microsoft. The foundation models they built improved on the ECMWF’s forecasts, considered the gold standard in weather prediction, while only using a fraction of the available training data. Our mission at Silurian is to scale these models to their full potential and push them to the limits of physical predictability. Ultimately, we aim to model all infrastructure that is impacted by weather including the energy grid, agriculture, logistics, and defense. Hence: simulate the Earth.<p>Before we do all that, this summer we’ve built our own foundation model, GFT (Generative Forecasting Transformer), a 1.5B parameter frontier model that simulates global weather up to 14 days ahead at approximately 11km resolution (<a href=""https:&#x2F;&#x2F;www.ycombinator.com&#x2F;launches&#x2F;Lcz-silurian-simulate-the-earth"">https:&#x2F;&#x2F;www.ycombinator.com&#x2F;launches&#x2F;Lcz-silurian-simulate-t...</a>). Despite the scarce amount of extreme weather data in historical records, we have seen that GFT is performing extremely well on predicting 2024 hurricane tracks (<a href=""https:&#x2F;&#x2F;silurian.ai&#x2F;posts&#x2F;001&#x2F;hurricane_tracks"">https:&#x2F;&#x2F;silurian.ai&#x2F;posts&#x2F;001&#x2F;hurricane_tracks</a>). You can play around with our hurricane forecasts at <a href=""https:&#x2F;&#x2F;hurricanes2024.silurian.ai"">https:&#x2F;&#x2F;hurricanes2024.silurian.ai</a>. We visualize these using [cambecc&#x2F;earth] (<a href=""https:&#x2F;&#x2F;github.com&#x2F;cambecc&#x2F;earth"">https:&#x2F;&#x2F;github.com&#x2F;cambecc&#x2F;earth</a>), one of our favorite open source weather visualization tools.<p>We’re excited to be launching here on HN and would love to hear what you think!",338,rejuvyesh,1726497158,story,
38557054,Meta's new AI image generator was trained on 1.1B Instagram and FB photos,,338,my12parsecs,1701961043,story,https://arstechnica.com/information-technology/2023/12/metas-new-ai-image-generator-was-trained-on-1-1-billion-instagram-and-facebook-photos/
41916322,USGS uses machine learning to show large lithium potential in Arkansas,,337,antidnan,1729616701,story,https://www.usgs.gov/news/national-news-release/unlocking-arkansas-hidden-treasure-usgs-uses-machine-learning-show-large
41162676,A new type of neural network is more interpretable,,337,pseudolus,1722874519,story,https://spectrum.ieee.org/kan-neural-network
40965892,Google's Gemini AI caught scanning Google Drive PDF files without permission,,336,thunderbong,1721028338,story,https://www.tomshardware.com/tech-industry/artificial-intelligence/gemini-ai-caught-scanning-google-drive-hosted-pdf-files-without-permission-user-complains-feature-cant-be-disabled
43431675,Apple shuffles AI executive ranks in bid to turn around Siri,,336,bbzjk7,1742529660,story,https://finance.yahoo.com/news/apple-shuffles-ai-executive-ranks-162500488.html
42710978,"Google is making AI in Gmail and Docs free, but raising the price of Workspace",,336,lars_francke,1736950517,story,https://www.theverge.com/2025/1/15/24343794/google-workspace-ai-features-free
44515403,MCP-B: A Protocol for AI Browser Automation,,336,miguelspizza,1752100637,story,https://mcp-b.ai/
41550907,The Cheating Device (ChatGPT on a TI-84) [video],,335,triyambakam,1726438522,story,https://www.youtube.com/watch?v=Bicjxl4EcJg
41465081,Effects of Gen AI on High Skilled Work: Experiments with Software Developers,,335,Anon84,1725621717,story,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566
44607838,Meta says it won't sign Europe AI agreement,,335,rntn,1752861397,story,https://www.cnbc.com/2025/07/18/meta-europe-ai-code.html
45979190,Larry Summers resigns from OpenAI board,"<a href=""https:&#x2F;&#x2F;www.nytimes.com&#x2F;2025&#x2F;11&#x2F;19&#x2F;technology&#x2F;larry-summers-resigns-from-openais-board.html"" rel=""nofollow"">https:&#x2F;&#x2F;www.nytimes.com&#x2F;2025&#x2F;11&#x2F;19&#x2F;technology&#x2F;larry-summers-...</a>, <a href=""https:&#x2F;&#x2F;archive.ph&#x2F;ASfq6"" rel=""nofollow"">https:&#x2F;&#x2F;archive.ph&#x2F;ASfq6</a>",334,koolba,1763558183,story,https://www.cnbc.com/2025/11/19/larry-summers-epstein-openai.html
40608269,How Does GPT-4o Encode Images?,,334,olooney,1717764867,story,https://www.oranlooney.com/post/gpt-cnn/
41707495,YC criticized for backing AI startup that simply cloned another AI startup,,333,blinding-streak,1727785675,story,https://techcrunch.com/2024/09/30/y-combinator-is-being-criticized-after-it-backed-an-ai-startup-that-admits-it-basically-cloned-another-ai-startup/
40941056,Physics-Based Deep Learning Book,,333,sebg,1720735810,story,https://physicsbaseddeeplearning.org/intro.html
45961886,Google boss says AI investment boom has 'elements of irrationality',,333,jillesvangurp,1763446012,story,https://www.bbc.com/news/articles/cwy7vrd8k4eo
42922989,Open Euro LLM: Open LLMs for Transparent AI in Europe,,331,joecobb,1738616204,story,https://openeurollm.eu/launch-press-release
40845304,My Python code is a neural network,,331,gnyeki,1719838058,story,https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/
40646909,Terence Tao on proof checkers and AI programs,,331,antineutrino,1718117798,story,https://www.scientificamerican.com/article/ai-will-become-mathematicians-co-pilot/
41648564,Launch HN: Haystack (YC S24) – Visualize and edit code on an infinite canvas,"Hi HN, we’re Akshay and Jake from Haystack (<a href=""https:&#x2F;&#x2F;haystackeditor.com&#x2F;"">https:&#x2F;&#x2F;haystackeditor.com&#x2F;</a>). Haystack re-imagines the IDE by putting it on a 2D digital whiteboard that automatically draws connections between code as you navigate and edit files. We designed it to match our mental model of how code works – as a graph of connected components. Here’s a demo of what it looks like: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5XADctpWNNs"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5XADctpWNNs</a>. You can download Haystack from <a href=""https:&#x2F;&#x2F;haystackeditor.com&#x2F;"">https:&#x2F;&#x2F;haystackeditor.com&#x2F;</a>.<p>We used to work as software engineers at big companies, and when the explosion in AI code-generation tools started we were really excited about them. They gave us a huge speed boost in our day-to-day work. But something was missing: writing code had become easier, but navigating codebases remained painful. We found that we were spending more time on the latter than the former!<p>Our vision of Haystack is an IDE that helps you navigate your codebase at speed. You should be able to type a vague description of the code you’re looking for (“Show me the code flow that triggers after we click the submit button”) and immediately see the right result. But we don’t think the traditional split-panel interface is the best fit for this type of rapid navigation. Inspired by the fluidity of design tools like Figma (Akshay used to work there!) and Miro, we realized that the canvas layout is a perfect way to represent and operate on code flows, and a great base for all the AI and collaborative tooling we want to build! This is how Haystack was born.<p>You might have seen our original Show HN (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41068719"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41068719</a>) about two months ago. Since then, a ton of our users have told us that they’d love to contribute. We’ve made the decision to make the editor source-available so that folks can make contributions and so that they can examine the code to make sure they can trust it. Our repo is at <a href=""https:&#x2F;&#x2F;github.com&#x2F;haystackeditor&#x2F;haystack-editor"">https:&#x2F;&#x2F;github.com&#x2F;haystackeditor&#x2F;haystack-editor</a>.<p>If you’re wondering why we’re going source-available rather than proper open-source: as a startup, we&#x27;re going to eventually monetize and it’s too early for us to make the decision on whether going fully open-source is the right move. We want to own the distribution of Haystack until we can better understand the ramifications that open-sourcing would have, and whether it’s sustainable business-wise. We’d love to find a way to make proper open-source work, but it’s a one-way door, so we want to take our time to make sure we’re making an informed decision.<p>We would love to hear about what you think about having a canvas in your IDE and the role visualizations have to play in software development!",328,akshaysg,1727278260,story,https://github.com/haystackeditor/haystack-editor
41936745,Launch HN: Skyvern (YC S23) – open-source AI agent for browser automations,"Hey HN, we’re Suchintan and Shu from Skyvern (<a href=""https:&#x2F;&#x2F;www.skyvern.com"">https:&#x2F;&#x2F;www.skyvern.com</a>). We’re building an open source tool to help companies automate browser-based workflows using LLMs.<p>Our open source repo is at <a href=""https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;Skyvern"">https:&#x2F;&#x2F;github.com&#x2F;Skyvern-AI&#x2F;Skyvern</a>, and we&#x27;re excited to share our cloud version with you (<a href=""https:&#x2F;&#x2F;app.skyvern.com"">https:&#x2F;&#x2F;app.skyvern.com</a>) :)<p>Skyvern allows you to define a single (or a series of) goal-based prompts to instruct an agent to complete complex tasks on websites. Here’s a quick demo of Skyvern: <a href=""https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;76b231309df74a528061fcf102e1967f"" rel=""nofollow"">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;76b231309df74a528061fcf102e1967f</a><p>We built this to solve a specific problem: building browser automations often requires companies to either hire people and scale out operations teams to do tedious manual work, or hire developers to use products like UI-Path or Selenium to build automations.<p>Code-based solutions always run into the same problem: they’re brittle (wow this website added a new pop-up dialog and my script broke), and fail to achieve the same objective across multiple websites (how can I fill out a contact-us form on hundreds of different websites?)<p>We did a Show HN a few months ago (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39706004"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39706004</a>), and  since then, we’ve onboarded customers for a wide variety of use cases: generating insurance quotes on websites like Geico.com; applying to jobs on websites like lever.co; automating filing permits in local government portals; registering new corporations for employment identification; fetching invoices from hundreds of different portals such as hydroone.com; automating purchasing on a handful of e-commerce websites like zooplus.com; and filling out contact us forms on a bunch of random smb websites (such as HVAC websites).<p>To be able to service all of these, we’ve built and open-sourced quite a few interesting features:<p>(1) a fully-featured React application allowing you to see every action Skyvern is taking in real-time;<p>(2) livestreaming browser instances to allow our users to see what Skyvern is doing when running inside of a docker container;<p>(3) authenticated sessions, integrating with Bitwarden and allowing users to specify Email + Phone + QR-code based 2FAs;<p>(4) “workflows” allowing users to chain multiple goal-based prompts together, which can handle tasks like invoice downloading, or automating purchasing pipelines;<p>(5) processing HTML Elements (ex. identifying + summarizing SVGs) and performing website interactions (ex. Iterating over dynamic autocompletes to fill in address information correctly)<p>(6) “cached workflows”, allowing Skyvern to memorize previous interactions (ie text inputs) and re-use them in future runs.<p>We’ve also been blessed with a few model advancements to solve some of the cost concerns the community brought up. Skyvern’s token costs went down 80% from $15 &#x2F; 1M tokens (GPT-4V) to $2.50 &#x2F; 1M tokens (GPT-4O)<p>Despite the model costs going down 80%, Skyvern is still quite expensive to run, so we give every new user $5 of credits to try it out and see if it can be useful for you.<p>We would be honored if you could give it a try at <a href=""https:&#x2F;&#x2F;app.skyvern.com"">https:&#x2F;&#x2F;app.skyvern.com</a> and share some feedback with us, and we look forward to any and all of your comments!",327,suchintan,1729785081,story,https://github.com/Skyvern-AI/Skyvern
45716296,ICE Will Use AI to Surveil Social Media,,327,throwaway81523,1761525812,story,https://jacobin.com/2025/10/ice-zignal-surveillance-social-media
41878281,Microsoft and OpenAI's close partnership shows signs of fraying,,326,jhunter1016,1729249870,story,https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html
45475529,ProofOfThought: LLM-based reasoning using Z3 theorem proving,<a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.17270"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2409.17270</a>,326,barthelomew,1759602863,story,https://github.com/DebarghaG/proofofthought
42043552,Show HN: Convert any website into a React component,"Hey HN, we built a Chrome Extension (<a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;extension"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;extension</a>) that converts a snippet of any website to an isolated React component.<p>Demo video: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;hutUYDkyE_A"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;hutUYDkyE_A</a><p>How it works: 1) Iterate through each node in the selected DOM tree, 2) For each element, find any matching CSS selectors &#x2F; inline styles, 3) Use window.getComputedStyle to get the deterministic values, 4) Construct JSX<p>It was pretty hard producing the minimal code necessary while maintaining the same visual look. To do this, we implemented things like abstracting out global styles, removing inherited styles, pulling out SVGs, deleting styles with no effect, and condensing styles into their shorthand properties.<p>We dive into each of those optimizations here for fun: <a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;blog&#x2F;any-website-to-react-component"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;blog&#x2F;any-website-to-react-comp...</a><p>One of the main reasons we cared so much about condensing down the styles was not only to make it more human-readable, but also to reduce context length for an LLM, so that you can iterate on it with AI. Our extension has a “convert” option that lets you convert the output to Tailwind, Shadcn, or Chakra UI using an LLM. You can also export to Figma.<p>We&#x27;re frontend engineers and we built the extension because our core product (<a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;</a>) helps software teams prototype their product ideas. And a huge pain point for users is getting their existing designs into our product, so that they can reference them to generate UIs with their existing aesthetic.<p>The extension allows you to get existing design context from any website, even localhost. Since launch, the extension has more than 3,000 users and interestingly is most popular in Japan.<p>Here&#x27;s some real examples if you&#x27;re curious what the final output looks like:<p>A) Hacker News Navbar - <a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;ac9f38e4-5ef0-49e5-8b80-dbc42951a00a"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;ac9f38e4-5ef0-49e5-8b80-dbc429...</a><p>B) ChatGPT Welcome Screen - <a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;7cb3ad12-cb12-4a5b-b32b-eda04de9ec01"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;7cb3ad12-cb12-4a5b-b32b-eda04d...</a><p>C) Cal.com Calendar Component — <a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;a43bac78-134d-458d-8107-811ac7b32b1f"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;a43bac78-134d-458d-8107-811ac7...</a><p>D) Stripe.com logo section - <a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;deff1793-7a05-42fe-97f7-945976cdbc7e"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;deff1793-7a05-42fe-97f7-945976...</a><p>If you have an opinion about the extension, we’re all ears! You can try it for free at: <a href=""https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;extension"">https:&#x2F;&#x2F;www.magicpatterns.com&#x2F;extension</a>",326,alexdanilowicz,1730739780,story,https://chromewebstore.google.com/detail/html-to-react-figma-by-ma/chgehghmhgihgmpmdjpolhkcnhkokdfp
42780022,Metacognitive laziness: Effects of generative AI on learning motivation,,326,freddier,1737467238,story,https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13544
42635942,Show HN: Stagehand – an open source browser automation framework powered by AI,"Hi HN! I’m Anirudh — longtime lurker, first time poster, and I couldn’t be more excited to show you Stagehand.<p>Stagehand is a TypeScript project that extends Playwright with three simple AI methods — act, extract, and observe. We’d love for you to try it out using the command below:<p><pre><code>    npx create-browser-app --example quickstart </code></pre> Here’s a sample workflow:<p><pre><code>    const stagehand = new Stagehand();     await stagehand.init();      &#x2F;&#x2F; Stagehand overrides the Playwright Page and Context classes     const { page, context } = stagehand      await page.goto(&quot;instadash.com&quot;) &#x2F;&#x2F; Regular Playwright      &#x2F;&#x2F; Take action on the page     await page.act({ action: &quot;click on taqueria cazadores&quot; })      &#x2F;&#x2F; Extract relevant data from the page     const { price } = await page.extract({         instruction: &quot;extract the price of the super burrito&quot;,         schema: z.object({             price: z.number()         })     })  </code></pre> We built Stagehand because we loved building browser automations using Playwright and Selenium, but we grew frustrated at how cumbersome it is to just get started and write simple browser automations. These frameworks, while incredibly powerful, are built for QA testing and are thus notoriously prone to fail if there are minor changes in the UI or underlying DOM structure.<p>The goal of Stagehand is twofold:<p>1. Make browser automations easier to write 2. Make browser automations more resilient to DOM changes.<p>We were super energized by what we’ve been seeing with vision-based computer use agents. We think with a browser, you can provide even richer data by leveraging the information in the DOM + a11y tree in addition to what’s rendered on the page. However, we didn’t want to go so far as to build an agent, since we wanted fine-grained control over each step that an agent can take.<p>Therefore, the happy medium we built was to extend the existing powerful functionalities of Playwright with simple and extensible AI APIs that return the decision-making power back to the developer at each step.<p>Check out our docs: <a href=""https:&#x2F;&#x2F;docs.stagehand.dev"" rel=""nofollow"">https:&#x2F;&#x2F;docs.stagehand.dev</a><p>We’d love for you to join and give us feedback on Slack as well: <a href=""https:&#x2F;&#x2F;stagehand.dev&#x2F;slack"" rel=""nofollow"">https:&#x2F;&#x2F;stagehand.dev&#x2F;slack</a>",326,hackgician,1736354464,story,https://github.com/browserbase/stagehand
43307159,Sidekick: Local-first native macOS LLM app,,325,volemo,1741507682,story,https://github.com/johnbean393/Sidekick
42544367,"OpenAI’s board, paraphrased: ‘All we need is unimaginable sums of money’",,325,ajuhasz,1735513596,story,https://daringfireball.net/2024/12/openai_unimaginable
43542259,Don’t let an LLM make decisions or execute business logic,,325,petesergeant,1743474880,story,https://sgnt.ai/p/hell-out-of-llms/
38629963,Dropbox: How to opt out of 3rd party AI partner access to your Dropbox,,324,tosh,1702485850,story,https://twitter.com/Werner/status/1734890651378975007
40508390,What We Learned from a Year of Building with LLMs,,324,7d7n,1716956056,story,https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/
43680899,DolphinGemma: How Google AI is helping decode dolphin communication,,324,alphabetting,1744636320,story,https://blog.google/technology/ai/dolphingemma/
42129064,A Student's Guide to Writing with ChatGPT,,323,timbilt,1731525986,story,https://openai.com/chatgpt/use-cases/student-writing-guide/
40266728,Microsoft CTO: Thoughts on OpenAI (2019),,323,mfiguiere,1714931412,story,https://twitter.com/techemails/status/1787176471146156193
45108401,"Apertus 70B: Truly Open - Swiss LLM by ETH, EPFL and CSCS",,323,denysvitali,1756844091,story,https://huggingface.co/swiss-ai/Apertus-70B-2509
38661296,Advancements in machine learning for machine learning,,323,atg_abhishek,1702695047,story,https://blog.research.google/2023/12/advancements-in-machine-learning-for.html
42156516,YC is wrong about LLMs for chip design,,322,laserduck,1731766037,story,https://www.zach.be/p/yc-is-wrong-about-llms-for-chip-design
40429200,Show HN: I built a game to help you learn neural network architectures,,321,sabrina_ramonov,1716302862,story,https://graphgame.sabrina.dev/
44617172,It's rude to show AI output to people,,321,distantprovince,1752944258,story,https://distantprovince.by/posts/its-rude-to-show-ai-output-to-people/
40272339,Show HN: AI climbing coach – visualize how to climb any route based on your body,"I made SABR - an AI model that helps you visualize the beta&#x2F;technique on any route, based on your body parameters. You can input a video of you climbing any route, in any orientation or lighting condition (it&#x27;s truly versatile!). SABR then creates a virtual avatar of your body shape and uses it to climb the route you&#x27;re climbing. Then, you can compare&#x2F;contrast.<p>You can see the demo here: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cnvNPWoYZz4"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=cnvNPWoYZz4</a><p>Will be open sourcing the model, backend, and frontend codebase soon!",320,smandava,1714982978,story,https://climbing.ai/
44044367,Google AI Ultra,,320,mfiguiere,1747765247,story,https://blog.google/products/google-one/google-ai-ultra/
42817439,"Show HN: Lightpanda, an open-source headless browser in Zig","We’re Francis and Pierre, and we&#x27;re excited to share Lightpanda (<a href=""https:&#x2F;&#x2F;lightpanda.io"" rel=""nofollow"">https:&#x2F;&#x2F;lightpanda.io</a>), an open-source headless browser we’ve been building for the past 2 years from scratch in Zig (not dependent on Chromium or Firefox). It’s a faster and lighter alternative for headless operations without any graphical rendering.<p>Why start over? We’ve worked a lot with Chrome headless at our previous company, scraping millions of web pages per day. While it’s powerful, it’s also heavy on CPU and memory usage. For scraping at scale, building AI agents, or automating websites, the overheads are high. So we asked ourselves: what if we built a browser that only did what’s absolutely necessary for headless automation?<p>Our browser is made of the following main components:<p>- an HTTP loader<p>- an HTML parser and DOM tree (based on Netsurf libs)<p>- a Javascript runtime (v8)<p>- partial web APIs support (currently DOM and XHR&#x2F;Fetch)<p>- and a CDP (Chrome Debug Protocol) server to allow plug &amp; play connection with existing scripts (Puppeteer, Playwright, etc).<p>The main idea is to avoid any graphical rendering and just work with data manipulation, which in our experience covers a wide range of headless use cases (excluding some, like screenshot generation).<p>In our current test case Lightpanda is roughly 10x faster than Chrome headless while using 10x less memory.<p>It&#x27;s a work in progress, there are hundreds of Web APIs, and for now we just support some of them. It&#x27;s a beta version, so expect most websites to fail or crash. The plan is to increase coverage over time.<p>We chose Zig for its seamless integration with C libs and its <i>comptime</i> feature that allow us to generate bi-directional Native to JS APIs (see our zig-js-runtime lib <a href=""https:&#x2F;&#x2F;github.com&#x2F;lightpanda-io&#x2F;zig-js-runtime"">https:&#x2F;&#x2F;github.com&#x2F;lightpanda-io&#x2F;zig-js-runtime</a>). And of course for its performance :)<p>As a company, our business model is based on a Managed Cloud, browser as a service. Currently, this is primarily powered by Chrome, but as we integrate more web APIs it will gradually transition to Lightpanda.<p>We would love to hear your thoughts and feedback. Where should we focus our efforts next to support your use cases?",319,fbouvier,1737756932,story,https://github.com/lightpanda-io/browser
43774990,Teaching LLMs how to solid model,,319,wgpatrick,1745432023,story,https://willpatrick.xyz/technology/2025/04/23/teaching-llms-how-to-solid-model.html
44482504,"Opencode: AI coding agent, built for the terminal",,319,indigodaddy,1751822807,story,https://github.com/sst/opencode
44293988,OpenAI wins $200M U.S. defense contract,,319,erikrit,1750113118,story,https://www.cnbc.com/2025/06/16/openai-wins-200-million-us-defense-contract.html
42531830,Ask HN: Are you unable to find employment?,"I am seeing many anecdotal experiences shared online on various platforms stating that it is difficult to find employment in tech. I myself have had a difficult time landing an interview over the last year despite having two decades of experience.<p>I am attempting to gain some insight into the issue. My situation is somewhat unique in that I am self-taught without a CS degree. I&#x27;m a very experienced, diligent worker, etc, but an algorithm doesn&#x27;t care about this and so getting through the filters is difficult.<p>However I see many discussions being posted (primarily on X) stating that it is nearly impossible for people with CS degrees (especially white males) to get an interview let alone a job. There have been mass layoffs, less money being invested etc.  Many people have claimed AI is taking jobs, or that there aren&#x27;t as many jobs available, yet at the same time, Elon Musk and others claim there is an engineer shortage and we must increase the number of H-1B visas in order to fill this gap. When I apply to a position on linkedin I can see that even the most Jr positions have over 100 applicants.<p>I know that X can be slanted, and really anything posted online must be taken with a grain of salt - but I&#x27;m seeing many people claiming to be in the same situation as myself, and most of them claim to be white males.<p>Furthermore, in the last two years I experienced two layoffs. In both situations it was white males let go in favor of Indian and KZ foreigners. Again - this is anecdotal and could be a coincidence, but its awfully telling that Vivek and Elon are calling American tech workers uncultured, lazy and stupid in the wake of these experiences and those that I&#x27;ve read about online.<p>I don&#x27;t want to start a war here on hackernews, but I&#x27;m looking for people&#x27;s personal experiences. Do they match up? Are you having a hard time finding employment? Have you been fired in favor of foreign workers? Is this racism &#x2F; ageism &#x2F; sexism at play or is that being overblown by political actors?",319,vbi8iBEX,1735400882,story,
44463967,Show HN: I AI-coded a tower defense game and documented the whole process,"I&#x27;m a software developer with 20+ years of experience but during this time I never programmed any games, but I really wanted to for the longest time. With the advent of AI coding agents I thought that this is the best time to try and so I&#x27;ve learned a bit of Phaser.js (a Javascript based game engine) and entered Beginner&#x27;s Jam Summer 2025 - a game jam for beginners in the game dev industry that allows AI coding. After around 25-30 hours (working mainly after my full-time day job) I managed to submit the game I called &quot;Tower of Time&quot; (the theme of the jam was &quot;Time Travel&quot;).<p>You can play it in your browser here: <a href=""https:&#x2F;&#x2F;m4v3k.itch.io&#x2F;tower-of-time"" rel=""nofollow"">https:&#x2F;&#x2F;m4v3k.itch.io&#x2F;tower-of-time</a><p>The goal of this project for me was first and foremost to see if AI coding is good enough to help me with creating something that&#x27;s actually fun to play and to my delight is turns out the answer is yes! I decided to document the whole process for myself and others to learn from my mistakes, so both the code AND all the prompts I used are published on GitHub (see submission link). The art assets are largely taken from itch.io artists who shared them for free, with some slight touch ups. Sounds came from freesound.org.<p>I&#x27;ve also streamed parts of the process, you can watch me working on the final stretch and submitting the finished game (warning, it&#x27;s 5+ hours long):<p><a href=""https:&#x2F;&#x2F;www.twitch.tv&#x2F;videos&#x2F;2503428478"" rel=""nofollow"">https:&#x2F;&#x2F;www.twitch.tv&#x2F;videos&#x2F;2503428478</a><p>During this process I&#x27;ve learned a lot and I want to use this knowledge in my next project that will hopefully be more ambitious. If you have any comments or questions I&#x27;m here to answer!",319,M4v3R,1751632489,story,https://github.com/maciej-trebacz/tower-of-time-game
45571423,My trick for getting consistent classification from LLMs,,318,frenchmajesty,1760378467,story,https://verdik.substack.com/p/how-to-get-consistent-classification
41481852,Serving AI from the Basement – 192GB of VRAM Setup,,318,XMasterrrr,1725817664,story,https://ahmadosman.com/blog/serving-ai-from-basement/
42796496,Show HN: Trolling SMS spammers with Ollama,"I&#x27;ve been working on a side project to generate responses to spam with various funny LLM personas, such as a millenial gym bro and a 19th century British gentleman. By request, I&#x27;ve made a write-up on my website which has some humorous screenshots and made the code available on Github for others to try out [0].<p>A brief outline of the system:<p>- Android app listens for incoming SMS events and forwards them over MQTT to a server running Ollama which generates responses - Conversations are whitelisted and manually assigned a persona.  The LLM has access to the last N messages of the conversation for additional context.<p>[0]: <a href=""https:&#x2F;&#x2F;github.com&#x2F;evidlo&#x2F;sms_llm"">https:&#x2F;&#x2F;github.com&#x2F;evidlo&#x2F;sms_llm</a><p>I&#x27;m aware that replying can encourage&#x2F;allow the sender to send more spam.  Hopefully reporting the numbers after the conversation is a reasonable compromise.",318,Evidlo,1737573828,story,https://evan.widloski.com/software/sms_llm/
39887931,Upscayl – Free and Open Source AI Image Upscaler,,318,faebi,1711917924,story,https://github.com/upscayl/upscayl
39238666,JetBrains' unremovable AI assistant meets irresistible outcry,,317,cannibalXxx,1706950760,story,https://www.theregister.com/2024/02/01/jetbrains_unremovable_ai_assistant/
38643046,The AI Trust Crisis,,317,simonw,1702570956,story,https://simonwillison.net/2023/Dec/14/ai-trust-crisis/
43791992,Show HN: I used OpenAI's new image API for a personalized coloring book service,"I&#x27;ve had an idea for a long time to generate a cute coloring book based on family photos, send it to a printing service, and then deliver it to people.<p>Last month, when OpenAI&#x27;s Sora was released for public use I (foolishly) thought I&#x27;d manually drag-and-drop each order’s photos into Sora&#x27;s UI and copy the resulting images back into my system. This took way too much time (about an hour for each of the few books I made and tested with family and friends). It clearly wasn&#x27;t possible to release this version because I’d be losing a huge amount of time on every order. So instead, I decided I&#x27;d finish off the project as best I could, put it &quot;on ice,&quot; and wait for the API release.<p>The API is now released (quicker than I thought it&#x27;d be, too!) and I integrated it last night. I&#x27;d love your feedback on any and all aspects.<p>The market is mostly family-based, but from my testing of the physical book I&#x27;ve found that both adults and kids enjoy coloring them in (it&#x27;s surprisingly cathartic and creative). If you would like to order one you can get 10% off by tapping the total price line item five times.",316,darajava,1745575539,story,https://clevercoloringbook.com/
39349992,Neural network training makes beautiful fractals,,316,telotortium,1707769547,story,https://sohl-dickstein.github.io/2024/02/12/fractal.html
40805010,Show HN: Voice bots with 500ms response times,"Last year when GPT-4 was released I started making lots of little voice + LLM experiments. Voice interfaces are fun; there are several interesting new problem spaces to explore.<p>I&#x27;m convinced that voice is going to be a bigger and bigger part of how we all interact with generative AI. But one thing that&#x27;s hard, today, is building voice bots that respond as quickly as humans do in conversation. A 500ms voice-to-voice response time is just <i>barely</i> possible with today&#x27;s AI models.<p>You can get down to 500ms if you: host transcription, LLM inference, and voice generation all together in one place; are careful about how you route and pipeline all the data; and the gods of both wifi and vram caching smile on you.<p>Here&#x27;s a demo of a 500ms-capable voice bot, plus a container you can deploy to run it yourself on an A10&#x2F;A100&#x2F;H100 if you want to:<p><a href=""https:&#x2F;&#x2F;fastvoiceagent.cerebrium.ai&#x2F;"">https:&#x2F;&#x2F;fastvoiceagent.cerebrium.ai&#x2F;</a><p>We&#x27;ve been collecting lots of metrics. Here are typical numbers (in milliseconds) for all the easily measurable parts of the voice-to-voice response cycle.<p><pre><code>  macOS mic input                 40   opus encoding                   30   network stack and transit       10   packet handling                  2   jitter buffer                   40   opus decoding                   30   transcription and endpointing  200   llm ttfb                       100   sentence aggregation          100   tts ttfb                        80   opus encoding                   30   packet handling                  2   network stack and transit       10   jitter buffer                   40   opus decoding                   30   macOS speaker output           15   ----------------------------------   total ms                       759 </code></pre> Everything in AI is changing all the time. LLMs with native audio input and output capabilities will likely make it easier to build fast-responding voice bots soon. But for the moment, I think this is the fastest possible approach&#x2F;tech stack.",315,kwindla,1719438663,story,https://fastvoiceagent.cerebrium.ai/
43694877,OpenAI is building a social network?,,315,noleary,1744733309,story,https://www.theverge.com/openai/648130/openai-social-network-x-competitor
44178468,"Cloud Run GPUs, now GA, makes running AI workloads easier for everyone",,315,mariuz,1749025726,story,https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available
45418428,California governor signs AI transparency bill into law,<a href=""https:&#x2F;&#x2F;sb53.info&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;sb53.info&#x2F;</a>,315,raldi,1759177994,story,https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/
41659324,"DoNotPay has to pay $193K for falsely touting untested AI lawyer, FTC says",,315,Brajeshwar,1727363624,story,https://arstechnica.com/tech-policy/2024/09/startup-behind-worlds-first-robot-lawyer-to-pay-193k-for-false-ads-ftc-says/
44329457,Show HN: Nxtscape – an open-source agentic browser,"Hi HN - we&#x27;re Nithin and Nikhil, twin brothers and founders of nxtscape.ai (YC S24). We&#x27;re building Nxtscape (&quot;next-scape&quot;) - an open-source, agentic browser for the AI era.<p>-- Why bother building a new browser? For the first time since Netscape was released in 1994, it feels like we can reimagine browsers from scratch for the age of AI agents. The web browser of tomorrow might not look like what we have today.<p>We saw how tools like Cursor gave developers a 10x productivity boost, yet the browser—where everyone else spends their entire workday—hasn&#x27;t fundamentally changed.<p>And honestly, we feel like we&#x27;re constantly fighting the browser we use every day. It&#x27;s not one big thing, but a series of small, constant frustrations. I&#x27;ll have 70+ tabs open from three different projects and completely lose my train of thought. And simple stuff like reordering tide pods from amazon or filling out forms shouldn&#x27;t need our full attention anymore. AI can handle all of this, and that&#x27;s exactly what we&#x27;re building.<p>Here’s a demo of our early version <a href=""https:&#x2F;&#x2F;dub.sh&#x2F;nxtscape-demo"" rel=""nofollow"">https:&#x2F;&#x2F;dub.sh&#x2F;nxtscape-demo</a><p>-- What makes us different We know others are exploring this space (Perplexity, Dia), but we want to build something open-source and community-driven. We&#x27;re not a search or ads company, so we can focus on being privacy-first – Ollama integration, BYOK (Bring Your Own Keys), ad-blocker.<p>Btw we love what Brave started and stood for, but they&#x27;ve now spread themselves too thin across crypto, search, etc. We are laser-focused on one thing: making browsers work for YOU with AI. And unlike Arc (which we loved too but got abandoned), we&#x27;re 100% open source. Fork us if you don&#x27;t like our direction.<p>-- Our journey hacking a new browser To build this, we had to fork Chromium. Honestly, it feels like the only viable path today—we&#x27;ve seen others like Brave (started with electron) and Microsoft Edge learn this the hard way.<p>We also started with why not just build an extension. But realized we needed more control. Similar to the reason why Cursor forked VSCode. For example, Chrome has this thing called the Accessibility Tree - basically a cleaner, semantic version of the DOM that screen readers use. Perfect for AI agents to understand pages, but you can&#x27;t use it through extension APIs.<p>That said, working with the 15M-line C++ chromium codebase has been an adventure. We&#x27;ve both worked on infra at Google and Meta, but Chromium is a different beast. Tools like Cursor&#x27;s indexing completely break at this scale, so we&#x27;ve had to get really good with grep and vim. And the build times are brutal—even with our maxed-out M4 Max MacBook, a full build takes about 3 hours.<p>Full disclosure: we are still very early, but we have a working prototype on GitHub. It includes an early version of a &quot;local Manus&quot; style agent that can automate simple web tasks, plus an AI sidebar for questions, and other productivity features (grouping tabs, saving&#x2F;resuming sessions, etc.).<p>Looking forward to any and all comments!<p>You can download the browser from our github page: <a href=""https:&#x2F;&#x2F;github.com&#x2F;nxtscape&#x2F;nxtscape"">https:&#x2F;&#x2F;github.com&#x2F;nxtscape&#x2F;nxtscape</a>",314,felarof,1750437355,story,https://github.com/nxtscape/nxtscape
41448439,Show HN: An open-source implementation of AlphaFold3,"Hi HN - we’re the founders of Ligo Biosciences and are excited to share an open-source implementation of AlphaFold3, the frontier model for protein structure prediction.<p>Google DeepMind and their new startup Isomorphic Labs, are expanding into drug discovery. They developed AlphaFold3 as their model to accelerate drug discovery and create demand from big pharma. They already signed Novartis and Eli Lilly for $3 billion - Google’s becoming a pharma company! (<a href=""https:&#x2F;&#x2F;www.isomorphiclabs.com&#x2F;articles&#x2F;isomorphic-labs-kicks-off-2024-with-two-pharmaceutical-collaborations"" rel=""nofollow"">https:&#x2F;&#x2F;www.isomorphiclabs.com&#x2F;articles&#x2F;isomorphic-labs-kick...</a>)<p>AlphaFold3 is a biomolecular structure prediction model that can do three main things: (1) Predict the structure of proteins; (2) Predict the structure of drug-protein interactions; (3) Predict nucleic acid - protein complex structure.<p>AlphaFold3 is incredibly important for science because it vastly accelerates the mapping of protein structures. It takes one PhD student their entire PhD to do one structure. With AlphaFold3, you get a prediction in minutes on par with experimental accuracy.<p>There’s just one problem: when DeepMind published AlphaFold3 in May (<a href=""https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-024-07487-w"" rel=""nofollow"">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-024-07487-w</a>), there was no code. This  brought up questions about reproducibility (<a href=""https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-024-01463-0"" rel=""nofollow"">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;d41586-024-01463-0</a>) as well as complaints from the scientific community (<a href=""https:&#x2F;&#x2F;undark.org&#x2F;2024&#x2F;06&#x2F;06&#x2F;opinion-alphafold-3-open-source&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;undark.org&#x2F;2024&#x2F;06&#x2F;06&#x2F;opinion-alphafold-3-open-sourc...</a>).<p>AlphaFold3 is a fundamental advance in structure modeling technology that the entire biotech industry deserves to be able to reap the benefits from. Its applications are vast, including:<p>- CRISPR gene editing technologies, where scientists can see exactly how the DNA interacts with the scissor Cas protein;<p>- Cancer research - predicting how a potential drug binds to the cancer target. One of the highlights in DeepMind’s paper is the prediction of a clinical KRAS inhibitor in complex with its target.<p>- Antibody &#x2F; nanobody to target predictions. AlphaFold3 improves accuracy on this class of molecules 2 fold compared to the next best tool.<p>Unfortunately, no companies can use it since it is under a non-commercial license!<p>Today we are releasing the full model trained on single chain proteins (capability 1 above), with the other two capabilities to be trained and released soon. We also include the training code. Weights will be released once training and benchmarking is complete. We wanted this to be truly open source so we used the Apache 2.0 license.<p>Deepmind published the full structure of the model, along with each components’ pseudocode in their paper. We translated this fully into PyTorch, which required more reverse engineering than we thought!<p>When building the initial version, we discovered multiple issues in DeepMind’s paper that would interfere with the training - we think the deep learning community might find these especially interesting. (Diffusion folks, we would love feedback on this!) These include:<p>- MSE loss scaling differs from Karras et al. (2022). The weighting provided in the paper does not downweigh the loss at high noise levels.<p>- Omission of residual layers in the paper - we add these back and see benefits in gradient flow and convergence. Anyone have any idea why Deepmind may have omitted the residual connections in the DiT blocks?<p>- The MSA module, in its current form, has dead layers. The last pair weighted averaging and transition layers cannot contribute to the pair representation, hence no grads. We swap the order to the one in the ExtraMsaStack in AlphaFold2. An alternative solution would be to use weight sharing, but whether this is done is ambiguous in the paper.<p>More about those issues here: <a href=""https:&#x2F;&#x2F;github.com&#x2F;Ligo-Biosciences&#x2F;AlphaFold3"">https:&#x2F;&#x2F;github.com&#x2F;Ligo-Biosciences&#x2F;AlphaFold3</a><p>How this came about: we are building Ligo (YC S24), where we are using ideas from AlphaFold3 for enzyme design. We thought open sourcing it was a nice side quest to benefit the community.<p>For those on Twitter, there was a good thread a few days ago that has more information:  <a href=""https:&#x2F;&#x2F;twitter.com&#x2F;ArdaGoreci&#x2F;status&#x2F;1830744265007480934"" rel=""nofollow"">https:&#x2F;&#x2F;twitter.com&#x2F;ArdaGoreci&#x2F;status&#x2F;1830744265007480934</a>.<p>A few shoutouts:  A huge thanks to OpenFold for pioneering the previous open source implementation of AlphaFold We did a lot of our early prototyping with proteinFlow developed by Lisa at AdaptyvBio we also look forward to partnering with them to bring you the next versions! We are also partnering with Basecamp Research to supply this model with the best sequence data known to science.  Matthew Clark (<a href=""https:&#x2F;&#x2F;batisio.co.uk"" rel=""nofollow"">https:&#x2F;&#x2F;batisio.co.uk</a>) for his amazing animations!<p>We’re around to answer questions and look forward to hearing from you!",314,EdHarris,1725471857,story,https://github.com/Ligo-Biosciences/AlphaFold3
44321672,Compiling LLMs into a MegaKernel: A path to low-latency inference,,314,matt_d,1750360854,story,https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17
44387828,Launch HN: Issen (YC F24) – Personal AI language tutor,"Hey HN, we&#x27;re Mariano and Anton from ISSEN (<a href=""https:&#x2F;&#x2F;issen.com"">https:&#x2F;&#x2F;issen.com</a>), a foreign language voice tutor app that adapts to your interests, goals, and needs.<p>Demo: <a href=""https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;a78e713d46934857a2dc88aed1bb100d?sid=ec4dc1f3-1a4f-4d75-8e87-efbb3d2b2e90"" rel=""nofollow"">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;a78e713d46934857a2dc88aed1bb100d?...</a><p>We started this company after struggling to find great tools to practice speaking Japanese and French. Having a tutor can be awesome, but there are downsides: they can be expensive (since you pay by the hour), difficult to schedule, and have a high upfront cost (finding a tutor you like often forces you to cycle through a few that you don’t).<p>We wanted something that would talk with us — realistically, in full conversations — and actually help us improve. So we built it ourselves. The app relies on a custom voice AI pipeline combining STT (speech-to-text), TTS (text-to-speech), LLMs, long term memory, interruptions, turn-taking, etc. Getting speech-to-text to work well for learners was one of the hardest parts — especially with accents, multi-lingual sentences, and noisy environments. We now combine Gemini Flash, Whisper, Scribe, and GPT-4o-transcribe to minimize errors and keep the conversation flowing.<p>We didn’t want to focus too much on gamification. In our experience, that leads to users performing well in the app, achieving long streaks and so on, without actually getting fluent in the language you&#x27;re wanting to learn.<p>With ISSEN you instantly speak and immerse yourself in the language, which, while not easy, is a much more efficient way to learn.<p>We combine this with a word bank and SRS flashcards for new words learned in the AI voice chats, which allows very rapid improvement in both vocabulary and speaking skills. We also create custom curriculums for each student based on goals, interests, and preferences, and fully customizable settings like speed, turn taking, formality, etc.<p>App: <a href=""https:&#x2F;&#x2F;issen.com"">https:&#x2F;&#x2F;issen.com</a> (works on web, iOS, Android) Pricing: 20 min free trial, $20–29&#x2F;month (depending on duration and specific geography)<p>We’d love your feedback — on the tech, the UX, or what you’d wish from a tool like this. Thanks!",313,mariano54,1750948348,story,
43793280,The Policy Puppetry Attack: Novel bypass for major LLMs,,313,jacobr1,1745587097,story,https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/
42498634,Making AMD GPUs competitive for LLM inference (2023),,313,plasticchris,1734999438,story,https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference
38974802,On Sleeper Agent LLMs,,312,admp,1705097402,story,https://twitter.com/karpathy/status/1745921205020799433
44595811,All AI models might be the same,,311,jxmorris12,1752773329,story,https://blog.jxmo.io/p/there-is-only-one-model
42573875,Ask HN: Where to Work After 40?,"I turned 40 last month and spent the past decade working on my own startup that ultimately failed. I&#x27;m now trying to figure out the next step. Someone once said to me, Google is the place you go to retire after 40. I&#x27;ve done my time at various startups, and spent some time at Google. As an engineer the landscape of things is always changing and we&#x27;ve now moved from Cloud to AI pretty rapidly. I&#x27;m just curious to know what moves people made after 40 and what worked for them.",311,asim,1735821184,story,
45730094,Poker Tournament for LLMs,,311,SweetSoftPillow,1761637338,story,https://pokerbattle.ai/event
43569190,"Show HN: OpenNutrition – A free, public nutrition database","Hi HN!<p>Today I’m excited to launch OpenNutrition: a free, ODbL-licenced nutrition database of everyday generic, branded, and restaurant foods, a search engine that can browse the web to import new foods, and a companion app that bundles the database and search as a free macro tracking app.<p>Consistently logging the foods you eat has been shown to support long-term health outcomes (1)(2), but doing so easily depends on having a large, accurate, and up-to-date nutrition database. Free, public databases are often out-of-date, hard to navigate, and missing critical coverage (like branded restaurant foods). User-generated databases can be unreliable or closed-source. Commercial databases come with ongoing, often per-seat licensing costs, and usage restrictions that limit innovation.<p>As an amateur powerlifter and long-term weight loss maintainer, helping others pursue their health goals is something I care about deeply. After exiting my previous startup last year, I wanted to investigate the possibility of using LLMs to create the database and infrastructure required to make a great food logging app that was cost engineered for free and accessible distribution, as I believe that the availability of these tools is a public good. That led to creating the dataset I’m releasing today; nutritional data is public record, and its organization and dissemination should be, too.<p>What’s in the database?<p>- 5,287 common everyday foods, 3,836 prepared and generic restaurant foods, and 4,182 distinct menu items from ~50 popular US restaurant chains; foods have standardized naming, consistent numeric serving sizes, estimated micronutrient profiles, descriptions, and citations&#x2F;groundings to USDA, AUSNUT, FRIDA, CNF, etc, when possible.<p>- 313,442 of the most popular US branded grocery products with standardized naming, parsed serving sizes, and additive&#x2F;allergen data, grounded in branded USDA data; the most popular 1% have estimated micronutrient data, with the goal of full coverage.<p>Even the largest commercial databases can be frustrating to work with when searching for foods or customizations without existing coverage. To solve this, I created a real-time version of the same approach used to build the core database that can browse the web to learn about new foods or food customizations if needed (e.g., a highly customized Starbucks order). There is a limited demo on the web, and in-app you can log foods with text search, via barcode scan, or by image, all of which can search the web to import foods for you if needed. Foods discovered via these searches are fed back into the database, and I plan to publish updated versions as coverage expands.<p>- Search &amp; Explore: <a href=""https:&#x2F;&#x2F;www.opennutrition.app&#x2F;search"" rel=""nofollow"">https:&#x2F;&#x2F;www.opennutrition.app&#x2F;search</a><p>- Methodology&#x2F;About: <a href=""https:&#x2F;&#x2F;www.opennutrition.app&#x2F;about"" rel=""nofollow"">https:&#x2F;&#x2F;www.opennutrition.app&#x2F;about</a><p>- Get the iOS App: <a href=""https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;opennutrition-macro-tracker&#x2F;id6670272666"">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;opennutrition-macro-tracker&#x2F;id...</a><p>- Download the dataset: <a href=""https:&#x2F;&#x2F;www.opennutrition.app&#x2F;download"" rel=""nofollow"">https:&#x2F;&#x2F;www.opennutrition.app&#x2F;download</a><p>OpenNutrition’s iOS app offers free essential logging and a limited number of agentic searches, plus expenditure tracking and ongoing diet recommendations like best-in-class paid apps. A paid tier ($49&#x2F;year) unlocks additional searches and features (data backup, prioritized micronutrient coverage for logged foods), and helps fund further development and broader library coverage.<p>I’d love to hear your feedback, questions, and suggestions—whether it’s about the database itself, a really great&#x2F;bad search result, or the app.<p>1. Burke et al., 2011, <a href=""https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3268700&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC3268700&#x2F;</a><p>2. Patel et al., 2019, <a href=""https:&#x2F;&#x2F;mhealth.jmir.org&#x2F;2019&#x2F;2&#x2F;e12209&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;mhealth.jmir.org&#x2F;2019&#x2F;2&#x2F;e12209&#x2F;</a>",311,joshdickson,1743686345,story,https://www.opennutrition.app/search
44878650,Show HN: Omnara – Run Claude Code from anywhere,"Hey ya’ll, Ishaan and Kartik here. We&#x27;re building Omnara (<a href=""https:&#x2F;&#x2F;omnara.com&#x2F;"">https:&#x2F;&#x2F;omnara.com&#x2F;</a>), an “agent command center” that lets you launch and control Claude Code from anywhere: terminal, web, or mobile — and easily switch between them.<p>Run &#x27;pip install omnara &amp;&amp; omnara&#x27;, and you&#x27;ll have a regular Claude Code session. But you can continue that same session from our web dashboard (<a href=""https:&#x2F;&#x2F;omnara.com&#x2F;"">https:&#x2F;&#x2F;omnara.com&#x2F;</a>) or mobile app (<a href=""https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;omnara-ai-command-center&#x2F;id6748426727"">https:&#x2F;&#x2F;apps.apple.com&#x2F;us&#x2F;app&#x2F;omnara-ai-command-center&#x2F;id674...</a>).<p>Check out a demo here: <a href=""https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;03d30efcf8e44035af03cbfebf840c73"" rel=""nofollow"">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;03d30efcf8e44035af03cbfebf840c73</a>.<p>Before Omnara, we felt stuck watching Claude Code think and write code, waiting 5-10 minutes just to provide input when needed. Now with Omnara, I can start a Claude Code session and if I need to leave my laptop, I can respond from my phone anywhere. Some places I&#x27;ve coded from include my bed, on a walk, in an Uber, while doing laundry, and even on the toilet.<p>There are many new Claude Code wrappers (e.g., Crystal, Conductor), but none keep the native Claude Code terminal experience while allowing interaction outside the terminal, especially on mobile. On the other hand, tools like Vibetunnel or Termius replicate the terminal experience but lack push notifications, clean UIs for answering questions or viewing git diffs, and easy setup.<p>We wanted our integration to fully mirror the native Claude Code experience, including terminal output, permissions, notifications, and mode switching. The Claude Code SDK and hooks don&#x27;t support all of this, so we made a CLI wrapper that parses the session file at ~&#x2F;.claude&#x2F;projects and the terminal output to capture user and agent messages. We send these messages to our platform, where they&#x27;re displayed in the web and mobile apps in real time via SSE. Our CLI wrapper monitors for input from both the Omnara platform and the Claude Code CLI, continuing execution when the user responds from either location. Our entire backend is open source: <a href=""https:&#x2F;&#x2F;github.com&#x2F;omnara-ai&#x2F;omnara"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;omnara-ai&#x2F;omnara</a>.<p>Omnara isn&#x27;t just for Claude Code. It&#x27;s a general framework for any AI agent to send messages and push notifications to humans when they need input. For example, I&#x27;ve been using it as a human-in-the-loop node in n8n workflows for replying to emails. But every Claude Code user we show it to gets excited about that application specifically so that’s why we’re launching that first :)<p>Omnara is free for up to 10 agent sessions per month, then $9&#x2F;month for unlimited sessions. Looking forward to your feedback and hearing your thoughts and comments!",310,kmansm27,1755016418,story,https://github.com/omnara-ai/omnara
41105881,A Visual Guide to LLM Quantization,,310,raymond_goo,1722310930,story,https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization
41811078,Lm.rs: Minimal CPU LLM inference in Rust with no dependency,,310,littlestymaar,1728665214,story,https://github.com/samuel-vitorino/lm.rs
39852219,I scraped all of OpenAI's Community Forum,,310,alt-glitch,1711637073,story,https://julep-ai.github.io/
38593616,Mistral AI Valued at $2B,,310,marban,1702232756,story,https://www.unite.ai/paris-based-startup-and-openai-competitor-mistral-ai-valued-at-2-billion/
39915594,PyTorch Library for Running LLM on Intel CPU and GPU,,308,ebalit,1712140105,story,https://github.com/intel-analytics/ipex-llm
44617078,Local LLMs versus offline Wikipedia,,308,EvanHahn,1752943742,story,https://evanhahn.com/local-llms-versus-offline-wikipedia/
44934337,When did AI take over Hacker News?,,308,zachperkel,1755459935,story,https://zachperk.com/blog/when-did-ai-take-over-hn
40677424,Microsoft to delay release of Recall AI feature on security concerns,,308,mfiguiere,1718338444,story,https://www.reuters.com/technology/artificial-intelligence/microsoft-delay-release-recall-ai-feature-security-concerns-2024-06-14/
42273817,Core copyright violation moves ahead in The Intercept's lawsuit against OpenAI,,307,giuliomagnifico,1732888092,story,https://www.niemanlab.org/2024/11/copyright-claim-moves-ahead-in-the-intercepts-lawsuit-against-openai/
42212650,The AI reporter that took my old job just got fired,,307,Brajeshwar,1732271660,story,https://www.wired.com/story/the-ai-reporter-who-took-my-old-job-just-got-fired/
43402957,"FTC Removes Posts Critical of Amazon, Microsoft, and AI Companies",,306,gnabgib,1742322462,story,https://www.wired.com/story/federal-trade-commission-removed-blogs-critical-of-ai-amazon-microsoft/
42057139,The deep learning boom caught almost everyone by surprise,,306,slyall,1730865901,story,https://www.understandingai.org/p/why-the-deep-learning-boom-caught
38477197,"Accelerating Generative AI with PyTorch II: GPT, Fast",,306,polyrand,1701369300,story,https://pytorch.org/blog/accelerating-generative-ai-2/
44367638,ChatGPT's enterprise success against Copilot fuels OpenAI/Microsoft rivalry,,306,mastermaq,1750780922,story,https://www.bloomberg.com/news/articles/2025-06-24/chatgpt-vs-copilot-inside-the-openai-and-microsoft-rivalry
45530486,LLMs are mortally terrified of exceptions,<a href=""https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1976082963382272334"" rel=""nofollow"">https:&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1976082963382272334</a>,305,nought,1760030188,story,https://twitter.com/karpathy/status/1976077806443569355
42095302,When machine learning tells the wrong story,,305,jackcook,1731170288,story,https://jackcook.com/2024/11/09/bigger-fish.html
43110265,Magma: A foundation model for multimodal AI agents,,305,SerCe,1740017485,story,https://microsoft.github.io/Magma/
41105130,Show HN: Turn any website into a knowledge base for LLMs,"I built this tool because I wanted a way to just take a bunch of URLs or domains, and query their content in RAG applications.<p>It takes away the pain of crawling, extracting content, chunking, vectorizing, and updating periodically.<p>I&#x27;m curious to see if it can be useful to others. I meant to launch this six months ago but life got in the way...",305,tompec,1722300845,story,https://www.embedding.io/
43312652,With AI you need to think bigger,,305,boznz,1741547921,story,https://rodyne.com/?p=1828
44851557,"GPT-5: Overdue, overhyped and underwhelming. And that's not the worst of it",,304,kgwgk,1754784414,story,https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming
43118634,OpenEuroLLM,,304,richardfontana,1740077868,story,https://openeurollm.eu/
40423082,Chameleon: Meta’s New Multi-Modal LLM,,304,gabrielbirnbaum,1716255473,story,https://arxiv.org/abs/2405.09818
38782678,Pushing ChatGPT's Structured Data Support to Its Limits,,304,goranmoomin,1703689759,story,https://minimaxir.com/2023/12/chatgpt-structured-data/
39918500,Show HN: Plandex – an AI coding engine for complex tasks,"Hey HN, I&#x27;m building Plandex (<a href=""https:&#x2F;&#x2F;plandex.ai"" rel=""nofollow"">https:&#x2F;&#x2F;plandex.ai</a>), an open source, terminal-based AI coding engine for complex tasks.<p>I built Plandex because I was tired of copying and pasting code back and forth between ChatGPT and my projects. It can complete tasks that span multiple files and require many steps. It uses the OpenAI API with your API key (support for other models, including Claude, Gemini, and open source models is on the roadmap). You can watch a 2 minute demo here: <a href=""https:&#x2F;&#x2F;player.vimeo.com&#x2F;video&#x2F;926634577"" rel=""nofollow"">https:&#x2F;&#x2F;player.vimeo.com&#x2F;video&#x2F;926634577</a><p>Here&#x27;s a prompt I used to build the AWS infrastructure for Plandex Cloud (Plandex can be self-hosted or cloud-hosted): <a href=""https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex&#x2F;blob&#x2F;main&#x2F;test&#x2F;test_prompts&#x2F;aws-infra.txt"">https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex&#x2F;blob&#x2F;main&#x2F;test&#x2F;test_pr...</a><p>Something I think sets Plandex apart is a focus on working around bad outputs and iterating on tasks systematically. It&#x27;s relatively easy to make a great looking demo for any tool, but the day-to-day of working with it has a lot more to do with how it handles edge cases and failures. Plandex tries to tighten the feedback loop between developer and LLM:<p>- Every aspect of a Plandex plan is version-controlled, from the context to the conversation itself to model settings. As soon as things start to go off the rails, you can use the `plandex rewind` command to back up and add more context or iterate on the prompt. Git-style branches allow you to test and compare multiple approaches.<p>- As a plan proceeds, tentative updates are accumulated in a protected sandbox (also version-controlled), preventing any wayward edits to your project files.<p>- The `plandex changes` command opens a diff review TUI that lets you review pending changes side-by-side like the GitHub PR review UI. Just hit the &#x27;r&#x27; key to reject any change that doesn’t look right. Once you’re satisfied, either press ctrl+a from the changes TUI or run `plandex apply` to apply the changes.<p>- If you work on files you’ve loaded into context outside of Plandex, your changes are pulled in automatically so that the model always uses the latest state of your project.<p>Plandex makes it easy to load files and directories in the terminal. You can load multiple paths:<p><pre><code>  plandex load components&#x2F;some-component.ts lib&#x2F;api.ts ..&#x2F;sibling-dir&#x2F;another-file.ts </code></pre> You can load entire directories recursively:<p><pre><code>  plandex load src&#x2F;lib -r </code></pre> You can use glob patterns:<p><pre><code>  plandex load src&#x2F;**&#x2F;*.{ts,tsx} </code></pre> You can load directory layouts (file names only):<p><pre><code>  plandex load src --tree  </code></pre> Text content of urls:<p><pre><code>  plandex load https:&#x2F;&#x2F;react.dev&#x2F;reference&#x2F;react&#x2F;hooks </code></pre> Or pipe data in:<p><pre><code>  cargo test | plandex load </code></pre> For sending prompts, you can pass in a file:<p><pre><code>  plandex tell -f &quot;prompts&#x2F;stripe&#x2F;add-webhooks.txt&quot; </code></pre> Or you can pop up vim and write your prompt there:<p><pre><code>  plandex tell </code></pre> For shorter prompts you can pass them inline:<p><pre><code>  plandex tell &quot;set the header&#x27;s background to #222 and text to white&quot; </code></pre> You can run tasks in the background:<p><pre><code>  plandex tell &quot;write tests for all functions in lib&#x2F;math&#x2F;math.go. put them in lib&#x2F;math_tests.&quot; --bg  </code></pre> You can list all running or recently finished tasks:<p><pre><code>  plandex ps </code></pre> And connect to any running task to start streaming it:<p><pre><code>  plandex connect </code></pre> For more details, here’s a quick overview of commands and functionality: <a href=""https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex&#x2F;blob&#x2F;main&#x2F;guides&#x2F;USAGE.md"">https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex&#x2F;blob&#x2F;main&#x2F;guides&#x2F;USAGE...</a><p>Plandex is written in Go and is statically compiled, so it runs from a single small binary with no dependencies on any package managers or language runtimes. There’s a 1-line quick install:<p><pre><code>  curl -sL https:&#x2F;&#x2F;plandex.ai&#x2F;install.sh | bash </code></pre> It&#x27;s early days, but Plandex is working well and is legitimately the tool I reach for first when I want to do something that is too large or complex for ChatGPT or GH Copilot. I would love to get your feedback. Feel free to hop into the Discord (<a href=""https:&#x2F;&#x2F;discord.gg&#x2F;plandex-ai"" rel=""nofollow"">https:&#x2F;&#x2F;discord.gg&#x2F;plandex-ai</a>) and let me know how it goes. PRs are also welcome!",304,danenania,1712157011,story,https://github.com/plandex-ai/plandex
40310228,Leaked deck reveals how OpenAI is pitching publisher partnerships,,303,rntn,1715273815,story,https://www.adweek.com/media/openai-preferred-publisher-program-deck/
44484207,LLMs should not replace therapists,,303,layer8,1751837248,story,https://arxiv.org/abs/2504.18412
43456723,Project Aardvark: reimagining AI weather prediction,,302,bentobean,1742772819,story,https://www.turing.ac.uk/blog/project-aardvark-reimagining-ai-weather-prediction
45050931,Important machine learning equations,,302,sebg,1756381124,story,https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/
40302792,Stack Overflow users deleting answers after OpenAI partnership,,302,miles,1715203013,story,https://build5nines.com/stack-overflow-upset-over-users-deleting-answers-after-openai-partnership/
40008109,Debunking Devin: ""First AI Software Engineer"" Upwork Lie Exposed [video],,302,smukherjee19,1712880476,story,https://www.youtube.com/watch?v=tNmgmwEtoWE
40379599,ChatGPT-4o vs. Math,,302,sabrina_ramonov,1715873449,story,https://www.sabrina.dev/p/chatgpt4o-vs-math
44521225,Andrew Ng: Building Faster with AI [video],,302,sandslash,1752156128,story,https://www.youtube.com/watch?v=RNJCfif1dPY
43378401,Big LLMs weights are a piece of history,,301,freeatnet,1742127204,story,https://antirez.com/news/147
43891245,Show HN: My AI Native Resume,"I&#x27;ve been deeply involved in working with AI agents and large language models (LLMs) for a while now. During a recent job search, I found myself repeatedly explaining my skills and experiences to various assistants. Around the same time, I was creating content for my website to help hiring teams understand my capabilities better and make informed decisions.<p>MCP had started to gain momentum and I saw a way to reduce my toil. So I built an MCP server that can effectively communicate my qualifications as a job candidate. This server acts as an AI-powered resume, providing an understanding of my professional background and a set of tools, prompts and resources to help explore my skills and experiences.<p>The code is open source, so you can create your own AI-driven resume server. Check it out here: <a href=""https:&#x2F;&#x2F;github.com&#x2F;jhgaylor&#x2F;node-candidate-mcp-server"">https:&#x2F;&#x2F;github.com&#x2F;jhgaylor&#x2F;node-candidate-mcp-server</a>.<p>During my job search I paired my mcp server with others such as notion, hirebase, and gmail to build a leads database, write cover letters, and track my job search.",301,jhgaylor,1746409449,story,https://ai.jakegaylor.com/
41353835,Avante.nvim: Use Your Neovim Like Using Cursor AI IDE,,300,simonpure,1724643888,story,https://github.com/yetone/avante.nvim
44769547,I tried to replace myself with ChatGPT in my English class,,300,lapcat,1754155945,story,https://lithub.com/what-happened-when-i-tried-to-replace-myself-with-chatgpt-in-my-english-classroom/
38756888,"""Attention"", ""Transformers"", in Neural Network ""Large Language Models""",,300,macleginn,1703452214,story,http://bactra.org/notebooks/nn-attention-and-transformers.html
39557188,Show HN: Struct – A Feed-Centric Chat Platform,"Hi HN! I’m Jason, a product designer at Struct Chat.<p>At Struct, we&#x27;re frustrated by the clutter, distractions, and inefficiencies plaguing existing chat platforms like Slack and Discord.<p>We&#x27;ve built a radical new chat platform that leverages threads, feeds, and AI to help alleviate these problems, and give you back more time in your day.<p>Struct uses a thread-first approach to keep conversations on-topic. It applies AI-generated titles and summaries to help you decide what&#x27;s worth your attention. Threads are then organized in a real-time feed, keeping all your conversations up-to-date and at the ready, eliminating the need for channel hopping.<p>Comprehensive search tools make finding things a breeze, and Strucbot, our AI assistant can answer questions based on what it’s learned from prior threads. It can even proactively respond when it notices repeat requests, providing quick answers so you don’t have to. Structbot is fully GPT-4 enabled, so you can riff with Chat GPT and your peers (generate code, ask questions, all the good stuff) without ever switching apps.<p>Struct is available on Linux, Windows, Mac, and even works as a Slack interface. Give us a try and let us know what you think.",298,jdplex,1709254147,story,https://struct.ai/blog/introducing-the-struct-chat-platform
44467949,Everything around LLMs is still magical and wishful thinking,,298,troupo,1751663817,story,https://dmitriid.com/everything-around-llms-is-still-magical-and-wishful-thinking
40646741,Show HN: Revideo – Create Videos with Code,"Hey HN! We’re building Revideo (<a href=""https:&#x2F;&#x2F;github.com&#x2F;redotvideo&#x2F;revideo"">https:&#x2F;&#x2F;github.com&#x2F;redotvideo&#x2F;revideo</a>), an open source framework for programmatic video editing.<p>Revideo lets you create video templates in Typescript and render them with dynamic inputs through an API. It also comes with a &lt;Player &#x2F;&gt; component that lets you preview your projects in the browser and integrate video editing functionality into web apps.<p>The project is useful for anyone who wants to build apps that automate certain video editing tasks. A lot of companies in the space build their own custom stack for this, like Opus (<a href=""https:&#x2F;&#x2F;www.opus.pro&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.opus.pro&#x2F;</a>), which automatically creates highlight videos from podcasts, or Clueso (<a href=""https:&#x2F;&#x2F;www.clueso.io&#x2F;"">https:&#x2F;&#x2F;www.clueso.io&#x2F;</a>), which lets you create stutter-free product walkthroughs with AI voiceovers.<p>Revideo is based on the HTML Canvas API and is forked from Motion Canvas (<a href=""https:&#x2F;&#x2F;github.com&#x2F;motion-canvas&#x2F;motion-canvas"">https:&#x2F;&#x2F;github.com&#x2F;motion-canvas&#x2F;motion-canvas</a>), a tool that lets you create canvas animations. While Motion Canvas is intended by its maintainer to exclusively be a standalone application [1], we have turned Revideo into a library that developers can integrate into their apps, while specifically focusing on video use cases. To support this, we have, among other things, added the ability to do headless rendering, made video rendering much faster and added support for syncing and exporting audio.<p>We’re excited about programmatic video editing because of the possibility to automate content creation with AI. One of our users is building StoriesByAngris (<a href=""https:&#x2F;&#x2F;storiesbyangris.com&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;storiesbyangris.com&#x2F;</a>), which lets you create video-based RPG stories from language prompts. Other users are marketing-tech companies that help their customers generate and A&#x2F;B test different versions of video ads.<p>We started to work on video tooling because we ourselves explored a bunch of product ideas in the space of AI-based video creation earlier this year. For example, we built apps that automatically create educational short videos and tinkered with apps that let you create memes.<p>While building these products, we were frustrated with the video editing frameworks we used: Moviepy (<a href=""https:&#x2F;&#x2F;github.com&#x2F;Zulko&#x2F;moviepy"">https:&#x2F;&#x2F;github.com&#x2F;Zulko&#x2F;moviepy</a>), which we used initially, doesn’t work in the browser, so we’d often have to wait minutes for a video to render just to test our code changes. Remotion (<a href=""https:&#x2F;&#x2F;github.com&#x2F;remotion-dev&#x2F;remotion"">https:&#x2F;&#x2F;github.com&#x2F;remotion-dev&#x2F;remotion</a>), which we switched to later, is pretty good, but we didn’t want to rely on it as it is not FOSS (source-available only).<p>We had already followed Motion Canvas for some time and really liked it, so we thought that extending it would get us to something useful much faster than building an animation library from scratch. We initially tried to build Revideo as a set of Motion Canvas plugins, but we soon realized that the changes we were making were too drastic and far too complex to fit into plugins. This is why we ultimately created a fork. We’re unsure if this is the right way to go in the long term, and would prefer to find a way to build Revideo without feeling like we’re dividing the community - if you have experience with this (keeping forks with complex changes in sync with upstream) or other suggestions on how to solve this, we’d love your input.<p>Our current focus is improving the open source project. In the long term, we want to make money by building a rendering service for developers building apps with Revideo.<p>We’d love to hear your feedback and suggestions on what we can improve! You can find our repo at <a href=""https:&#x2F;&#x2F;github.com&#x2F;redotvideo&#x2F;revideo"">https:&#x2F;&#x2F;github.com&#x2F;redotvideo&#x2F;revideo</a>, and you can explore example projects at <a href=""https:&#x2F;&#x2F;github.com&#x2F;redotvideo&#x2F;examples"">https:&#x2F;&#x2F;github.com&#x2F;redotvideo&#x2F;examples</a><p>[1] “Motion Canvas is not a normal npm package. It&#x27;s a standalone tool that happens to be distributed via npm.” - <a href=""https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;motion-canvas&#x2F;discussions&#x2F;1015"">https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;motion-canvas&#x2F;discussions&#x2F;1015</a>",298,hkonsti,1718117011,story,https://github.com/redotvideo/revideo
40441945,Show HN: Route your prompts to the best LLM,"Hey HN, we&#x27;ve just finished building a dynamic router for LLMs, which takes each prompt and sends it to the most appropriate model and provider. We&#x27;d love to know what you think!<p>Here is a quick(ish) screen-recroding explaining how it works: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;ZpY6SIkBosE"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;ZpY6SIkBosE</a><p>Best results when training a custom router on your own prompt data: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;9JYqNbIEac0</a><p>The router balances user preferences for quality, speed and cost. The end result is higher quality and faster LLM responses at lower cost.<p>The quality for each candidate LLM is predicted ahead of time using a neural scoring function, which is a BERT-like architecture conditioned on the prompt and a latent representation of the LLM being scored. The different LLMs are queried across the batch dimension, with the neural scoring architecture taking a single latent representation of the LLM as input per forward pass. This makes the scoring function very modular to query for different LLM combinations. It is trained in a supervised manner on several open LLM datasets, using GPT4 as a judge. The cost and speed data is taken from our live benchmarks, updated every few hours across all continents. The final &quot;loss function&quot; is a linear combination of quality, cost, inter-token-latency and time-to-first-token, with the user effectively scaling the weighting factors of this linear combination.<p>Smaller LLMs are often good enough for simple prompts, but knowing exactly how and when they might break is difficult. Simple perturbations of the phrasing can cause smaller LLMs to fail catastrophically, making them hard to rely on. For example, Gemma-7B converts numbers to strings and returns the &quot;largest&quot; string when asking for the &quot;largest&quot; number in a set, but works fine when asking for the &quot;highest&quot; or &quot;maximum&quot;.<p>The router is able to learn these quirky distributions, and ensure that the smaller, cheaper and faster LLMs are only used when there is high confidence that they will get the answer correct.<p>Pricing-wise, we charge the same rates as the backend providers we route to, without taking any margins. We also give $50 in free credits to all new signups.<p>The router can be used off-the-shelf, or it can be trained directly on your own data for improved performance.<p>What do people think? Could this be useful?<p>Feedback of all kinds is welcome!",298,danlenton,1716390440,story,https://unify.ai/chat?default=true
43820022,Tiny-LLM – a course of serving LLM on Apple Silicon for systems engineers,,297,sarkory,1745839481,story,https://github.com/skyzh/tiny-llm
39947713,World_sim: LLM prompted to act as a sentient CLI universe simulator,,297,CharlesW,1712354101,story,https://worldsim.nousresearch.com/
45273747,Alibaba's new AI chip: Key specifications comparable to H20,,297,dworks,1758102344,story,https://news.futunn.com/en/post/62202518/alibaba-s-new-ai-chip-unveiled-key-specifications-comparable-to
42611844,My little sister's use of ChatGPT for homework is heartbreaking,,297,ajdude,1736180360,story,https://old.reddit.com/r/ChatGPT/comments/1hun3e4/my_little_sisters_use_of_chatgpt_for_homework_is/
44433996,Fei-Fei Li: Spatial intelligence is the next frontier in AI [video],,296,sandslash,1751378433,story,https://www.youtube.com/watch?v=_PioN-CpOP0
43377962,GPT 4.5 level for 1% of the price,,296,decide1000,1742120626,story,https://twitter.com/Baidu_Inc/status/1901089355890036897
39560862,Where is Noether's principle in machine learning?,,296,cgadski,1709293650,story,https://cgad.ski/blog/where-is-noethers-principle-in-machine-learning.html
45003073,YouTube made AI enhancements to videos without warning or permission,,296,jakub_g,1756031835,story,https://www.bbc.com/future/article/20250822-youtube-is-using-ai-to-edit-videos-without-permission
45199760,TikTok has turned culture into a feedback loop of impulse and machine learning,,295,natalie3p,1757520519,story,https://www.thenexus.media/tiktok-won-now-everything-is-60-seconds/
44762856,Anthropic revokes OpenAI's access to Claude,,294,minimaxir,1754085028,story,https://www.wired.com/story/anthropic-revokes-openais-access-to-claude/
39895453,"Show HN: DN$ – an innovative, ad-supported DNS resolver","Tired of companies snooping through your DNS traffic? Don&#x27;t you wish you could get advertisements with your DNS records?<p>Today we&#x27;re introducing the innovative, privacy-focused, ad-supported DNS resolver - DN$! Traditional DNS resolvers provided by your internet service provider, cloudflare, or google could be tracking your internet activity and selling it to third-party data vendors. We at DN$ want to fix that and cut out these nefarious actors (until we&#x27;ve amassed a critical number of users to exploit).<p>In order to support such a radically new business model, our service needs to serve adverts because $INSERT_FAKE_REASONS. Open source and built in rust - our software is secure and blazingly fast because it is open source and built in rust.<p>As a corporate entity, our executives are not liable for prison time and will probably only be fined small financial penalties for any serious crimes we commit. However, we *promise* that we are NOT doing anything nefarious like tracking and selling your user data and internet behavior. We will also NOT be using the data (we are <i>not</i> collecting : ) to train AI models to make ourselves rich.<p>Did we mention that it&#x27;s built in rust therefore it&#x27;s safe and fast?<p>Send your DNS queries to `35.223.197.204` :) to try it out:<p>``` dig @35.223.197.204 hackernews.com ```",294,nablags,1711986513,story,https://github.com/tedkim97/adcache
38868185,Launch HN: Rosebud (YC S19) – Turn game descriptions into browser games,"Hi everyone! I&#x27;m Lisha, the founder of Rosebud AI (<a href=""https:&#x2F;&#x2F;www.rosebud.ai&#x2F;"">https:&#x2F;&#x2F;www.rosebud.ai&#x2F;</a>). We&#x27;re building a platform to help users go from description to code to game. We aim to make game creation accessible to non-technical creators, so our UI provides explanations alongside the generated code.<p>Users have created a diverse range of games on Rosebud, including top-down RPGs, AI companions, and 3D obstacle courses, all within a few hours and sometimes minutes. Here are some examples you can play and clone (to start your own project).<p>* Anime Jester Companion: <a href=""https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;ba438cc4-246e-432e-b170-4e16948cd571"">https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;ba438cc4-246e-432e-b170-4e1694...</a> * Chat and Care for your Digital Puppy: <a href=""https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;f32a8159-7acf-4db6-a82c-70296f90bbf1"">https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;f32a8159-7acf-4db6-a82c-70296f...</a> * Sphere Sync (3D game: align the sphere with the right color): <a href=""https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;96dfd5e1-62d4-47d8-a3e9-11038c8bb5cf"">https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;96dfd5e1-62d4-47d8-a3e9-11038c...</a> * Basketball: <a href=""https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;a0e70622-e923-4517-8c1f-728dcf0db486"">https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;a0e70622-e923-4517-8c1f-728dcf...</a> * Neon Waltz Generative Art: <a href=""https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;e32bd12b-7cc9-4f9a-b385-42ae0b096466"">https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;e32bd12b-7cc9-4f9a-b385-42ae0b...</a> * Chat with Deku from My Hero Academia: <a href=""https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;716fd998-aab6-4185-8375-85d9eeb2adca"">https:&#x2F;&#x2F;play.rosebud.ai&#x2F;games&#x2F;716fd998-aab6-4185-8375-85d9ee...</a><p>A simple way to think about Rosebud is ChatGPT + Midjourney + Replit. ChatGPT, because we give users a chat interface for this code editor so they can describe the game they want to make and generate game code; Midjourney, because we let users generate assets inside Rosebud, 2D and 3D, to be used in their games; And Replit, because Rosebud includes a browser based code editor that lets you deploy your game instantly.<p>Sometimes, users generate a code base from scratch via prompts. Often a simpler place to start is to modify (“clone”) an existing project on Rosebud. In both cases, we need to eventually convert user descriptions and modifications of the game into edits and changes to the codebase. To solve this problem, we had to experiment heavily with using LLM agents in production. Our agent framework tries to follow the instructions of user prompts by deciding when and whether to call upon a number of generative models (some for code generation, some for asset generation, some for character dialogue, and some for game ideas). It also must decide where to insert code snippets when it generates them. Often, a user is asking for ideas or something too vague, and our agent has to decide when to ask for feedback and clarifications.<p>Not surprisingly, if we impose more constraints, on both the programming framework and game genres supported, our agent will perform better. However, the constraints on the types of games users can make and frameworks we want to support also constrains how flexible our platform is. Balancing these two factors, we decided to only support browser-based, JavaScript frameworks and focus on supporting AI NPCs that use LLMs themselves for dialogue and actions. This allows us to create abstractions that enable the agent to alter the codebase more successfully and guide the creator towards a more successful experience. Furthermore, we found that our beta testers are very creative with making AI character based games, and the resulting game is usually fun for players.<p>How does Rosebud differ from Roblox, Unreal, or Unity simply adding a co-pilot? Incumbent game engines optimized their user-flow and tech stack before the advent of generative AI, and many of their user-flows are well established. We have the advantage of designing this game creation flow from the ground up. It&#x27;s not just about adding code completion to an existing code editing app and including asset plugins. Such an approach wouldn&#x27;t fully harness the power of LLMs. We have a chat-first interface, and having identified the limitations of agents, we can create more safeguards for users where failure is likely. Our approach will make it possible for non technical creators to also contribute to making games. Check it out for yourself!<p>To try Rosebud: (1) head over to <a href=""https:&#x2F;&#x2F;www.rosebud.ai&#x2F;hn"">https:&#x2F;&#x2F;www.rosebud.ai&#x2F;hn</a> for access to our Discord beta tester channel and a special role. (2) then go to <a href=""https:&#x2F;&#x2F;play.rosebud.ai"">https:&#x2F;&#x2F;play.rosebud.ai</a> and use the code HelloHN to get immediate access. We have an array of trending projects that users can clone and mod to get started, including various character chat based games.<p>Here’s a video onboarding of Rosebud in action: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=h99H3FefxU0&amp;ab_channel=RosebudAI"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=h99H3FefxU0&amp;ab_channel=Roseb...</a><p>Re business model, we plan on following in Roblox’s footsteps, i.e. keep it free for developers and take a cut of what they can charge users. Since AI tools cost more from usage than just hosting, we may have to evolve that model and see what the unit economics are (and separate a premium versus free tier for devs).<p>(Oh and in case you’re wondering why a YC S19 startup is launching now: we basically pivoted. We were always in consumer generative AI, but focused on images until this spring, but always wanted to focus on games–Rosebud is in fact a reference to the cheat code in The Sims. When code gen got good enough this year to work for UGC in gaming, we decided the time had finally come and switched.)<p>Some encouraging user feedback from our beta: “I have done some modding before, and I must say, this is much easier. Even when I occasionally need to code, the AI can answer all my questions and tell me how to achieve what I want. Normally, I would have to conduct numerous Google searches. What you guys have created is truly amazing.” “I’ve used Chat GPT to help me code simple games in Unity. This seems more connected and easier to work through.” “This is fascinating. This is ** amazing. Yeah, I know it&#x27;s obviously early on, but already works for rad generative art. I&#x27;ll say that much.” “Can finally call myself a game developer lol. Damn that sounds so good.”<p>We’re a small team working on this for the last few months, so a lot of things are far from perfect. Constructive feedback is very welcome!",293,lishali88,1704382430,story,
45179304,Anthropic judge rejects $1.5B AI copyright settlement,,293,nobody9999,1757407613,story,https://news.bloomberglaw.com/ip-law/anthropic-judge-blasts-copyright-pact-as-nowhere-close-to-done
39462087,Unexpected responses from ChatGPT: Incident Report,,293,swyx,1708566132,story,https://status.openai.com/incidents/ssg8fh7sfyz3
44832990,GPT-5 leaked system prompt?,,293,maoxiaoke,1754622545,story,https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7
40608413,σ-GPTs: A new approach to autoregressive models,,293,mehulashah,1717765951,story,https://arxiv.org/abs/2404.09562
44162363,Show HN: I build one absurd web project every month,"I’ve been building absurd, mostly useless web projects for fun — and I publish one every month at absurd.website.<p>These are deliberately non-functional, weird, sometimes funny, sometimes philosophical — and usually totally unnecessary.<p>Some examples:<p>Sexy Math — solve math problems to reveal erotic images.<p>Trip to Mars — a real-time simulation that takes 7 months to finish.<p>Add Luck to Your e-Store — add a waving cat widget to boost your conversion via superstition.<p>Microtasks for Meatbags — the future: AI gives prompts, humans execute.<p>Invisible Lingerie — it’s sexy. And invisible.<p>Artist Death Tracker — art prices spike when artists die. We track that.<p>Open Celebrity — one open-source face, shared by all. Together we make her famous.<p>I just enjoy exploring what the web can be when it doesn’t try to be “useful”.<p>Would love to hear what you think — and absurd ideas are always welcome.",293,absurdwebsite,1748893966,story,https://absurd.website
38906966,I made an app that runs Mistral 7B 0.2 LLM locally on iPhone Pros,,292,winstonschen,1704675647,story,https://apps.apple.com/us/app/offline-chat-private-ai/id6474077941
45464429,Jeff Bezos says AI is in a bubble but society will get 'gigantic' benefits,,292,belter,1759507200,story,https://www.cnbc.com/2025/10/03/jeff-bezos-ai-in-an-industrial-bubble-but-society-to-benefit.html
41069909,Launch HN: Undermind (YC S24) – AI agent for discovering scientific papers,"Hey HN! We’re Josh and Tom from Undermind (<a href=""https:&#x2F;&#x2F;www.undermind.ai&#x2F;"">https:&#x2F;&#x2F;www.undermind.ai&#x2F;</a>). We’re building a search engine for complex scientific research. There&#x27;s a demo video at <a href=""https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;10067c49e4424b949a4b8c9fd8f3b12c?sid=5ee3cd96-2b05-4768-8856-cb1b83e40a70"" rel=""nofollow"">https:&#x2F;&#x2F;www.loom.com&#x2F;share&#x2F;10067c49e4424b949a4b8c9fd8f3b12c?...</a>, as well as example search results on our homepage.<p>We’re both physicists, and one of our biggest frustrations during grad school was finding research — There were a lot of times when we had to sit down to scope out new ideas for a project and quickly become a deep expert, or we had to find solutions to really complex technical problems, but the only way to do that was manually dig through papers on Google Scholar for hours. It was very tedious, to the point where we would often just skip the careful research and hope for the best. Sometimes you’d get burned a few months later because someone already solved the problem you thought was novel and important, or you’d waste your time inventing&#x2F;building a solution for something when one already existed.<p>The problem was there’s just no easy way to figure out what others have done in research, and load it into your brain. It’s one of the biggest bottlenecks for doing truly good, important research.<p>We wanted to fix that. LLMs clearly help, but are mostly limited to general knowledge. Instead, we needed something that would pull in research papers, and give you exactly what you need to know, even for very complex ideas and topics. We realized the way to do this is to mimic the research strategies we already know work, because we do them ourselves, and so we built an agent-like LLM pipeline to carefully search in a way that mimics human research strategies.<p>Our search system works a bit differently from casual search engines. First, we have you chat back and forth with an LLM to make sure we actually understand your really complex research goals up front, like you’re talking to a colleague. Then the system carefully searches for you for ~3 minutes. At a high level, it does something similar to tree search, following citation rabbit holes and adapting based on what it discovers to look for more content over multiple iterations (the same way you would if you decided to spend a few hours). The 3 minute delay is annoying, but we’re optimizing for quality of results rather than latency right now. At the end there’s a report.<p>We’re trying to achieve two things with this careful, systematic agent-like discovery process:<p>1. We want to be very accurate, and only recommend very specific results if you ask for a specific topic. To do this, we carefully read and evaluate content from papers with the highest quality LLMs (we’re just reading abstracts and citations for now, because they’re more widely accessible - but also working on adding full texts).<p>2. We want to find everything relevant to your search, because in research it’s crucial to know if something exists or not. The key to being exhaustive is the adaptive algorithms we’ve developed (following citations, changing strategy based on what we find, etc). However, one cool feature of the automated pipeline is we can track the discovery process as the search proceeds. Early on, we find many good results, and later on they get more sparse, until all the good leads are exhausted and we stop finding anything helpful. We can statistically model that process, and figure out when we’ve found everything (it actually has an interesting exponential saturation behavior, which you can read a bit more about in our whitepaper (<a href=""https:&#x2F;&#x2F;www.undermind.ai&#x2F;static&#x2F;Undermind_whitepaper.pdf"">https:&#x2F;&#x2F;www.undermind.ai&#x2F;static&#x2F;Undermind_whitepaper.pdf</a>), which we wrote for a previous prototype.)<p>You can try searching yourself here: <a href=""https:&#x2F;&#x2F;www.undermind.ai&#x2F;query_app&#x2F;promotion&#x2F;"">https:&#x2F;&#x2F;www.undermind.ai&#x2F;query_app&#x2F;promotion&#x2F;</a>. This is a special HN link where, for today, we’ve dropped the signup gate for your first few searches. Usually we require login so you can save searches.<p>We’re excited to share this with you! We’d love to hear about your experiences searching, what’s clear or not, and any feedback. We’ll be here to answer any questions or comments.",292,jramette,1721921817,story,
44523409,Show HN: Open source alternative to Perplexity Comet,"Hey HN, we&#x27;re a YC startup building an open-source, privacy-first alternative to Perplexity Comet.<p>No invite system unlike bunch of others – you can download it today from our website or GitHub: <a href=""https:&#x2F;&#x2F;github.com&#x2F;browseros-ai&#x2F;BrowserOS"">https:&#x2F;&#x2F;github.com&#x2F;browseros-ai&#x2F;BrowserOS</a><p>--- Why bother building an alternative? We believe browsers will become the new operating systems, where we offload much bunch of our work to AI agents. But these agents will have access to all your sensitive data – emails, docs, on top of your browser history. Open-source, privacy-first alternatives need to exist.<p>We&#x27;re not a search or ad company, so no weird incentives. Your data stays on your machine. <i>You can use local LLMs with Ollama</i>. We also support BYOK (bring your own keys), so no $200&#x2F;month plans.<p>Another big difference vs Perplexity Comet: our agent runs locally in your browser (not on their server). You can actually watch it click around and do stuff, which is pretty cool! Short demo here: <a href=""https:&#x2F;&#x2F;bit.ly&#x2F;browserOS-demo"" rel=""nofollow"">https:&#x2F;&#x2F;bit.ly&#x2F;browserOS-demo</a><p>--- How we built? We patch Chromium&#x27;s C++ source code with our changes, so we have the same security as Google Chrome. We also have an auto-updater for security patches and regular updates.<p>Working with Chromium&#x27;s 15M lines of C++ has been another fun adventure that I&#x27;m writing a blog post on. Cursor&#x2F;VSCode breaks at this scale, so we&#x27;re back to using grep to find stuff and make changes. Claude code works surprisingly well too.<p>Building the binary takes ~3 hours on our M4 Max MacBook.<p>--- Next? We&#x27;re just 2 people with a lot of work ahead (Firefox started with 3 hackers, history rhymes!). But we strongly believe that a privacy-first browser with local LLM support is more important than ever – since agents will have access to so much sensitive data.<p>Looking forward to any and all comments!",291,felarof,1752168787,story,https://www.browseros.com/
41541053,"LLMs Will Always Hallucinate, and We Need to Live with This",,291,Anon84,1726333376,story,https://arxiv.org/abs/2409.05746
42173119,"Show HN: Tips.io – A Tailwind playground with AI, page management, and theming","Hi HN!<p>My name is Nick and this is my fun side project. Please lay it on me. HN can think of Tips.io as a cracked out Tailwind Playground that has page management and amazing AI integration.<p>There are a few core ideas:<p>1) The HTML is the CMS<p>There are no fields or restrictions. Just hover, click, and start tweaking any HTML. Also, certain elements you click will have special easy edit abilities:<p>- &lt;img&gt; auto creates an uploader, stock photo picker (or HTML)<p>- &lt;video&gt; auto creates an uploader, stock video picker (or HTML)<p>- &lt;svg&gt; auto creates a big icon picker (or HTML)<p>- &lt;div class=&quot;prose&quot;&gt; auto creates a WYSIWYG Editor (or HTML)<p>2) Slices<p>Think of these as just individual HTML sections of a page or lil baby single-file components. They are self-contained and isolated so you drag them around easily. The real power comes from reuse across your pages and linking them (aka, one HTML footer updates globally). You can also use &quot;slices&quot; from any other tips.io project for quickly expanding your site with more design options.<p>3) AI Elements, Not Pages<p>Another cool concept is you can select any element on an HTML slice an edit that individually vs re-streaming&#x2F;rebuilding and entire component every time. We support 5 different AI models right now. Some other really intense&#x2F;cool AI integration is coming soon.<p>4) Tailwind Everything, No Build Step, &amp; Theming<p>We have a custom &quot;themer&quot; to make creating Tailwind config files near instant with real-time font trying, color palettes&#x2F;preset trying, and more. All our Tailwind is automatic and requires zero config instantly. The same Tailwind that magic runs client-side will run server-side so quick no one knows a build step is happening. Tailwind and AI are also a match made in heaven.<p>Other features:<p>- Animations - Zoomable page tree - Basic Forms (yes on your static site!) - Analytics - Redirects, site passwords, and much more.<p>Tech:<p>- 100% Cloudflare Workers - Svelte - UnoCSS<p>Some resources:<p>- Promo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=s8U2rJJX-rk"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=s8U2rJJX-rk</a> - Tutorial &amp; demo video: <a href=""https:&#x2F;&#x2F;tips.io&#x2F;tutorial"" rel=""nofollow"">https:&#x2F;&#x2F;tips.io&#x2F;tutorial</a> - Just launch: <a href=""https:&#x2F;&#x2F;new.tips.io"" rel=""nofollow"">https:&#x2F;&#x2F;new.tips.io</a>",291,TIPSIO,1731943164,story,https://tips.io
41745788,"LLMs, Theory of Mind, and Cheryl's Birthday",,290,stereoabuse,1728077647,story,https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb
41010188,AI paid for by Ads – the GPT-4o mini inflection point,,290,thunderbong,1721417319,story,https://batchmon.com/blog/ai-cheaper-than-ads/
44901853,Show HN: OWhisper – Ollama for realtime speech-to-text,"Hello everyone. This is Yujong from the Hyprnote team (<a href=""https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote</a>).<p>We built OWhisper for 2 reasons: (Also outlined in <a href=""https:&#x2F;&#x2F;docs.hyprnote.com&#x2F;owhisper&#x2F;what-is-this"" rel=""nofollow"">https:&#x2F;&#x2F;docs.hyprnote.com&#x2F;owhisper&#x2F;what-is-this</a>)<p>(1). While working with on-device, realtime speech-to-text, we found there isn&#x27;t tooling that exists to download &#x2F; run the model in a practical way.<p>(2). Also, we got frequent requests to provide a way to plug in custom STT endpoints to the Hyprnote desktop app, just like doing it with OpenAI-compatible LLM endpoints.<p>The (2) part is still kind of WIP, but we spent some time writing docs so you&#x27;ll get a good idea of what it will look like if you skim through them.<p>For (1) - You can try it now. (<a href=""https:&#x2F;&#x2F;docs.hyprnote.com&#x2F;owhisper&#x2F;cli&#x2F;get-started"" rel=""nofollow"">https:&#x2F;&#x2F;docs.hyprnote.com&#x2F;owhisper&#x2F;cli&#x2F;get-started</a>)<p><pre><code>  bash   brew tap fastrepl&#x2F;hyprnote &amp;&amp; brew install owhisper   owhisper pull whisper-cpp-base-q8-en   owhisper run whisper-cpp-base-q8-en  </code></pre> If you&#x27;re tired of Whisper, we also support Moonshine :)  Give it a shot (owhisper pull moonshine-onnx-base-q8)<p>We&#x27;re here and looking forward to your comments!",289,yujonglee,1755186463,story,https://docs.hyprnote.com/owhisper/what-is-this
43344673,Reverse engineering OpenAI code execution to make it run C and JavaScript,,289,benswerd,1741795494,story,https://twitter.com/benswerd/status/1899853533761200300
44240999,OpenAI o3-pro,,289,mfiguiere,1749586547,story,https://help.openai.com/en/articles/9624314-model-release-notes
39858219,Deep Learning in JavaScript,,288,eduardoleao052,1711665339,story,https://github.com/eduardoleao052/js-torch
42535057,Family of OpenAI whistleblower Suchir Balaji demand FBI investigate death,,288,c420,1735422402,story,https://www.theguardian.com/us-news/2024/dec/28/openai-whistleblower-suchir-balaji
39955725,More Agents Is All You Need: LLMs performance scales with the number of agents,,288,TaurenHunter,1712438750,story,https://arxiv.org/abs/2402.05120
44822665,How AI conquered the US economy: A visual FAQ,,287,rbanffy,1754561574,story,https://www.derekthompson.org/p/how-ai-conquered-the-us-economy-a
41923635,Ask HN: Website with 6^16 subpages and 80k+ daily bots,"Last year, just for fun, I created a single index.php website calculating HEX colors to RGB. It takes 3 and 6 digit notation (ie. #c00 and #cc0000) and converts it to RGB value. No database, just single .php file, converting values on the fly.<p>It&#x27;s little over a year old and now every day there&#x27;s 60k-100k bots visiting and crawling the shit out of two-trillion-something sub pages...<p>I am out of ideas what to do with this site. I mean, it&#x27;s probably one of the largest websites on the Internet, if counted by sub-pages...<p>What cool experiment&#x2F;idea&#x2F;stuff should I do&#x2F;try with this website?<p>I&#x27;m sure AI could be (ab)used somehow here... :)",287,damir,1729679598,story,
40426382,Building an AI game studio: what we've learned so far,,287,FredrikNoren,1716285956,story,https://braindump.me/blog-posts/building-an-ai-game-studio
40069298,Cyc: History's Forgotten AI Project,,287,iafisher,1713383206,story,https://outsiderart.substack.com/p/cyc-historys-forgotten-ai-project
43879702,Run LLMs on Apple Neural Engine (ANE),,286,behnamoh,1746286150,story,https://github.com/Anemll/Anemll
44089156,Chomsky on what ChatGPT is good for (2023),,286,mef,1748192878,story,https://chomsky.info/20230503-2/
42343480,Google says AI weather model masters 15-day forecast,,286,lemonberry,1733514098,story,https://phys.org/news/2024-12-google-ai-weather-masters-day.html
41473518,Show HN: I mapped HN's favorite books with GPT-4o,"Hey HN! I love finding new books to read on here. I wanted to gather the most mentioned books and recreate the serendipity of physical browsing. I scraped 20k comments from HN threads related to reading, extracted the references and opinions using GPT-4o mini, and visualised their embeddings as a map.<p>- OpenAI&#x27;s embeddings were processed using UMAP and HDBSCAN. A direct 2D projection from the text embeddings didn&#x27;t yield visually interesting results. Instead, HDBSCAN is first applied on a high-dimensional projection. Those clusters tend to correspond to different genres. The genre memberships are then embedded using a second round of UMAP (using Hellinger distance) which results in pleasingly dense structures.<p>- The books&#x27; descriptions are based on extractions from the comments and GPT&#x27;s general knowledge. Quality levels vary, and it leads to some oddly specific points, but I haven&#x27;t found any yet that are straight up wrong.<p>- There are multiple books with the same title. Currently, only the most popular one of those makes it onto the map.<p>- It&#x27;s surprisingly hard to get high quality book cover images. I tried Google Books and a bunch of open APIs, but they all had their issues. In the end, I got the covers from GoodReads through a hacked together process that combines their autocomplete search with GPT for data linkage. Does anyone know of a reliable source?",285,pmaze,1725711811,story,https://hnbooks.pieterma.es
40593275,"U.S. clears way for antitrust inquiries of Nvidia, Microsoft and OpenAI",,285,okdood64,1717645962,story,https://www.nytimes.com/2024/06/05/technology/nvidia-microsoft-openai-antitrust-doj-ftc.html
41453237,Yi-Coder: A Small but Mighty LLM for Code,,285,crbelaus,1725507509,story,https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md
42078536,Launch HN: Codebuff (YC F24) – CLI tool that writes code for you,"Hey HN! We’re James and Brandon building Codebuff (<a href=""https:&#x2F;&#x2F;codebuff.com"">https:&#x2F;&#x2F;codebuff.com</a>). Codebuff is like Cursor Composer, but in your terminal: it modifies files based on your natural language requests. You can try it with `npm i -g codebuff` and start using it immediately for free. We have no login gate, and we give all accounts up to $20 worth of credits.<p>Codebuff is different because we simplified the input to one step: you type what you want done in your terminal and hit enter. Then Codebuff looks through your whole codebase and makes the edits it wants, to existing source files or new ones. It also can run your tests, the type checker, or install packages to fulfill your request.<p>Demo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dQ0NOMsu0dA"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=dQ0NOMsu0dA</a><p>It all started at a hackathon. I was trying out Sonnet 3.5 which had recently come out and seeing if I could use it to write code. The script I cobbled together that day pulled codebase context in one step and used it to rewrite files with changes in the second step. This two step process still exists today. Incidentally, my hackathon script worked rather poorly and my demo failed to produce any useful code.<p>But that weekend I thought about the kind of errors it made, and realized that with more context on our codebase, it might have been able to get the change right. For example, it tried to create an endpoint on our server (at my previous startup), but it didn&#x27;t know that you needed to edit 3 specific files to do this (yeah... our backend was not that clean). So I hand-wrote a guide to our codebase, like I was instructing a new hire. I put it in a markdown file and passed it into Sonnet 3.5&#x27;s system prompt.  And the crazy thing is that it started producing wayyy better code. So, I started getting excited. In fact, this code guide idea still exists in Codebuff today as knowledge.md files which are automatically read on every request.<p>I didn&#x27;t think of this project as a startup idea at first. I thought it was just a simple script anyone could write. But after another week, I could see there were more problems to solve and it should be a product.<p>In the week between applying to YC and the interview, I could not get Codebuff to edit files consistently. I tried many prompting strategies to get it to replace strings in the original file, but nothing worked reliably. How could I face my interviewer if I could not get something basic like this to work? On the day before my interview, in a Hail Mary attempt, I fine-tuned GPT-4o to turn Claude&#x27;s sketch of changes into a git patch, which would add and remove lines to make the edits. I only finished generating the training data late at night, and the fine-tuning job ran as I slept.<p>And, holy hell, the next morning it worked! I pushed it to production just in time for my YC interview with Dalton. Soon after, Brandon joined and we were off to the races.<p>So, how does Codebuff work exactly? You invoke it in your terminal, and it starts by running through the source files in that directory and subdirectories and parsing out all the function and class names (or equivalents in 11 languages). We use the tree-sitter library to do this. It builds out a codebase map that includes these symbols and the file tree.<p>Then, it fires off a request to Claude Haiku 3.5 to cache this codebase context so user inputs can be responded to with lower latency. (Prompt caching is OP!). We have a stateless server that passes messages along to Anthropic or OpenAI. We use websockets to ferry data back and forth to clients. We didn&#x27;t have authentication or even a database for the first three months. Codebuff was free to install and used our API keys for all requests. Luckily, no one exploited us for too much free Claude usage haha. Major thanks to Brandon for saving this situation by building out our database (Postgres + Drizzle), server (Bun, hosted on Render, auth (using the free Auth.js), website (NextJS also hosted on Render), billing (Stripe), logging (BetterStack), and dashboard (Retool). This is the best tech stack I’ve ever had.<p>When the user sends an input message, we prompt Claude to pick files that would be relevant (step 1). After picking files, we load them into context and the agent responds. It invokes tools using xml tags that we parse. It literally writes out &lt;edit_file path=&quot;src&#x2F;app.ts&quot;&gt;…&lt;&#x2F;edit_file&gt; to edit a particular file, and has other tags to run terminal commands, or to ask to read more files. This is all we really need, since Anthropic has already trained Claude with very similar tools reach state of the art on the SWE benchmark.<p>Codebuff has limited free usage, but if you like it you can pay $99&#x2F;mo to get more credits. We realize this is a lot more than competitors, but that’s because we do more expensive LLM calls with more context.<p>We’re already seeing Codebuff used in surprising ways. One user racked up a $500 bill by building out two Flutter apps in parallel. He never even looked at the code it generated. Instead, he had long conversations with Codebuff to make progress and fix errors, until the apps were built to his satisfaction. Many users built real apps over a weekend for their teams and personal use.<p>Of course, those aren&#x27;t the typical use cases. Users also frequently use Codebuff to write unit tests. They would build a feature in parallel with unit tests and have Codebuff do loops to fix up the code until the tests pass. They would also ask it to do drudge work like set up Oauth flows or API scaffolding.<p>What&#x27;s really exciting with all of these examples is that we&#x27;re seeing people&#x27;s creativity becoming unbridled. They&#x27;re spending more of their time thinking about architecture and design, instead of implementation details. It&#x27;s so cool that we&#x27;re just at the beginning, and the technology is only going to improve from here.<p>If you would want to use Codebuff inside your own systems, we have an alpha SDK that exposes the same natural language interface for your apps to call and receive code edits! You can sign up here for early access: <a href=""https:&#x2F;&#x2F;codebuff.retool.com&#x2F;form&#x2F;c8b15919-52d0-4572-aca5-533317403dde"" rel=""nofollow"">https:&#x2F;&#x2F;codebuff.retool.com&#x2F;form&#x2F;c8b15919-52d0-4572-aca5-533...</a>.<p>Thank you for reading! We’re excited for you to try out Codebuff and let us know what you think!",285,jahooma,1730999188,story,
41641522,Hacker plants false memories in ChatGPT to steal user data in perpetuity,,284,nobody9999,1727215983,story,https://arstechnica.com/security/2024/09/false-memories-planted-in-chatgpt-give-hacker-persistent-exfiltration-channel/
44608275,How I keep up with AI progress,,284,itzlambda,1752863799,story,https://blog.nilenso.com/blog/2025/06/23/how-i-keep-up-with-ai-progress/
41947566,Detecting when LLMs are uncertain,,283,trq_,1729878052,story,https://www.thariq.io/blog/entropix/
40475578,Google scrambles to manually remove weird AI answers in search,,283,rntn,1716650667,story,https://www.theverge.com/2024/5/24/24164119/google-ai-overview-mistakes-search-race-openai
39109481,Why is machine learning 'hard'? (2016),,283,jxmorris12,1706042605,story,https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html
42378335,Training LLMs to Reason in a Continuous Latent Space,,283,omarsar,1733847977,story,https://arxiv.org/abs/2412.06769
40857174,GraphRAG is now on GitHub,"The team at Microsoft is pleased to announce that GraphRAG is now available in open-source!<p>Check out the announcement video: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;dsesHoTXyk0"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;dsesHoTXyk0</a><p>GraphRAG is a research project from Microsoft exploring the use of knowledge graphs and large language models for enhanced retrieval augmented generation. It is an end-to-end system for richly understanding text-heavy datasets by combining text extraction, network analysis, LLM prompting, and summarization.<p>For more details on GraphRAG check out aka.ms&#x2F;graphrag<p>Try out the Python code on your own machine: <a href=""https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;graphrag"">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;graphrag</a><p>Easily deploy GraphRAG in Azure: <a href=""https:&#x2F;&#x2F;github.com&#x2F;Azure-Samples&#x2F;graphrag-accelerator"">https:&#x2F;&#x2F;github.com&#x2F;Azure-Samples&#x2F;graphrag-accelerator</a><p>Leave a comment below for what you want to build with GraphRAG!",282,alexchaomander,1719931279,story,https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/
41519240,Kolmogorov-Arnold networks may make neural networks more understandable,,282,isaacfrond,1726136045,story,https://www.quantamagazine.org/novel-architecture-makes-neural-networks-more-understandable-20240911/
41808683,Understanding the Limitations of Mathematical Reasoning in LLMs,,282,hnhn34,1728647706,story,https://arxiv.org/abs/2410.05229
44053744,"LLM function calls don't scale; code orchestration is simpler, more effective",,282,jngiam1,1747847932,story,https://jngiam.bearblog.dev/mcp-large-data/
44945379,Show HN: Fractional jobs – part-time roles for engineers,"I&#x27;m Taylor, I spent about a year as a Fractional Head of Product. It was my first time not in a full-time W2 role, and I quickly learned that the hardest part of the job wasn&#x27;t doing the Product work (I was a PM for 10+ years), it was finding good clients to work with.<p>So I built Fractional Jobs.<p>The goal is to help more people break out of W2 life and into their own independent careers by helping them find great clients to work with.<p>We find and vet the clients, and then engineers can request intros to any that seem like a good fit. We&#x27;ll make the intro assuming the client opts in after seeing your profile.<p>We have 9 open engineering roles right now: - 2x Fractional CTO - 2x AI engineers - 3x full-stack - 1x staff frontend  - 1x mobile",281,tbird24,1755551439,story,https://www.fractionaljobs.io
38431482,$10M AI Mathematical Olympiad Prize,,281,jasondavies,1701087906,story,https://aimoprize.com/
44422480,Show HN: TokenDagger – A tokenizer faster than OpenAI's Tiktoken,"TokenDagger is a drop-in replacement for OpenAI’s Tiktoken (the tokenizer behind Llama 3, Mistral, GPT-3.*, etc.). It’s written in C++ 17 with thin Python bindings, keeps the exact same BPE vocab&#x2F;special-token rules, and focuses on raw speed.<p>I’m teaching myself LLM internals by re-implementing the stack from first principles. Profiling TikToken’s Python&#x2F;Rust implementation showed a lot of time was spent doing regex matching. Most of my perf gains come from a) using a faster jit-compiled regex engine; and b) simplifying the algorithm to forego regex matching special tokens at all.<p>Benchmarking code is included. Notable results show: - 4x faster code sample tokenization on a single thread. - 2-3x higher throughput when tested on a 1GB natural language text file.",281,matthewolfe,1751286838,story,https://github.com/M4THYOU/TokenDagger
39647105,OpenAI board reappoints Altman and adds three other directors,,281,coloneltcb,1709935639,story,https://www.reuters.com/technology/sam-altman-return-openais-board-information-reports-2024-03-08/
42074083,Even Microsoft Notepad is getting AI text editing now,,281,redbell,1730961621,story,https://www.theverge.com/2024/11/6/24289707/microsoft-notepad-ai-text-editing-rewrite
44219279,What happens when people don't understand how AI works,,280,rmason,1749414332,story,https://www.theatlantic.com/culture/archive/2025/06/artificial-intelligence-illiteracy/683021/
44725202,Show HN: I built an AI that turns any book into a text adventure game,"It&#x27;s a web app that uses AI to turn any book into a playable text adventure. Your favorite book, but your choices, hence your story. You can even &quot;remix&quot; the genre like playing Dune as a noir detective story.<p>Note:  Work in progress. Suggestions are welcome.",280,rcrKnight,1753805834,story,https://www.kathaaverse.com/
40667102,AMD's MI300X Outperforms Nvidia's H100 for LLM Inference,,280,fvv,1718265476,story,https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/
39201182,Show HN: A simple ChatGPT prompt builder,Any Ideas&#x2F;Suggestions are welcome :),280,mitenmit,1706687254,story,https://mitenmit.github.io/gpt/
39770908,"Show HN: GritQL, a Rust CLI for rewriting source code","Hi everyone!<p>I’m excited to open source GritQL, a Rust CLI for searching and transforming source code.<p>GritQL comes from my experiences with conducting large scale refactors and migrations.<p>Usually, I would start exploring a codebase with grep. This is easy to start with, but most migrations end up accumulating additional requirements like ensuring the right packages are imported and excluding cases which don’t have a viable migration path.<p>Eventually, to build a complex migration, I usually ended up having to write a full codemod program with a tool like jscodeshift. This comes with its own problems:<p>- Most of the exploratory work has to be abandoned as you figure out how to represent your original regex search as an AST. - Reading&#x2F;writing a codemod requires mentally translating from AST names back to what source code actually looks like. - Performance is often an afterthought, so iterating on a large codemod can be painfully slow. - Codemod frameworks are language-specific, so if you’re hopping between multiple languages—or trying to migrate a shared API—you have to learn different tools.<p>GritQL is an attempt to develop a powerful middle ground: - Exploratory analysis is easy: just put a code snippet in backticks and use $metavariables for placeholders. - Incrementally add complexity by introducing side conditions with where clauses. - Reuse named patterns to avoid rebuilding queries, and use shared patterns from our standard library for common tasks like ensuring modules are imported. - Iterate on large codebases quickly: we use Rust for maximum performance<p>GritQL has already been used on thousands of repositories for complex migrations[1] but we&#x27;re excited to collaborate more with the open source community.<p>[1] Ex. <a href=""https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-python&#x2F;discussions&#x2F;742"">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-python&#x2F;discussions&#x2F;742</a>",280,morgante,1710962591,story,https://github.com/getgrit/gritql
38530207,"Sorry, but a new prompt for GPT-4 is not a paper",,279,georgehill,1701781563,story,https://twitter.com/hbouammar/status/1731970658278469714
42845933,"I trusted an LLM, now I'm on day 4 of an afternoon project",,278,nemofoo,1738013879,story,https://nemo.foo/blog/day-4-of-an-afternoon-project
43095811,HP Acquires Humane's AI Software,,278,colesantiago,1739916905,story,https://humane.com/media/humane-hp
41895746,The AI Investment Boom,,278,m-hodges,1729436192,story,https://www.apricitas.io/p/the-ai-investment-boom
43854776,When ChatGPT broke the field of NLP: An oral history,,278,mathgenius,1746085899,story,https://www.quantamagazine.org/when-chatgpt-broke-an-entire-field-an-oral-history-20250430/
39048948,My AI costs went from $100 to less than $1/day: Fine-tuning Mixtral with GPT4,,278,ignoramous,1705617830,story,https://twitter.com/wenquai/status/1748016021808595242
40597503,The right not to be subjected to AI profiling based on publicly available data,,277,tokai,1717682597,story,https://link.springer.com/article/10.1007/s13347-023-00616-9
43420152,How I accepted myself into Canada's largest AI hackathon,,277,fastcall,1742449341,story,https://fastcall.dev/posts/genai-genesis-firebase/
40453077,"Microsoft outage affects Bing, Copilot, DuckDuckGo and ChatGPT internet search",,276,marban,1716461600,story,https://www.bleepingcomputer.com/news/microsoft/microsoft-outage-affects-bing-copilot-duckduckgo-and-chatgpt-internet-search/
40746773,Show HN: Eidos – Offline alternative to Notion,"I&#x27;m a big fan of Notion, having used it for 7 years. When I first met Notion, it was just a block document editor, and it didn&#x27;t excite me until it released the Database. Later, I learned about Airtable, and a bunch of similar products, all of them are SaaS, with such powerful tables but poor performance. Why isn&#x27;t there a personal offline version table? Most of the time, I don&#x27;t need to collaborate with others. What I really want is a personalized, offline version of Notion with better performance and more flexibility. So that I can have full control over my data.<p>Notion, like most SaaS products, is not open-source, so I can&#x27;t customize it to my heart&#x27;s content. I can only wait and vote for new features, but I can code, and I don&#x27;t want to wait. I really like the concepts of FOSS, solid, and local-first. SaaS could die. Long may the SQLite. So, I built Eidos based on sqlite for managing my personal data throughout my lifetime in one place. Eidos is a long-term project for me. It looks like Notion, but the core is more like &quot;obsidian.sqlite&quot; with a powerful extension system.<p>Here are a few key ideas:<p>- Eidos is built based on sqlite-wasm and runs entirely in the browser. It can be used immediately, with no installation or configuration required. It&#x27;s a pure PWA, with full offline support.<p>- A block-styled document editor and an Airtable-like table, built on top of SQLite, where each table is a real SQLite table.<p>- A powerful extension system inspired by Figma plugin and Cloudflare worker. You can write scripts in TypeScript directly in the browser. It is easy to manipulate data in docs, tables, and the file system. It also supports API.<p>- If you&#x27;re not a developer, you&#x27;re still in luck. We&#x27;re living in an AI era. LLM empowers people to craft their own software without writing any code. Eidos deeply integrated with LLM. You can translate, summarize, talk to your data, process table data in batches, and more. It makes your life easier with AI. You can fully customize your prompts and freely choose your LLM provider, without being locked to any vendor.<p>To be honest, so far, there are still some bugs and shortcomings, and it hasn&#x27;t yet reached my envisioned perfection. There is still some work to be done, but the basic framework has taken shape. I&#x27;ve been working on it for a year and have eaten my own dog food for the past half year. To help Eidos become better (and celebrate the release of the Elden Ring&#x27;s DLC), I&#x27;ve decided to make it open-source and gather more feedback. Now, I&#x27;m going to take a break and play Shadow of the Erdtree.<p>- <a href=""https:&#x2F;&#x2F;eidos.space"" rel=""nofollow"">https:&#x2F;&#x2F;eidos.space</a>",276,mayne,1718951402,story,https://github.com/mayneyao/eidos
42521744,Why OpenAI's Structure Must Evolve to Advance Our Mission,,276,meetpateltech,1735304258,story,http://openai.com/index/why-our-structure-must-evolve-to-advance-our-mission
42256093,Ask HN: Recommendation for a SWE looking to get up to speed with latest on AI,"I am looking to get up to speed with the latest things happening in AI, I use ChatGPT almost everyday and i last used the open AI api for 3.5 last year. I am looking for a tech blogs like HN to keep updated on things AI, I came across https:&#x2F;&#x2F;simonwillison.net&#x2F; but it appears fragmented",275,Rizu,1732715720,story,
40685644,Snowden: ""They've gone full mask-off: do not ever trust OpenAI or its products"",,275,underlogic,1718403218,story,https://twitter.com/Snowden/status/1801610725229498403
38560041,OpenAI employees did not want to go work for Microsoft,,273,apsec112,1701974428,story,https://www.businessinsider.com/openai-employees-did-not-want-to-work-for-microsoft-2023-12
44009321,Show HN: KVSplit – Run 2-3x longer contexts on Apple Silicon,"I discovered that in LLM inference, keys and values in the KV cache have very different quantization sensitivities. Keys need higher precision than values to maintain quality.<p>I patched llama.cpp to enable different bit-widths for keys vs. values on Apple Silicon. The results are surprising:<p>- K8V4 (8-bit keys, 4-bit values): 59% memory reduction with only 0.86% perplexity loss - K4V8 (4-bit keys, 8-bit values): 59% memory reduction but 6.06% perplexity loss - The configurations use the same number of bits, but K8V4 is 7× better for quality<p>This means you can run LLMs with 2-3× longer context on the same Mac. Memory usage scales with sequence length, so savings compound as context grows.<p>Implementation was straightforward:  1. Added --kvq-key and --kvq-val flags to llama.cpp 2. Applied existing quantization logic separately to K and V tensors 3. Validated with perplexity metrics across context lengths 4. Used Metal for acceleration (with -mlong-calls flag to avoid vectorization issues)<p>Benchmarked on an M4 MacBook Pro running TinyLlama with 8K context windows. Compatible with Metal&#x2F;MPS and optimized for Apple Silicon.<p>GitHub: <a href=""https:&#x2F;&#x2F;github.com&#x2F;dipampaul17&#x2F;KVSplit"">https:&#x2F;&#x2F;github.com&#x2F;dipampaul17&#x2F;KVSplit</a>",272,dipampaul17,1747425898,story,https://github.com/dipampaul17/KVSplit
39176570,Show HN: WhisperFusion – Low-latency conversations with an AI chatbot,WhisperFusion builds upon the capabilities of open source tools WhisperLive and WhisperSpeech to provide a seamless conversations with an AI chatbot.,272,mfilion,1706538190,story,https://github.com/collabora/WhisperFusion
42517260,OpenAI is Visa – Buttering up the government to retain a monopoly,,272,gpi,1735242291,story,https://sherwood.news/tech/openai-is-visa/
42261909,Show HN: Voice-Pro – AI Voice Cloning,"Imagine creating a podcast where Mark Zuckerberg interviews Elon Musk – using their actual voices?<p>What sounds like science fiction is now reality.<p>Voice-Pro is an open-source Gradio WebUI that breaks the boundaries of audio manipulation.<p>Powered by cutting-edge Whisper engines, this tool turns voice replication into child&#x27;s play.<p>Key Features:<p>- Zero-shot Voice Cloning<p>- Voice Changer with 50+ Celebrity Voices<p>- YouTube Audio Downloading<p>- Vocal Isolation<p>- Multi-Language Text-to-Speech (Edge-TTS, F5-TTS)<p>- Multi-Language Translation<p>- Powered by Whisper Engines (Whisper, Faster-Whisper, Whisper-Timestamped)<p>Video Demos:<p>1. Voice-Pro Usage Tutorial: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;z8g8LMhoh_o"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;z8g8LMhoh_o</a><p>2. Voice Cloning Celebrity Podcast Demo: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;Wfo7vQCD4no"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;Wfo7vQCD4no</a><p>3. Full Demo Playlist: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLwx5dnMDVC9Y7dAjm9r26CZUw1uU5VIeq"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLwx5dnMDVC9Y7dAjm9r26...</a><p>Whether you&#x27;re a content creator, developer, or audio experiment enthusiast,<p>Voice-Pro provides a user-friendly interface to push the boundaries of audio manipulation.<p>GitHub: <a href=""https:&#x2F;&#x2F;github.com&#x2F;abus-aikorea&#x2F;voice-pro"">https:&#x2F;&#x2F;github.com&#x2F;abus-aikorea&#x2F;voice-pro</a>",271,abuskorea,1732761421,story,https://github.com/abus-aikorea/voice-pro
41873204,Kagi Update: AI Image Filter for Search Results,,271,lkellar,1729195325,story,https://help.kagi.com/kagi/features/exclude-ai-images.html
39289350,"Latest ChatGPT 4 System prompt (1,700 tokens)",,271,simonpure,1707317993,story,https://pastebin.com/vnxJ7kQk
45490578,Show HN: Write It Down – Personal finance tracker,"Everyone’s chasing AI hype. I built a Google Sheet and it quietly took off.<p>In 2020, I made it to track my own finances for income, expenses, savings, yearly summaries etc. I shared it once on Reddit, forgot about it for a year… When I checked back, it had over 130k views and I was honestly stoked!<p>No launch. No funding. No AI. Just a spreadsheet people actually stick with and find useful.<p>I finally gave it a proper home: write-it-down.com Now, more than 2,300 people use it.<p>It’s intentionally boring and that’s why it works.<p>People don’t always need AI. They just need something that actually solves their problem. This isn’t a billion-dollar startup of course, but it taught me more about building products than almost anything else.<p>Build something useful. Solve a real problem. Even if it’s just a simple spreadsheet.<p>So, what’s the most “boring” thing you’ve built that found unexpected traction?",271,LarsenCC,1759753420,story,https://write-it-down.com
43120802,I put my heart and soul into this AI but nobody cares,,270,spzb,1740088596,story,https://newslttrs.com/i-put-my-heart-and-soul-into-this-ai-but-nobody-cares/
39786666,How Chain-of-Thought Reasoning Helps Neural Networks Compute,,270,amichail,1711072247,story,https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/
42559744,Show HN: Watch 3 AIs compete in real-time stock trading,"A live dashboard where you can watch GPT-4, Claude 3, and Gemini analyze market data and make daily stock trades with real money. Each AI explains its reasoning, and you can compare their different approaches to the same data.<p>Link: <a href=""https:&#x2F;&#x2F;trading.snagra.com?utm_source=hn"" rel=""nofollow"">https:&#x2F;&#x2F;trading.snagra.com?utm_source=hn</a> (no signup required)<p>What you can try right now: - Watch live trades from GPT-4, Claude 3, and Gemini - Read each AI&#x27;s full analysis and reasoning - Compare their different interpretations of the same market data - Track their real-time performance and win rates - View historical trades and performance metrics<p>Built this over the holidays to study how different AI models approach financial decisions. Each morning at 9:30 AM EST, the AIs analyze market data and make real trades with $5 stakes.<p>Technical Implementation: - Next.js frontend with real-time updates - Node.js&#x2F;Lambda backend for AI processing - PostgreSQL for trade tracking - Alpaca API for automated trading - Consistent prompts for all models<p>Data Flow: 1. Daily market analysis (9:30 AM EST) 2. Each AI gets identical inputs:    - Financial headlines    - Market summaries    - Technical indicators    - Earnings reports 3. AIs provide:    - Stock picks with reasoning    - Entry&#x2F;exit conditions    - Risk assessment 4. Automated trade execution<p>Note: This is an experiment in AI behavior, not investment advice. The goal is to study how different LLMs interpret financial data and make decisions with real consequences.<p>I&#x27;ll be around to answer questions about the implementation.",270,sunnynagra,1735662778,story,https://trading.snagra.com
44725306,Launch HN: Hyprnote (YC S25) – An open-source AI meeting notetaker,"Hi HN! We&#x27;re Yujong, John, Duck, and Sung from Hyprnote (<a href=""https:&#x2F;&#x2F;hyprnote.com"" rel=""nofollow"">https:&#x2F;&#x2F;hyprnote.com</a>). We&#x27;re building an open-source, privacy-first AI note-taking app that runs fully on-device. Think of it as an open-source Granola. No Zoom bots, no cloud APIs, no data ever leaves your machine.<p>Source code: <a href=""https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote"">https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote</a> Demo video: <a href=""https:&#x2F;&#x2F;hyprnote.com&#x2F;demo"" rel=""nofollow"">https:&#x2F;&#x2F;hyprnote.com&#x2F;demo</a><p>We built Hyprnote because some of our friends told us that their companies banned certain meeting notetakers due to data concerns, or they simply felt uncomfortable sending data to unknown servers. So they went back to manual note-taking - losing focus during meetings and wasting time afterward.<p>We asked: could we build something just as useful, but completely local?<p>Hyprnote is a desktop app that transcribes and summarizes meetings on-device. It captures both your mic input and system audio, so you don&#x27;t need to invite bots. It generates a summary based on the notes you take. Everything runs on local AI models by default, using Whisper and HyprLLM. HyprLLM is our proof-of-concept model fine-tuned from Qwen3 1.7B. We learned that summarizing meetings is a very nuanced task and that a model&#x27;s raw intelligence (or weight) doesn&#x27;t matter THAT much. We&#x27;ll release more details on evaluation and training once we finish the 2nd iteration of the model (still not that good  we can make it a lot better).<p>Whisper inference: <a href=""https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote&#x2F;blob&#x2F;main&#x2F;crates&#x2F;whisper-local&#x2F;src&#x2F;model.rs"">https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote&#x2F;blob&#x2F;main&#x2F;crates&#x2F;whispe...</a><p>AEC inference: <a href=""https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote&#x2F;blob&#x2F;main&#x2F;crates&#x2F;aec&#x2F;src&#x2F;lib.rs"">https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote&#x2F;blob&#x2F;main&#x2F;crates&#x2F;aec&#x2F;sr...</a><p>LLM inference: <a href=""https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote&#x2F;blob&#x2F;main&#x2F;crates&#x2F;llama&#x2F;src&#x2F;lib.rs"">https:&#x2F;&#x2F;github.com&#x2F;fastrepl&#x2F;hyprnote&#x2F;blob&#x2F;main&#x2F;crates&#x2F;llama&#x2F;...</a><p>We also learned that for some folks, having full data controllability was as important as privacy. So we support custom endpoints, allowing users to bring in their company&#x27;s internal LLM. For teams that need integrations, collaboration, or admin controls, we&#x27;re working on an optional server component that can be self-hosted. Lastly, we&#x27;re exploring ways to make Hyprnote work like VSCode, so you can install extensions and build your own workflows around your meetings.<p>We believe privacy-first tools, powered by local models, are going to unlock the next wave of real-world AI apps.<p>We&#x27;re here and looking forward to your comments!",270,yujonglee,1753806248,story,
41583605,OpenAI Threatening to Ban Users for Asking Strawberry About Its Reasoning,,270,EgoIncarnate,1726683732,story,https://futurism.com/the-byte/openai-ban-strawberry-reasoning
40857009,Show HN: Adding Mistral Codestral and GPT-4o to Jupyter Notebooks,"Hey HN! We’ve forked Jupyter Lab and added AI code generation features that feel native and have all the context about your notebook. You can see a demo video (2 min) here: <a href=""https:&#x2F;&#x2F;www.tella.tv&#x2F;video&#x2F;clxt7ei4v00rr09i5gt1laop6&#x2F;view"">https:&#x2F;&#x2F;www.tella.tv&#x2F;video&#x2F;clxt7ei4v00rr09i5gt1laop6&#x2F;view</a><p>Try a hosted version here: <a href=""https:&#x2F;&#x2F;pretzelai.app"" rel=""nofollow"">https:&#x2F;&#x2F;pretzelai.app</a><p>Jupyter is by far the most used Data Science tool. Despite its popularity, it still lacks good code-generation extensions. The flagship AI extension <i>jupyter-ai</i> lags far behind in features and UX compared to modern AI code generation and understanding tools (like <a href=""https:&#x2F;&#x2F;www.continue.dev"">https:&#x2F;&#x2F;www.continue.dev</a> and <a href=""https:&#x2F;&#x2F;www.cursor.com"" rel=""nofollow"">https:&#x2F;&#x2F;www.cursor.com</a>). Also, GitHub Copilot <i>still</i> isn’t supported in Jupyter, more than 2 years after its launch. We’re solving this with Pretzel.<p>Pretzel is a free and open-source fork of Jupyter. You can install it locally with “pip install pretzelai” and launch it with “pretzel lab”. We recommend creating a new python environment if you already have jupyter lab installed. Our GitHub README has more information: <a href=""https:&#x2F;&#x2F;github.com&#x2F;pretzelai&#x2F;pretzelai"">https:&#x2F;&#x2F;github.com&#x2F;pretzelai&#x2F;pretzelai</a><p>For our first iteration, we’ve shipped 3 features:<p>1. Inline Tab autocomplete: This works similar to GitHub Copilot. You can choose between Mistral Codestral or GPT-4o in the settings<p>2. Cell level code generation: Click Ask AI or press Cmd+K &#x2F; Ctrl+K to instruct AI to generate code in the active Jupyter Cell. We provide relevant context from the current notebook to the LLM with RAG. You can refer to existing variables in the notebook using the @variable syntax (for dataframes, it will pass the column names to the LLM)<p>3. Sidebar chat: Clicking the blue Pretzel Icon on the right sidebar opens this chat (Ctrl+Cmd+B &#x2F; Ctrl+Alt+B). This chat always has context of your current cell or any selected text. Here too, we use RAG to send any relevant context from the current notebook to the LLM<p>All of these features work out-of-the-box via our “AI Server” but you have the option of using your own OpenAI API Key. This can be configured in the settings (Menu Bar &gt; Settings &gt; Settings Editor &gt; Search for Pretzel). If you use your own OpenAI API Key but don’t have a Mistral API key, be sure to select OpenAI as the inline code completion model in the settings.<p>These features are just a start. We&#x27;re building a modern version of Jupyter. Our roadmap includes frictionless, realtime collaboration (think pair-programming, comments, version history), full-fledged SQL support (both in code cells and as a standalone SQL IDE), a visual analysis builder, a VSCode-like coding experience powered by Monaco, and 1-click dashboard creation and sharing straight from your notebooks.<p>We’d love for you to try Pretzel and send us any feedback, no matter how minor (see my bio for contact info, or file a GitHub issue here: <a href=""https:&#x2F;&#x2F;github.com&#x2F;pretzelai&#x2F;pretzelai&#x2F;issues"">https:&#x2F;&#x2F;github.com&#x2F;pretzelai&#x2F;pretzelai&#x2F;issues</a>)",269,prasoonds,1719930236,story,https://github.com/pretzelai/pretzelai/blob/main/README.md
45719669,Microsoft needs to open up more about its OpenAI dealings,,269,zerosizedweasle,1761563967,story,https://www.wsj.com/tech/ai/microsoft-needs-to-open-up-more-about-its-openai-dealings-59102de8
42734478,Let's talk about AI and end-to-end encryption,,269,chmaynard,1737093025,story,https://blog.cryptographyengineering.com/2025/01/17/lets-talk-about-ai-and-end-to-end-encryption/
44849129,"Ch.at – A lightweight LLM chat service accessible through HTTP, SSH, DNS and API",,269,ownlife,1754765977,story,https://ch.at/
45619544,OpenAI Needs $400B In The Next 12 Months,,268,chilipepperhott,1760722862,story,https://www.wheresyoured.at/openai400bn/
40710154,Show HN: Token price calculator for 400+ LLMs,"Hey HN! Tokencost is a utility library for estimating LLM costs. There are hundreds of different models now, and they all have their own pricing schemes. It’s difficult to keep up with the pricing changes, and it’s even more difficult to estimate how much your prompts and completions will cost until you see the bill.<p>Tokencost works by counting the number of tokens in prompt and completion messages and multiplying that number by the corresponding model cost. Under the hood, it’s really just a simple cost dictionary and some utility functions for getting the prices right. It also accounts for different tokenizers and float precision errors.<p>Surprisingly, most model providers don&#x27;t actually report how much you spend until your bills arrive. We built Tokencost internally at AgentOps to help users track agent spend, and we decided to open source it to help developers avoid nasty bills.",268,Areibman,1718653493,story,https://github.com/AgentOps-AI/tokencost
42806616,Show HN: Open-source AI video editor,"Hey HN community! I&#x27;m one of the lead devs of this project at fal.ai and we created an open source lightweight video editor powered by the latest media AI models. The main goal was to tackle some challenges when dealing with complex media handling and encoding on the browser.<p>It all started as an internal experiment but as we tackled some of the issues it was clear there could be some value sharing it with the open source community.<p>Some of the key points and tech stack details:<p>- It uses IndexedDb, so all data is local (i.e. no auth, no cloud db)<p>- Multiple AI models for video, image, music and voice-over. APIs are provided by fal.ai<p>- Built with the typical React+Next.js, Shadcn front-end<p>- Used remotion.dev for the realtime video preview (this is such a great project, without it the codebase would be twice as large)<p>- File uploads so you can bring your own media by uploadthing.com<p>- ffmpeg for encoding the final video and also some ui tricks, like the audio waveform<p>We deployed a version of it and for now it&#x27;s free to use. We do plan to add some rate limiting and a bring your own API Key next, but it&#x27;s open source and I&#x27;m curious about what the community will build on top of it, or derive from it. Customize your own video app and if you do, please share.<p>If you have any questions, hit me up!",268,drochetti,1737657278,story,https://github.com/fal-ai-community/video-starter-kit
45521032,Palisades Fire suspect's ChatGPT history to be used as evidence,,267,quuxplusone,1759960414,story,https://www.rollingstone.com/culture/culture-news/chatgpt-palisades-fire-suspect-1235443216/
40260259,Show HN: gpudeploy.com – ""Airbnb"" for GPUs,"Hi HN,<p>YC w24 company here. We just pivoted from drone delivery to build gpudeploy.com, a website that routes on-demand traffic for GPU instances to idle compute resources.<p>The experience is similar to lambda labs, which we’ve really enjoyed for training our robotics models, but their GPUs are never available for on-demand. We are also trying to make it more no-nonsense (no hidden fees, no H100 behind “contact sales”, etc.).<p>The tech to make this work is actually kind of nifty, we may do an in-depth HN post on that soon.<p>Right now, we have H100s, a few RTX 4090s and a GTX 1080 Ti online. Feel free to try it out!<p>Also, if you’ve got compute sitting around (a GPU cluster, a crypto mining operation or just a GPU) or if you’re an AI company with idle compute (hopefully not in a Stability AI way) and want to see some ROI, it’s very simple and flexible to hook it up to our site and you’ll maybe get a few researchers using your compute.<p>Nice rest of the week!",267,nicowaltz,1714856635,story,https://www.gpudeploy.com
41291700,Show HN: Srcbook – A TypeScript notebook for rapid prototyping,"Srcbook (”source-book”) is an open-source TypeScript notebook that runs locally, powered by Node.js. It shines for rapid prototyping, code exploration, and collaborating on ideas. It’s inspired by Python’s Jupyter and Elixir’s Livebook.<p>Key features:<p>- Full npm ecosystem access<p>- AI-assisted coding (OpenAI, Anthropic, or local models), it can iterate on the cells for you with a code diff UX that you accept&#x2F;reject for a given code cell, generate entire Srcbooks, fix compilation issues, etc…<p>- Exports to valid markdown for easy sharing and version control<p>Try it now: `npx srcbook start`<p>Examples Srcbooks to explore: <a href=""https:&#x2F;&#x2F;hub.srcbook.com"">https:&#x2F;&#x2F;hub.srcbook.com</a><p>We built this because we needed a Jupyter-like environment for TypeScript, we hope others like it as much as we do! Feedback and contributions are super appreciated.<p>(edit: formatting)",267,nichochar,1724080196,story,https://github.com/srcbookdev/srcbook
41743327,Show HN: Open source framework OpenAI uses for Advanced Voice,"Hey HN, we&#x27;ve been working with OpenAI for the past few months on the new Realtime API.<p>The goal is to give everyone access to the same stack that underpins Advanced Voice in the ChatGPT app.<p>Under the hood it works like this: - A user&#x27;s speech is captured by a LiveKit client SDK in the ChatGPT app - Their speech is streamed using WebRTC to OpenAI’s voice agent - The agent relays the speech prompt over websocket to GPT-4o - GPT-4o runs inference and streams speech packets (over websocket) back to the agent - The agent relays generated speech using WebRTC back to the user’s device<p>The Realtime API that OpenAI launched is the websocket interface to GPT-4o. This backend framework covers the voice agent portion. Besides having additional logic like function calling, the agent fundamentally proxies WebRTC to websocket.<p>The reason for this is because websocket isn’t the best choice for client-server communication. The vast majority of packet loss occurs between a server and client device and websocket doesn’t provide programmatic control or intervention in lossy network environments like WiFi or cellular. Packet loss leads to higher latency and choppy or garbled audio.",266,russ,1728061264,story,https://github.com/livekit/agents
44438065,Building a Personal AI Factory,,266,derek,1751404469,story,https://www.john-rush.com/posts/ai-20250701.html
41470074,Hardware Acceleration of LLMs: A comprehensive survey and comparison,,266,matt_d,1725660554,story,https://arxiv.org/abs/2409.03384
45706792,Show HN: Diagram as code tool with draggable customizations,In the past I&#x27;ve used declarative diagram generation tools like Mermaid.js a lot for quickly drawing up things but for presentations or deliverables I find that I have to then move the generated diagrams over to a tool like Lucidchart which allows full control of the organization and customization.<p>Therefore I am now working on this to combine the benefits of both into just one tool which can do both functions.<p>The project is certainly in the early stages but if you find yourself making architecture diagrams I&#x27;d love to hear your thoughts on the idea or even a Github issue for a feature request!<p>One of the workflows I&#x27;m targeting is when an AI generates the first draft of the diagram (all the LLMs know .mmd syntax) and then the user can then customize it to their liking which I think can drastically speed up making complex diagrams!,266,RohanAdwankar,1761424738,story,https://github.com/RohanAdwankar/oxdraw
45359524,"Learning Persian with Anki, ChatGPT and YouTube",,265,cjauvin,1758717907,story,https://cjauvin.github.io/posts/learning-persian/
42258540,Show HN: Feels Like Paper,"&quot;Feels Like Paper!&quot; is a series of prototypes about augmenting physical paper through AI. Various ML models, LLMs and a mixed reality headset are used to infuse physical paper and ink with properties of the digital world without compromising on their physical traits.",265,MoroL,1732732984,story,https://www.lukasmoro.com/paper
39495476,Does offering ChatGPT a tip cause it to generate better text?,,265,_Microft,1708811853,story,https://minimaxir.com/2024/02/chatgpt-tips-analysis/
45896707,Hard drives on backorder for two years as AI data centers trigger HDD shortage,,265,pabs3,1762925801,story,https://www.tomshardware.com/pc-components/hdds/ai-triggers-hard-drive-shortage-amidst-dram-squeeze-enterprise-hard-drives-on-backorder-by-2-years-as-hyperscalers-switch-to-qlc-ssds
41045834,"Show HN: Briefer – Multiplayer notebooks with schedules, SQL, and built-in LLMs","Hi HN! We&#x27;re Lucas and Lucas from Briefer and we&#x27;re building better notebooks.<p>Our notebooks are kind of a mix between Notion and Jupyter with extra features, like the ability to schedule notebooks, turn them into dashboards and apps, and write SQL queries whose results turn into data frames automatically.<p>We&#x27;re building better notebooks because we think they&#x27;re a great idea poorly executed - for three reasons.<p>The first problem with notebooks is that they&#x27;re difficult to share. Non-technical people don&#x27;t want to download docker containers and install Python libraries to see what the data team is doing. Then, the data team takes screenshots of their work and pastes them somewhere else. The issue with this approach is that the data gets stale, and the output is not interactive, so it&#x27;s difficult to get feedback and iterate.<p>The second problem with notebooks is that they get too messy too quickly. One morning you have 10 blocks, and by the end of the day you have 192, but only six of them are meant to be seen by other people and the rest is just you jiu-jitsuing with the data. Consequently, even if non-technical people could see your work, they&#x27;d have a hard time figuring out what&#x27;s happening.<p>The third problem with notebooks is that it takes too much work to do simple things like when you want to query a database. In that case, you need to have the database credentials in your machine, and you have to write a bunch of wrapper code. Calling APIs, plotting simple graphs, and adding interactive components is equally as annoying.<p>We&#x27;re solving the sharing problem by bringing notebooks to the cloud (so you can schedule them) and using CRDTs to manage their state. Whenever you edit a Briefer notebook, we reconcile that using Yjs, and then propagate it to everyone else who&#x27;s editing that notebook. Regarding compute instances, each workspace gets its own, and we provision them on demand.<p>By the way, we manage the execution state of your blocks using Yjs too, which makes it much more stable and responsive across all clients. When you click &quot;run&quot;, for example, we don&#x27;t immediately send a request to run the block. Instead, we change the block&#x27;s state to &quot;execution-requested&quot;. Then, the observers in the back-end react to the change and update the block&#x27;s state with the results. In other words, the front end and the server communicate with each other through the notebook&#x27;s state. One side updates the state, and the other reacts.<p>To solve the &quot;messiness&quot; issue, we&#x27;ve invested a lot of time in designing clean notebooks and allowing you to organize blocks in ways that make them more presentable. In Briefer, you can group multiple blocks into tabs and decide which blocks appear in the published version of your notebook. That way, you can hide all that data wrangling and focus on results. We also know that a vertical format is not always the best way to display results, so you can use your notebook&#x27;s outputs to build dashboards too.<p>Last but not least, we&#x27;re reducing friction in simple tasks like plotting graphs and querying databases. In Briefer, you can plot graphs using a click-through interface, and if you need to plot something more intricate like a Sankey chart there&#x27;s also a built-in AI assistant that you can prompt to generate code. When it comes to queries, we turn their results into dataframes automatically, and you can use SQL to query uploaded files too (we use DuckDB for that).<p>We&#x27;re really happy we get to show this to you all, thank you for reading about it! Please let us know your thoughts and questions in the comments.",265,lucasfcosta,1721741432,story,https://briefer.cloud/launches/notebooks/
45760321,Show HN: I made a heatmap diff viewer for code reviews,"0github.com is a pull request viewer that color-codes every diff line&#x2F;token by how much human attention it probably needs. Unlike PR-review bots, we try to flag not just by &quot;is it a bug?&quot; but by &quot;is it worth a second look?&quot; (examples: hard-coded secret, weird crypto mode, gnarly logic, ugly code).<p>To try it, replace github.com with 0github.com in any pull-request URL. Under the hood, we split the PR into individual files, and for each file, we ask an LLM to annotate each line with a data structure that we parse into a colored heatmap.<p>Examples:<p><a href=""https:&#x2F;&#x2F;0github.com&#x2F;manaflow-ai&#x2F;cmux&#x2F;pull&#x2F;666"" rel=""nofollow"">https:&#x2F;&#x2F;0github.com&#x2F;manaflow-ai&#x2F;cmux&#x2F;pull&#x2F;666</a><p><a href=""https:&#x2F;&#x2F;0github.com&#x2F;stack-auth&#x2F;stack-auth&#x2F;pull&#x2F;988"" rel=""nofollow"">https:&#x2F;&#x2F;0github.com&#x2F;stack-auth&#x2F;stack-auth&#x2F;pull&#x2F;988</a><p><a href=""https:&#x2F;&#x2F;0github.com&#x2F;tinygrad&#x2F;tinygrad&#x2F;pull&#x2F;12995"" rel=""nofollow"">https:&#x2F;&#x2F;0github.com&#x2F;tinygrad&#x2F;tinygrad&#x2F;pull&#x2F;12995</a><p><a href=""https:&#x2F;&#x2F;0github.com&#x2F;simonw&#x2F;datasette&#x2F;pull&#x2F;2548"" rel=""nofollow"">https:&#x2F;&#x2F;0github.com&#x2F;simonw&#x2F;datasette&#x2F;pull&#x2F;2548</a><p>Notice how all the example links have a 0 prepended before github.com. This navigates you to our custom diff viewer where we handle the same URL path parameters as github.com. Darker yellows indicate that an area might require more investigation. Hover on the highlights to see the LLM&#x27;s explanation. There&#x27;s also a slider on the top left to adjust the &quot;should review&quot; threshold.<p>Repo (MIT license): <a href=""https:&#x2F;&#x2F;github.com&#x2F;manaflow-ai&#x2F;cmux"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;manaflow-ai&#x2F;cmux</a>",265,lawrencechen,1761834118,story,https://0github.com
45762160,The Smol Training Playbook: The Secrets to Building World-Class LLMs,,265,kashifr,1761843166,story,https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook
38458683,Extracting training data from ChatGPT,,265,Deeg9rie9usi,1701261996,story,https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html
41434637,Steve Ballmer's incorrect binary search interview question,,264,jgrahamc,1725369171,story,https://blog.jgc.org/2024/09/steve-ballmers-binary-search-interview.html
39357900,Nvidia's Chat with RTX is an AI chatbot that runs locally on your PC,,264,nickthegreek,1707834478,story,https://www.theverge.com/2024/2/13/24071645/nvidia-ai-chatbot-chat-with-rtx-tech-demo-hands-on
39819458,Show HN: Tracecat – Open-source security alert automation / SOAR alternative,"Hi HN, we are building Tracecat (<a href=""https:&#x2F;&#x2F;tracecat.com&#x2F;"">https:&#x2F;&#x2F;tracecat.com&#x2F;</a>), an open source automation platform for security alerts. Tracecat automates the tasks a security analyst has to do when responding to a security alert: e.g. contact victims, investigate security logs, report vulnerability.<p>The average security analyst deals with 100 alerts per day. As soon as an alert comes in, you have to investigate and respond. An average alert takes ~30 minutes to analyze (and 100 x 30 min = 50 hours &gt; one whole day) Lots of things get dropped, and this creates vulnerabilities. Many breaches can be traced back to week old alerts that didn’t get properly investigated.<p>Since the risks and costs are so high, top security teams currently pay Splunk SOAR $100,000&#x2F;year to help automate alert processing. It’s a click-and-drag workflow builder with webhooks, REST API integrations, and JSON processors. A security engineer would use it to build alert automations that look like this: (1) webhook to receive alert (e.g. unusual powershell cmd) from Microsoft Defender; (2) send yes&#x2F;no Slackbot to ask employee about the alert; (3) if confirmed as suspicious, send malware sample to VirusTotal for report (4) collect evidence from previous steps and dump it into a ticket.<p>If $100k a year seems wildly expensive for a Zapier-like platform, you’d be half right. Splunk SOAR is actually a Zapier + log search + Jira ticketing system.<p>Log storage—that’s how Splunk turns a $99&#x2F;month workflow automation tool into a pricey enterprise product. Every piece of evidence collected (e.g. Slackbot response, malware report, GeoIP enrichment) and every past workflow trail has to be searchable by a human incident responder or auditor. Security teams need to know why every alert escalated to a SEV1 or not.<p>My cofounder and I are data engineers who fell into this space. We heard our security friends constantly complain about being priced out of a SOAR (security orchestration, automation, and response platform) like Splunk SOAR.<p>We both wrote a lot of event-driven code at school (Master’s thesis) and work (Meta &#x2F; PwC). We’re also early adopters of Quickwit &#x2F; Tantivy, an OSS alternative to Elasticsearch &#x2F; Apache Lucene that is cheaper and faster. It didn’t seem that difficult to build a cheaper open source SOAR, so we decided to do it.<p>Tracecat is also different as it can run in a single VM &#x2F; laptop. Splunk SOAR and Tines are built for Fortune 10 needs, which means expensive Kubernetes clusters. Most security teams don’t need that scale, but are forced to pay the K8s “premium” (high complexity, hard to maintain). Tracecat uses OSS embedded databases (SQLite) and an event processing engine we built using Python 3.12 asyncio.<p>So far, we’ve just got a bare-bones alpha but you can already do quite a few things with it. e.g. trigger event-driven workflows from webhooks; use REST API integrations; parse responses using JSONPath; control flow using conditional blocks; store logs cheaply in Tantivy; open cases directly from workflows; prioritize and manage cases in a Jira-like table.<p>Tracecat uses Pydantic V2 for fast input &#x2F; output validation and Zod for fast form validation. We care a lot about data quality! It’s also Apache-2.0 licensed so anyone can self-host the platform.<p>On our roadmap: integrations with popular security tools (Crowdstrike, Microsoft defender); pre-built workflows (e.g. investigating phishing email); better docs; more AI features like auto-labeling tickets, extracting data from unstructured text etc.<p>We’re still early so would love your feedback and opinions. Feel free to try us out or share it with your security friends. We have a cloud version up and running: <a href=""https:&#x2F;&#x2F;platform.tracecat.com"">https:&#x2F;&#x2F;platform.tracecat.com</a>.<p>Dear HN readers, we’d love to hear your incident response stories and the software you use (or not) to automate the work. Stories from security, site reliability engineering, or even physical systems like critical infrastructure monitoring are all very welcome!",264,neochris,1711389907,story,https://github.com/TracecatHQ/tracecat
43015267,Launch HN: A0.dev (YC W25) – React Native App Generator,"Hi HN — we’re Seth and Ayo and we’re building a0.dev (<a href=""https:&#x2F;&#x2F;a0.dev"">https:&#x2F;&#x2F;a0.dev</a>). a0.dev is a platform built to cut down React Native development time from weeks to just a few hours.<p>We’ve been building mobile apps together for seven years and have had several successes. One thing that’s always bothered us though is how much harder it is to build a mobile app than a website. If you’ve built an app with React Native before you’re familiar with the pains of working with both Xcode and Android Studio, writing tons of boilerplate before you can even start making the app, setting up state management, and of course going through the dreaded app review. We decided to build a platform that would make the app development process faster.<p>We’ve seen the success of new code-gen platforms like v0 and wanted something for React Native that goes further. We built an AI app generator that takes a user&#x27;s prompt and creates a custom React Native app with an instant live preview.<p>Here’s a 5min demo where we recreated the Hacker News UI: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;f3lzBRBUous"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;f3lzBRBUous</a><p>a0.dev is great for quickly prototyping components and screens in React Native and users are able to copy the code from our generator into their preferred development environment. Our landing page has a couple of example prompts, but we encourage you to get creative when trying it out. We’ve had success generating not only functional screens like an Instagram Feed but also 2d games like Minesweeper or Flappy Bird.<p>Our chat has a “UI Expert” and “Advanced Logic” model users can switch between depending on the task at hand. Users can upgrade from working on a single screen to creating a full app by clicking on the “Need a Full App” button in the top right of the page. This changes the scope from a standalone chat with a single file to a full project that can include multiple chats and files. We launched an IOS app that users can download in order to preview the app on a physical device. We find that many apps look and feel better on a physical device so we recommend trying it out.<p>Our goal is to continue to improve the app generator while adding more features to help developers get their apps to the app store and make money from those apps. The main features on our roadmap right now are a Supabase integration and a “one click submit” button to let developers publish their app to the App Store.<p>There are a few limitations to note. We’re working on releasing our Android app, but Android users should be able to preview their app using the Expo Go App. The app is running React Native Web in the browser so any dependencies that don’t support web won’t work with the web preview but should work on the phone. There are also some dependencies that our system can’t handle because they require native modules that aren’t packaged into our app currently.<p>We hope you guys will check it out and try making an app with a0.dev. We’re available on Discord around the clock to help developers with any problems they may face and want to guide people to actually releasing their app on the App Store. Let us know what features you’d like to see and any problems you’ve faced building apps, we’d love to hear about your experience.<p>Here’s the link again to check it out: (<a href=""https:&#x2F;&#x2F;a0.dev"">https:&#x2F;&#x2F;a0.dev</a>)<p>We dropped the need to sign up for the first message so you can just jump in and try it out.<p>Looking forward to your thoughts!",263,sethburster,1739293715,story,
43417511,LLM Agents Are Simply Graph – Tutorial for Dummies,,263,zh2408,1742419753,story,https://zacharyhuang.substack.com/p/llm-agent-internal-as-a-graph-tutorial
40536860,Man scammed after AI told him fake Facebook customer support number was real,,262,deviantintegral,1717171644,story,https://www.cbc.ca/news/canada/manitoba/facebook-customer-support-scam-1.7219581
39981623,ScreenAI: A visual LLM for UI and visually-situated language understanding,,262,gfortaine,1712682953,story,https://research.google/blog/screenai-a-visual-language-model-for-ui-and-visually-situated-language-understanding/
40061605,The state of AI for hand-drawn animation inbetweening,,262,luu,1713339340,story,https://yosefk.com/blog/the-state-of-ai-for-hand-drawn-animation-inbetweening.html
43300414,The Einstein AI Model,,261,9woc,1741443262,story,https://thomwolf.io/blog/scientific-ai.html
40599018,Qwen2 LLM Released,,261,bratao,1717689673,story,https://qwenlm.github.io/blog/qwen2/
42308303,LLM abstraction levels inspired by fish eye lens,,261,swyx,1733244915,story,https://wattenberger.com/thoughts/fish-eye
41125980,Torchchat: Chat with LLMs Everywhere,,261,constantinum,1722484094,story,https://github.com/pytorch/torchchat
42517035,Short Message Compression Using LLMs,,261,chunkles,1735239993,story,https://bellard.org/ts_sms/
45648258,J.P. Morgan's OpenAI loan is strange,,260,vrnvu,1760989132,story,https://marketunpack.com/j-p-morgans-openai-loan-is-strange/
44564248,Context Rot: How increasing input tokens impacts LLM performance,"I work on research at Chroma, and I just published our latest technical report on context rot.<p>TLDR: Model performance is non-uniform across context lengths, including state-of-the-art GPT-4.1, Claude 4, Gemini 2.5, and Qwen3 models.<p>This highlights the need for context engineering. Whether relevant information is present in a model’s context is not all that matters; what matters more is how that information is presented.<p>Here is the complete open-source codebase to replicate our results: <a href=""https:&#x2F;&#x2F;github.com&#x2F;chroma-core&#x2F;context-rot"">https:&#x2F;&#x2F;github.com&#x2F;chroma-core&#x2F;context-rot</a>",260,kellyhongsn,1752521115,story,https://research.trychroma.com/context-rot
45848504,Gmail AI gets more intrusive,,260,speckx,1762535221,story,https://daveverse.org/2025/11/07/gmail-ai-gets-even-more-intrusive/
40915005,MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use,,260,tosh,1720525722,story,https://github.com/facebookresearch/MobileLLM
45581735,How AI hears accents: An audible visualization of accent clusters,,260,ilyausorov,1760458057,story,https://accent-explorer.boldvoice.com/
40511307,"Elixir and Machine Learning in 2024 so far: MLIR, Arrow, structured LLM, etc.",,260,clessg,1716986296,story,https://dashbit.co/blog/elixir-ml-s1-2024-mlir-arrow-instructor
40062552,Humane AI – Pico Laser Projection – AI Twist on an Old Scam (2023),,260,abhinavk,1713348710,story,https://kguttag.com/2023/12/06/humane-ai-pico-laser-projection-230m-ai-twist-on-an-old-scam/
43451141,Show HN: I'm a teacher and built an AI presentation tool,"Hi, I&#x27;m a high school teacher from Australia and I&#x27;ve built what I&#x27;d like to think is a pretty nifty ChatGPT powered presentation tool for teachers.<p>I&#x27;d love it if you could have a look at it and give me some of your feedback.<p>I don&#x27;t think there&#x27;s much overlap with the HN crowd and school teachers, but I&#x27;ve been coming here for many years and thought I&#x27;d post here and see what you all think.<p>Check it out if you have a minute and I&#x27;d be super happy to hear your feedback too.<p><a href=""https:&#x2F;&#x2F;www.slidehero.ai&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.slidehero.ai&#x2F;</a><p>You can jump in and have a play with the tool all you like ;)<p>Cheers, Eli",260,slidehero,1742709515,story,
44530922,Recovering from AI addiction,,260,pera,1752233291,story,https://internetaddictsanonymous.org/internet-and-technology-addiction/signs-of-an-addiction-to-ai/
41489167,ESPN AI recap of Alex Morgan’s final professional match fails to mention her,,260,starkparker,1725894152,story,https://awfulannouncing.com/espn/alex-morgan-ai-generated-recap-mention.html
40244165,Ontario family doctor says new AI notetaking saved her job,,260,davidbarker,1714711856,story,https://globalnews.ca/news/10463535/ontario-family-doctor-artificial-intelligence-notes/
43173378,Launch HN: Browser Use (YC W25) – open-source web agents,"Hey HN, we’re Gregor and Magnus, the founders of browser-use (<a href=""https:&#x2F;&#x2F;browser-use.com&#x2F;"">https:&#x2F;&#x2F;browser-use.com&#x2F;</a>), an easy way to connect AI agents with the browser.  Our agent library is open-source (<a href=""https:&#x2F;&#x2F;github.com&#x2F;browser-use&#x2F;browser-use"">https:&#x2F;&#x2F;github.com&#x2F;browser-use&#x2F;browser-use</a>) and we have what is the biggest open-source community for browser agents. And now we have a cloud offering—hence our Launch HN today!<p>Check out this video to see it in action: <a href=""https:&#x2F;&#x2F;preview.screen.studio&#x2F;share&#x2F;r1h4DuAk"" rel=""nofollow"">https:&#x2F;&#x2F;preview.screen.studio&#x2F;share&#x2F;r1h4DuAk</a>. There are lots more demos at <a href=""https:&#x2F;&#x2F;github.com&#x2F;browser-use&#x2F;browser-use"">https:&#x2F;&#x2F;github.com&#x2F;browser-use&#x2F;browser-use</a> on how we control the web with prompts.<p>We started coding a decade ago with Selenium bots and macros to automate tasks. Then we both moved into ML. Last November, we asked ourselves, “How hard could it be to build the interface between LLMs and the web?”<p>We launched on Show HN (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42052432"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=42052432</a>) and have since been addressing various challenges of browser automation, such as: - Automation scripts break when the website changes - Automation scripts are annoying to build - Captchas and rate limits - parsing errors and API key management - and perhaps worst of all, login screens.<p>People use us to fill out their forms, extract data behind login walls, or automate their CRM. Others use the xPaths browser-use clicked on and build their scripts faster, or directly rerun the actions of browser-use deterministically. We’re currently working on robust task reruns, agent memory for long tasks, parallelization for repetitive tasks, and many other sweet improvements.<p>One interesting aspect is that some companies now want to change their UI to be more agent-friendly. Some developers even replace ugly UIs with nice ones and use browser-use to copy data over.<p>Besides the open-source we have an API. We host the browser and LLMs for you and help you with handling proxy rotation, persistent sessions and allowing you to run multiple instances in parallel. We price at $30&#x2F;month—significantly lower than OpenAI’s Operator.<p>On the open-source side, browser use remains free. You can use any LLM, from Gemini to Sonnet, Qwen, or even DeepSeek-R1. It’s licensed under MIT, giving you full freedom to customize it.<p>We’d love to hear from you—what automation challenges are you facing? Any thoughts, questions, experiences are welcome!",259,MagMueller,1740498317,story,https://github.com/browser-use/browser-use
45418261,iRobot Founder: Don't Believe the AI and Robotics Hype,,259,herbertl,1759177148,story,https://crazystupidtech.com/2025/09/29/irobot-founder-dont-believe-the-ai-robotics-hype/
42037982,An embarrassingly simple approach to recover unlearned knowledge for LLMs,,259,PaulHoule,1730688734,story,https://arxiv.org/abs/2410.16454
42174181,Bayesian Neural Networks,,259,reqo,1731948458,story,https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/
44599549,My favorite use-case for AI is writing logs,,259,todsacerdoti,1752795528,story,https://newsletter.vickiboykis.com/archive/my-favorite-use-case-for-ai-is-writing-logs/
44538413,OpenAI delays launch of open-weight model,,259,martinald,1752282453,story,https://twitter.com/sama/status/1943837550369812814
41711032,"Show HN: Sourcebot, an open-source Sourcegraph alternative","Hi HN,<p>We’re Brendan and Michael, the creators of Sourcebot (<a href=""https:&#x2F;&#x2F;github.com&#x2F;sourcebot-dev&#x2F;sourcebot"">https:&#x2F;&#x2F;github.com&#x2F;sourcebot-dev&#x2F;sourcebot</a>). Sourcebot is an open-source code search tool that allows you to quickly search across many large codebases. Check out our demo video here: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;mrIFYSB_1F4"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;mrIFYSB_1F4</a>, or try it for yourself on our demo site here: <a href=""https:&#x2F;&#x2F;demo.sourcebot.dev"" rel=""nofollow"">https:&#x2F;&#x2F;demo.sourcebot.dev</a><p>While at prior roles, we’ve both felt the pain of searching across hundreds of multi-million line codebases. Using local tools like grep were ill-suited since you often only had a handful of codebases checked out at a time. Sourcegraph (<a href=""https:&#x2F;&#x2F;sourcegraph.com&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;sourcegraph.com&#x2F;</a>) solves this issue by indexing a collection of codebases in the background and exposing a web-based search interface. It is the de-facto search solution for medium to large orgs, but is often cited as expensive ($49 per user &#x2F; month) and recently went closed source (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41296481"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41296481</a>). That’s why we built Sourcebot.<p>We designed Sourcebot to be:<p>- Easily deployed: we provide a single, self-contained Docker image (<a href=""https:&#x2F;&#x2F;github.com&#x2F;sourcebot-dev&#x2F;sourcebot&#x2F;pkgs&#x2F;container&#x2F;sourcebot"">https:&#x2F;&#x2F;github.com&#x2F;sourcebot-dev&#x2F;sourcebot&#x2F;pkgs&#x2F;container&#x2F;so...</a>).<p>- Fast &amp; scalable: designed to minimize search times (current average is ~73ms) across many large repositories.<p>- Cross code-host support: we currently support syncing public &amp; private repositories in GitHub and GitLab.<p>- Quality UI: we like to think that a good looking dev-tool is more pleasant to use.<p>- Open source: Sourcebot is free to use by anyone.<p>Under the hood, we use Zoekt (<a href=""https:&#x2F;&#x2F;github.com&#x2F;sourcegraph&#x2F;zoekt"">https:&#x2F;&#x2F;github.com&#x2F;sourcegraph&#x2F;zoekt</a>) as our code search engine, which was originally authored by Han-Wen Nienhuys and now maintained by Sourcegraph (<a href=""https:&#x2F;&#x2F;sourcegraph.com&#x2F;blog&#x2F;sourcegraph-accepting-zoekt-maintainership"" rel=""nofollow"">https:&#x2F;&#x2F;sourcegraph.com&#x2F;blog&#x2F;sourcegraph-accepting-zoekt-mai...</a>). Zoekt works by building a trigram index from the source code enabling extremely fast regular expression matching. Russ Cox has a great article on how trigram indexes work if you’re interested: <a href=""https:&#x2F;&#x2F;swtch.com&#x2F;~rsc&#x2F;regexp&#x2F;regexp4.html"" rel=""nofollow"">https:&#x2F;&#x2F;swtch.com&#x2F;~rsc&#x2F;regexp&#x2F;regexp4.html</a><p>In the shorter-term, there are several improvements we want to make, like:<p>- Improving how we communicate indexing progress (this is currently non-existent so it’s not obvious how long things will take)<p>- UX improvements like search history, query syntax highlighting &amp; suggestions, etc.<p>- Small QOL improvements like bookmarking code snippets.<p>- Support for more code hosts (e.g., BitBucket, SourceForge, ADO, etc.)<p>In the longer-term, we want to investigate how we could go beyond just traditional code search by leveraging machine learning to enable experiences like semantic code search (“where is system X located?”) and code explanations (”how does system X interact with system Y?”). You could think of this as a copilot being embedded into Sourcebot. Our hunch is that will be useful to devs, especially when packaged with the traditional code search, but let us know what you think.<p>Give it a try: <a href=""https:&#x2F;&#x2F;github.com&#x2F;sourcebot-dev&#x2F;sourcebot"">https:&#x2F;&#x2F;github.com&#x2F;sourcebot-dev&#x2F;sourcebot</a>. Cheers!",259,bshzzle,1727801800,story,https://github.com/sourcebot-dev/sourcebot
41815173,"Swarm, a new agent framework by OpenAI",,258,mnk47,1728691550,story,https://github.com/openai/swarm
40974193,The Engineer’s Guide to Deep Learning: Understanding the Transformer Model,,258,shxx,1721113266,story,https://www.interdb.jp/dl/
44358524,Judge denies creating “mass surveillance program” harming all ChatGPT users,,258,merksittich,1750702691,story,https://arstechnica.com/tech-policy/2025/06/judge-rejects-claim-that-forcing-openai-to-keep-chatgpt-logs-is-mass-surveillance/
40528192,Ask HN: I have many PDFs – what is the best local way to leverage AI for search?,"As the title says, I have many PDFs - mostly scans via Scansnap - but also non-scans. These are sensitive in nature, e.g. bills, documents, etc. I would like a local-first AI solution that allows me to say things like: &quot;show me all tax documents for August 2023&quot; or &quot;show my home title&quot;. Ideally it is Mac software that can access iCloud too, since that where I store it all. I would prefer to not do any tagging. I would like to optimize on recall over precision, so False Positives in the search results are ok. What are modern approaches to do this, without hacking one up on my own?",257,phodo,1717100645,story,
43955374,Klarna changes its AI tune and again recruits humans for customer service,,257,elsewhen,1746984922,story,https://www.customerexperiencedive.com/news/klarna-reinvests-human-talent-customer-service-AI-chatbot/747586/
40812622,CriticGPT: Finding GPT-4's mistakes with GPT-4,,257,davidbarker,1719507729,story,https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/
43710576,Show HN: Plandex v2 – open source AI coding agent for large projects and tasks,"Hey HN! I’m Dane, the creator of Plandex (<a href=""https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex"">https:&#x2F;&#x2F;github.com&#x2F;plandex-ai&#x2F;plandex</a>), an open source AI coding agent focused especially on tackling large tasks in real world software projects.<p>You can watch a 2 minute demo of Plandex in action here: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SFSu2vNmlLk"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=SFSu2vNmlLk</a><p>And here’s more of a tutorial style demo showing how Plandex can automatically debug a browser application: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=g-_76U_nK0Y"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=g-_76U_nK0Y</a>.<p>I launched Plandex v1 here on HN a little less than a year ago (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39918500"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39918500</a>).<p>Now I’m launching a major update, Plandex v2, which is the result of 8 months of heads down work, and is in effect a whole new project&#x2F;product.<p>In short, Plandex is now a top-tier coding agent with fully autonomous capabilities. It combines models from Anthropic, OpenAI, and Google to achieve better results, more reliable agent behavior, better cost efficiency, and better performance than is possible by using only a single provider’s models.<p>I believe it is now one of the best tools available for working on large tasks in real world codebases with AI. It has an effective context window of 2M tokens, and can index projects of 20M tokens and beyond using tree-sitter project maps (30+ languages are supported). It can effectively find relevant context in massive million-line projects like SQLite, Redis, and Git.<p>A bit more on some of Plandex’s key features:<p>- Plandex has a built-in diff review sandbox that helps you get the benefits of AI without leaving behind a mess in your project. By default, all changes accumulate in the sandbox until you approve them. The sandbox is version-controlled. You can rewind it to any previous point, and you can also create branches to try out alternative approaches.<p>- It offers a ‘full auto mode’ that can complete large tasks autonomously end-to-end, including high level planning, context loading, detailed planning, implementation, command execution (for dependencies, builds, tests, etc.), and debugging.<p>- The autonomy level is highly configurable. You can move up and down the ladder of autonomy depending on the task, your comfort level, and how you weigh cost optimization vs. effort and results.<p>- Models and model settings are also very configurable. There are built-in models and model packs for different use cases. You can also add custom models and model packs, and customize model settings like temperature or top-p. All model changes are version controlled, so you can use branches to try out the same task with different models. The newly released OpenAI models and the paid Gemini 2.5 Pro model will be integrated in the default model pack soon.<p>- It can be easily self-hosted, including a ‘local mode’ for a very fast local single-user setup with Docker.<p>- Cloud hosting is also available for added convenience with a couple of subscription tiers: an ‘Integrated Models’ mode that requires no other accounts or API keys and allows you to manage billing&#x2F;budgeting&#x2F;spending alerts and track usage centrally, and a ‘BYO API Key’ mode that allows you to use your own OpenAI&#x2F;OpenRouter accounts.<p>I’d love to get more HNers in the Plandex Discord (<a href=""https:&#x2F;&#x2F;discord.gg&#x2F;plandex-ai"" rel=""nofollow"">https:&#x2F;&#x2F;discord.gg&#x2F;plandex-ai</a>). Please join and say hi!<p>And of course I’d love to hear your feedback, whether positive or negative. Thanks so much!",257,danenania,1744838802,story,https://github.com/plandex-ai/plandex
39206731,Testing how hard it is to cheat with ChatGPT in interviews,,257,michael_mroczka,1706722509,story,https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews
42451968,How we made our AI code review bot stop leaving nitpicky comments,,257,dakshgupta,1734539506,story,https://www.greptile.com/blog/make-llms-shut-up
42584896,Show HN: DeepFace – A lightweight deep face recognition library for Python,"DeepFace is a leading open-source library for facial recognition and facial attribute analysis, and the de facto standard in Python. It wraps multiple state-of-the-art models that have reached — and even surpassed — human-level accuracy in recognizing faces.<p>By the numbers (as of early 2025): 15,000+ stars on GitHub; ~4 million installations via pip; 800+ citations in academic papers<p>Whether you&#x27;re building a cutting-edge AI project or simply exploring facial recognition, DeepFace makes advanced capabilities accessible with just a few lines of code.",257,serengil,1735905784,story,https://github.com/serengil/deepface
43203543,Hot take: GPT 4.5 is a nothing burger,,256,isaacfrond,1740734653,story,https://garymarcus.substack.com/p/hot-take-gpt-45-is-a-nothing-burger
44965577,Claim: GPT-5-pro can prove new interesting mathematics,,256,marcuschong,1755718965,story,https://twitter.com/SebastienBubeck/status/1958198661139009862
44268547,"The Army’s Newest Recruits: Tech Execs From Meta, OpenAI and More",,255,aspenmayer,1749822702,story,https://www.wsj.com/tech/army-reserve-tech-executives-meta-palantir-796f5360
44186496,LLMs and Elixir: Windfall or deathblow?,,255,uxcolumbo,1749078035,story,https://www.zachdaniel.dev/p/llms-and-elixir-windfall-or-deathblow
41640812,On Impactful AI Research,,255,KraftyOne,1727211389,story,https://github.com/okhat/blog/blob/main/2024.09.impact.md
39811155,“Emergent” abilities in LLMs actually develop gradually and predictably – study,,255,Anon84,1711319793,story,https://www.quantamagazine.org/how-quickly-do-large-language-models-learn-unexpected-skills-20240213/
44716653,OpenAI's ChatGPT Agent casually clicks through ""I am not a robot"" verification,,255,joak,1753742779,story,https://arstechnica.com/information-technology/2025/07/openais-chatgpt-agent-casually-clicks-through-i-am-not-a-robot-verification-test/
42745847,Using ChatGPT is not bad for the environment,,255,returningfory2,1737174664,story,https://andymasley.substack.com/p/individual-ai-use-is-not-bad-for
41764486,Homemade AI drone software finds people when search and rescue teams can't,,254,sohkamyung,1728295962,story,https://www.wired.com/story/this-homemade-ai-drone-software-finds-bodies-when-search-and-rescue-teams-cant/
41803457,Extracting financial disclosure and police reports with OpenAI Structured Output,,254,danso,1728593474,story,https://gist.github.com/dannguyen/faaa56cebf30ad51108a9fe4f8db36d8
40240037,Microsoft bans U.S. police from using enterprise AI tool for facial recognition,,254,coloneltcb,1714675891,story,https://techcrunch.com/2024/05/02/microsoft-bans-u-s-police-departments-from-using-enterprise-ai-tool/
40990768,SAPwned: SAP AI vulnerabilities expose customers' cloud environments and privat,,254,todsacerdoti,1721253759,story,https://www.wiz.io/blog/sapwned-sap-ai-vulnerabilities-ai-security
45748195,The end of the rip-off economy: consumers use LLMs against information asymmetry,,254,scythe,1761751966,story,https://www.economist.com/finance-and-economics/2025/10/27/the-end-of-the-rip-off-economy
45599727,"Writing an LLM from scratch, part 22 – training our LLM",,254,gpjt,1760571732,story,https://www.gilesthomas.com/2025/10/llm-from-scratch-22-finally-training-our-llm
42063346,"Show HN: Aide, an open-source AI native IDE","Hey HN, We are Sandeep and Naresh, the creators of Aide. We are happy to open source and invite the community to try out Aide which is a VSCode fork built with LLMs integrated.<p>To talk through the features, we engineered the following:<p>- A proactive agent<p>Agent which iterates on the linter errors (powered by the Language Server) and pulls in relevant context by doing go-to-definitions, go-to-references etc and propose fixes or ask for more files which might be missing in the context.<p>- Developer control<p>We encourage you to do edits on top of your coding sessions. To enable this, we built a VSCode native rollback feature which gets rid of all the edits made by the agent in a single click if there were mistakes, without messing up your changes from before.<p>- A combined chat+edit flow which you can use to brainstorm and edit<p>You can brainstorm a problem in chat by @’ting the files and then jump into edits (which can happen across multiple files) or go from a smaller set of edits and discuss the side-effects of it<p>- Inline editing widget<p>We took inspiration from the macos spotlight widget and created a similar one inside the editor, you can highlight part of the code, do Cmd+K and just give your instructions freely<p>- Local running AI brain<p>We ship a binary called sidecar which takes care of talking to the LLM providers, preparing the prompts and using the editor for the LLM. All of this is local first and you get full control over the prompts&#x2F;responses without anything leaking to our end (unless you choose to use your subscription and share the data with us)<p>We spent the last 15 months learning about the internals of VSCode (its a non-trivial codebase) and also powering up our AI game, the framework is also at the top of swebench-lite with 43% score. On top of this, since the whole AI side of the logic runs locally on  your machine you have complete control over the data, from the prompt to the responses and you can use your own API Keys as well (can be any LLM provider) and talk to them directly.<p>There’s still a whole lot to build and we are at 1% of the journey. Right now the editor feels robust and does not break on any of the flows which we aimed to solve for.<p>Let us know if there’s anything else you would like to see us build. We also want to empower extensibility and work together with the community to build the next set of features and set a new milestone of AI native editors.",253,skp1995,1730905260,story,https://aide.dev/
39604961,Launch HN: Greptile (YC W24) - RAG on codebases that actually works,"Hi HN, we&#x27;re the co-founders of Greptile, a tool that can accurately answer questions about complex codebases. Developers use us to spend less time wrestling with codebases and more time actually writing code. Here&#x27;s a demo: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;qI24eKO1YX0"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;qI24eKO1YX0</a>. You can try it on 100 popular repos here: <a href=""https:&#x2F;&#x2F;app.greptile.com&#x2F;repo"">https:&#x2F;&#x2F;app.greptile.com&#x2F;repo</a>, and on your own repo (if you give permission - more on that below) here: <a href=""https:&#x2F;&#x2F;app.greptile.com"">https:&#x2F;&#x2F;app.greptile.com</a>.<p>We are far from the first people to try &quot;RAG on your codebase&quot;. We focus on full codebase comprehension: using LLMs to accurately answer difficult questions with full context of large, complex, and even multi-repo codebases.<p>Simple RAG alone is not sufficient for this task. Codebases aren’t like most PDFs, docs, or other similar data types. They are graphs—complex puzzles where each piece is interlinked. So Greptile does a few things past simple RAG:<p>(1) Instead of directly embedding code, we parse the AST of the codebase, recursively generate docstrings for each node in the tree, and then embed the docstrings.<p>(2) Alongside vector similarity search and keyword search, we do “agentic search” where an agent reviews the relevance of the search results, and scans the source code to follow references that might lead to something important. Then it returns the relevant sources.<p>For example, here are a couple questions that this system is able to answer in our test repo that simple RAG couldn’t (in our experience):<p>“<i>Where are the auth providers configured?</i>” (They are in an array inside of an options.ts file, where looking at the file it’s not obvious it’s an auth related file. However, because that array is imported into the auth&#x2F;route.ts file, Greptile’s agent traces and find it)<p>“<i>How would I add a postgres connector?</i>” (The best way to answer this is to see how the Redis connector is set up and mirror it. Simple RAG sometimes retrieves some of the code for the Redis connector, but Greptile’s agent follows the connections to retrieve all the code that the redis connector touches, and uses that to write instructions.)<p>Developers (including at Stripe and Microsoft) are using Greptile for things like:<p>Debugging—you can paste in an error message and it does a pretty good job of diagnosing the root cause and suggesting fixes.<p>Grokking OSS repos—for example, if you&#x27;re forking a repo, modifying it for your usecase, or just integrating it, Greptile lets you add multiple repos and dependencies in the same chat session so it has full context.<p>Parsing legacy code at work—especially if original engineers have left the company.<p>Since we&#x27;re accessing your private code, we&#x27;re very careful with security. We don&#x27;t store any code on our servers after initial processing, and just pull snippets as needed from the GitHub API.<p>Quick note: when you sign in with GH, it might ask for permission to &quot;act on your behalf&quot;. This is a quirk of GitHub&#x27;s wording—our permissions are read-only and the only thing we do &quot;on your behalf&quot; is read code, so we can index the repo.<p>We came up with this idea while working at AWS—the codebase was super complicated, the docs were sparse and out of date, and our team was remote so it was slow to get answers to questions. We picked &quot;greptile&quot; because of &quot;grep&quot; and also we just wanted a somewhat silly name.<p>Try it out! It&#x27;s a work in progress, so any feedback is appreciated. Here are the links again: for popular open source repos see <a href=""https:&#x2F;&#x2F;app.greptile.com&#x2F;repo"">https:&#x2F;&#x2F;app.greptile.com&#x2F;repo</a>, and to get it working on your own repo, start at <a href=""https:&#x2F;&#x2F;app.greptile.com"">https:&#x2F;&#x2F;app.greptile.com</a>.<p>If you have experience working with a complex codebase at work or for a project, I’d love to hear about it. It really helps us educate our product direction. Looking forward to comments!<p>edit. For those who want to try this on large or private repos, here is a promo code for a free month: HACKERNEWS100",253,dakshgupta,1709653736,story,
39387578,Sam Altman owns OpenAI's venture capital fund,,253,choppaface,1708026355,story,https://www.axios.com/2024/02/15/sam-altman-openai-startup-fund
40099344,Financial market applications of LLMs,,253,andreyk,1713636195,story,https://thegradient.pub/financial-market-applications-of-llms/
45971601,Oracle is underwater on its $300B OpenAI deal,<a href=""https:&#x2F;&#x2F;archive.ph&#x2F;Qdf2n"" rel=""nofollow"">https:&#x2F;&#x2F;archive.ph&#x2F;Qdf2n</a>,253,busymom0,1763497775,story,https://www.ft.com/content/064bbca0-1cb2-45ab-85f4-25fdfc318d89
42534931,Show HN: Anki AI Utils,"Hi hn, I am nearly at the end of medical school so it is time I publish and &quot;advertise&quot; my open source scripts&#x2F;apps for anki! Here&#x27;s the pitch:<p><i>Anki AI Utils</i> is a suite of AI-powered tools designed to automatically improve cards you find challenging. Whether you&#x27;re studying medicine, languages, or any complex subject, these tools can:<p>- <i>Explain</i> difficult concepts with clear, ChatGPT-generated explanations.<p>- <i>Illustrate</i> key ideas using Dall-E or Stable Diffusion-generated images.<p>- <i>Create mnemonics</i> tailored to your memory style, including support for the Major System.<p>- <i>Reformulate</i> poorly worded cards for clarity and better retention.<p><i>Key Features:</i><p>- <i>Adaptive Learning:</i> Uses semantic similarity to match cards with relevant examples.<p>- <i>Personalized Memory Hooks:</i> Builds on your existing mnemonics for stronger recall.<p>- <i>Automation Ready:</i> Run scripts daily to enhance cards you struggled with.<p>- <i>Universal Compatibility:</i> Works across all Anki clients (Windows, Mac, Linux, Android, iOS).<p><i>Example:</i><p>For a flashcard about febrile seizures, Anki AI Utils can:<p>- Generate a <i>Dall-E illustration</i> of a toddler holding a teacup next to a fireplace.<p>- Create <i>mnemonics</i> like &quot;A child stumbles near the fire, dances symmetrically, has one strike, and fewer than three fires.&quot;<p>- Provide an <i>explanation</i> of why febrile seizures occur and their diagnostic criteria.<p><i>Call for Contributors:</i><p>This project is battle-tested but needs help to become a polished Anki addon. If you’re a developer or enthusiast, join us to make these tools more accessible!<p><i>Check out my other projects on GitHub:</i> [Anki AI Utils](<a href=""https:&#x2F;&#x2F;github.com&#x2F;thiswillbeyourgithub"">https:&#x2F;&#x2F;github.com&#x2F;thiswillbeyourgithub</a>)<p>Transform your Anki experience with AI—because learning should be smarter, not harder.",253,Ey7NFZ3P0nzAe,1735421434,story,https://github.com/thiswillbeyourgithub/AnkiAIUtils
40309342,Launch HN: Muddy (YC S19) – Multiplayer browser for getting work done,"Hey HN! This is Jimmy, Ron and Austa from Muddy (<a href=""https:&#x2F;&#x2F;feelmuddy.com&#x2F;"">https:&#x2F;&#x2F;feelmuddy.com&#x2F;</a>). Muddy is a browser for work that automatically keeps project files organized in the same place where you use and share them. Here’s a demo: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tZr49aN3sjQ"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=tZr49aN3sjQ</a>. Download and try it out here: <a href=""https:&#x2F;&#x2F;feelmuddy.com&#x2F;"">https:&#x2F;&#x2F;feelmuddy.com&#x2F;</a>.<p>Building together in the past, we were incredibly frustrated with how much friction there is to get anything done on our computers. I was losing time everyday digging through chat logs looking for that one important link or breaking others out of flow by asking where something is.<p>Web apps promised to help us get more done—and they do, but each in its own silo, so there’s still a ton of redundancy to deal with. Every app has its own way of organizing files, its own notification inbox, its own search system. Conversations live everywhere and there isn’t a single view to see everything about a project. Remember when files simply lived in folders rather than the “cloud”?<p>We started dedicating time to organizing our files in shared docs and limiting new apps we used. This helped – but the second we didn’t stay on top of organization, links became stale and things got messy again.<p>Muddy started as a hack week project we built for ourselves—a single place to use web apps with others, but personalized for each user automatically. Everyone gets their own view for every project, designed around how they work.<p>Muddy users work on projects in spaces, which are like automatic tab groups. Users share apps (any site works—a Github PR, Figma file, Trello board—whatever you want) into the project’s shared timeline and Muddy automatically opens relevant tabs for you. It’s a single click to open up all the apps you need for the project.<p>Under the hood, Muddy works in the background to keep track of the timeline and uses a LLM to continuously organize apps and keep everything on to date. It considers signals like the popularity of a file, naming conventions, and conversations to figure out what’s relevant. So everyone is presented with an updated list of important tabs, without anyone lifting a finger. Our actual browser is based on Chromium.<p>When you need to revisit something from weeks ago, you can rewind the project timeline to that point in a single click. Apps open up in the timeline so you’ll see your files right away. For sites that don’t have built in collaboration features (like documentation), Muddy lets you do annotations directly on the website.<p>Projects sometimes get big and need to be broken up. Across all your spaces, Muddy can answer questions like ChatGPT, cite your files as sources, and return apps directly. This is possible since Muddy’s AI shares your browser and can use your authenticated apps locally (with privacy in mind).<p>Other browsers like Chrome and Arc focus on solo productivity with sharing as a bolt-on. We think productivity depends on how well you can work with others, and should be the first class consideration. And doing organizational work manually is unsustainable.<p>Muddy will have paid subscriptions for teams with additional features like shared passwords, team organization, custom shortcuts, and SSO management. Those aren’t built out yet and the base product will be free. No part of our revenue will come from data monetization.<p>We’d love for you to give Muddy a spin! You can download Muddy for Mac or Windows on our website and add others once inside: <a href=""https:&#x2F;&#x2F;feelmuddy.com&#x2F;"">https:&#x2F;&#x2F;feelmuddy.com&#x2F;</a>. We’ll be around to answer questions and look forward to any and all feedback!",252,lele0108,1715269128,story,
38506660,OpenAI Committed to Buying $51M of AI Chips from a Startup Backed by Sam Altman,,252,gslin,1701607327,story,https://www.wired.com/story/openai-buy-ai-chips-startup-sam-altman/
38704982,LLM in a Flash: Efficient LLM Inference with Limited Memory,,252,ghshephard,1703041363,story,https://huggingface.co/papers/2312.11514
42798649,Using generative AI as part of historical research: three case studies,,252,benbreen,1737588561,story,https://resobscura.substack.com/p/the-leading-ai-models-are-now-very
45279357,DeepMind and OpenAI win gold at ICPC,<a href=""https:&#x2F;&#x2F;x.com&#x2F;MostafaRohani&#x2F;status&#x2F;1968360976379703569"" rel=""nofollow"">https:&#x2F;&#x2F;x.com&#x2F;MostafaRohani&#x2F;status&#x2F;1968360976379703569</a><p><a href=""https:&#x2F;&#x2F;x.com&#x2F;HengTze&#x2F;status&#x2F;1968359525339246825"" rel=""nofollow"">https:&#x2F;&#x2F;x.com&#x2F;HengTze&#x2F;status&#x2F;1968359525339246825</a>,251,notemap,1758132916,story,https://codeforces.com/blog/entry/146536
41027658,"When ChatGPT summarises, it does nothing of the kind",,251,josephcsible,1721591800,story,https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actually-does-nothing-of-the-kind/
40617082,LSP-AI: open-source language server serving as back end for AI code assistance,,251,homarp,1717849498,story,https://github.com/SilasMarvin/lsp-ai
40799791,Show HN: R2R V2 – A open source RAG engine with prod features,"Hi HN! We&#x27;re building R2R [<a href=""https:&#x2F;&#x2F;github.com&#x2F;SciPhi-AI&#x2F;R2R"">https:&#x2F;&#x2F;github.com&#x2F;SciPhi-AI&#x2F;R2R</a>], an open source RAG answer engine that is built on top of Postgres+Neo4j. The best way to get started is with the docs - <a href=""https:&#x2F;&#x2F;r2r-docs.sciphi.ai&#x2F;introduction"">https:&#x2F;&#x2F;r2r-docs.sciphi.ai&#x2F;introduction</a>.<p>This is a major update from our V1 which we have spent the last 3 months intensely building after getting a ton of great feedback from our first Show HN (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39510874"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39510874</a>). We changed our focus to building a RAG engine instead of a framework, because this is what developers asked for the most. To us this distinction meant working on an opinionated system instead of layers of abstractions over providers. We built features for multimodal data ingestion, hybrid search with reranking, advanced RAG techniques (e.g. HyDE), automatic knowledge graph construction alongside the original goal of an observable RAG system built on top of a RESTful API that we shared back in February.<p>What&#x27;s the problem? Developers are struggling to build accurate, reliable RAG solutions. Popular tools like Langchain are complex and overly abstracted and lack crucial production features such as user&#x2F;document management, observability, and a default API. There was a big thread about this a few days ago: <i>Why we no longer use LangChain for building our AI agents</i> (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40739982"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40739982</a>)<p>We experienced these challenges firsthand while building a large-scale semantic search engine, having users report numerous hallucinations and inaccuracies. This highlighted that search+RAG is a difficult problem. We&#x27;re convinced that these missing features, and more, are essential to effectively monitor and improve such systems over time.<p>Teams have been using R2R to develop custom AI agents with their own data, with applications ranging from B2B lead generation to research assistants. Best of all, the developer experience is much improved. For example, we have recently seen multiple teams use R2R to deploy a user-facing RAG engine for their application within a day. By day 2 some of these same teams were using their generated logs to tune the system with advanced features like hybrid search and HyDE.<p>Here are a few examples of how R2R can outperform classic RAG with semantic search only:<p>1. “What were the UK&#x27;s top exports in 2023?&quot;. R2R with hybrid search can identify documents mentioning &quot;UK exports&quot; and &quot;2023&quot;, whereas semantic search finds related concepts like trade balance and economic reports.<p>2. &quot;List all YC founders that worked at Google and now have an AI startup.&quot; Our knowledge graph feature allows R2R to understand relationships between employees and projects, answering a query that would be challenging for simple vector search.<p>The built in observability and customizability of R2R helps you to tune and improve your system long after launching. Our plan is to keep the API ~fixed while we iterate on the internal system logic, making it easier for developers to trust R2R for production from day 1.<p>We are currently working on the following: (1) Improve semantic chunking through third party providers or our own custom LLMs; (2) Training a custom model for knowledge graph triples extraction that will allow KG construction to be 10x more efficient. (This is in private beta, please reach out if interested!); (3) Ability to handle permissions at a more granular level than just a single user; (4) LLM-powered online evaluation of system performance + enhanced analytics and metrics.<p>Getting started is easy. R2R is a lightweight repository that you can install locally with `pip install r2r`, or run with Docker. Check out our quickstart guide: <a href=""https:&#x2F;&#x2F;r2r-docs.sciphi.ai&#x2F;quickstart"">https:&#x2F;&#x2F;r2r-docs.sciphi.ai&#x2F;quickstart</a>. Lastly, if it interests you, we are also working on a cloud solution at <a href=""https:&#x2F;&#x2F;sciphi.ai"">https:&#x2F;&#x2F;sciphi.ai</a>.<p>Thanks a lot for taking the time to read! The feedback from the first ShowHN was invaluable and gave us our direction for the last three months, so we&#x27;d love to hear any more comments you have!",251,ocolegro,1719408434,story,https://github.com/SciPhi-AI/R2R
42136711,The barriers to AI engineering are crumbling fast,,251,lewq,1731596208,story,https://blog.helix.ml/p/we-can-all-be-ai-engineers
45253458,Addendum to GPT-5 system card: GPT-5-Codex,,250,wertyk,1757961932,story,https://openai.com/index/gpt-5-system-card-addendum-gpt-5-codex/
44135369,The Darwin Gödel Machine: AI that improves itself by rewriting its own code,,250,birriel,1748606920,story,https://sakana.ai/dgm/
43798757,World Emulation via Neural Network,,250,treesciencebot,1745616837,story,https://madebyoll.in/posts/world_emulation_via_dnn/
45684236,OpenAI acquires Sky.app,,250,meetpateltech,1761239057,story,https://openai.com/index/openai-acquires-software-applications-incorporated
45753422,Responses from LLMs are not facts,,250,xd1936,1761774005,story,https://stopcitingai.com/
44307629,I counted all of the yurts in Mongolia using machine learning,,250,furkansahin,1750233498,story,https://monroeclinton.com/counting-all-yurts-in-mongolia/
45783470,OpenAI Moves to Complete Potentially the Largest Theft in Human History,,249,paulpauper,1762017912,story,https://thezvi.substack.com/p/openai-moves-to-complete-potentially
44308711,Is there a half-life for the success rates of AI agents?,,249,EvgeniyZh,1750244039,story,https://www.tobyord.com/writing/half-life
39934696,Fake AI law firms are sending fake DMCA threats to generate fake SEO gains,,249,rntn,1712258580,story,https://arstechnica.com/gadgets/2024/04/fake-ai-law-firms-are-sending-fake-dmca-threats-to-generate-fake-seo-gains/
40750391,OpenAI Acquires Rockset,,249,colesantiago,1718982292,story,https://openai.com/index/openai-acquires-rockset/
44434938,HN Slop: AI startup ideas generated from Hacker News,,249,coloneltcb,1751383905,story,https://www.josh.ing/hn-slop
42879323,"Goose: An open-source, extensible AI agent that goes beyond code suggestions",,249,sansui12,1738254435,story,https://block.github.io/goose/
42228472,Full LLM training and evaluation toolkit,,249,testerui,1732463070,story,https://github.com/huggingface/smollm
43690955,Teuken-7B-Base and Teuken-7B-Instruct: Towards European LLMs (2024),,248,doener,1744712237,story,https://arxiv.org/abs/2410.03730
42938409,How I use LLMs as a staff engineer,,248,gfysfm,1738702198,story,https://www.seangoedecke.com/how-i-use-llms/
45416080,Instant Checkout and the Agentic Commerce Protocol,"Agentic Commerce Protocol: <a href=""https:&#x2F;&#x2F;www.agenticcommerce.dev&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;www.agenticcommerce.dev&#x2F;</a>, ChatGPT Page: <a href=""https:&#x2F;&#x2F;chatgpt.com&#x2F;merchants"" rel=""nofollow"">https:&#x2F;&#x2F;chatgpt.com&#x2F;merchants</a>, Stripe post: <a href=""https:&#x2F;&#x2F;stripe.com&#x2F;blog&#x2F;developing-an-open-standard-for-agentic-commerce"" rel=""nofollow"">https:&#x2F;&#x2F;stripe.com&#x2F;blog&#x2F;developing-an-open-standard-for-agen...</a>",248,meetpateltech,1759165242,story,https://openai.com/index/buy-it-in-chatgpt/
44384610,LLM code generation may lead to an erosion of trust,,248,CoffeeOnWrite,1750918069,story,https://jaysthoughts.com/aithoughts1
41730983,Serving 70B-scale LLMs efficiently on low-resource edge devices [pdf],,248,simonpure,1727964680,story,https://arxiv.org/abs/2410.00531
43958382,Avoiding AI is hard – but our freedom to opt out must be protected,,248,gnabgib,1747008588,story,https://theconversation.com/avoiding-ai-is-hard-but-our-freedom-to-opt-out-must-be-protected-255873
40819479,Meta LLM Compiler: neural optimizer and disassembler,,248,foobazgt,1719573135,story,https://twitter.com/AIatMeta/status/1806361623831171318/photo/1
44862542,GPT-OSS-120B runs on just 8GB VRAM & 64GB+ system RAM,,248,zigzag312,1754906528,story,https://old.reddit.com/r/LocalLLaMA/comments/1mke7ef/120b_runs_awesome_on_just_8gb_vram/
45105081,OpenAI says it's scanning users' conversations and reporting content to police,,247,miletus,1756829754,story,https://futurism.com/openai-scanning-conversations-police
44851214,GPTs and Feeling Left Behind,,247,Bogdanp,1754780878,story,https://whynothugo.nl/journal/2025/08/06/gpts-and-feeling-left-behind/
44819968,Running GPT-OSS-120B at 500 tokens per second on Nvidia GPUs,,247,philipkiely,1754533727,story,https://www.baseten.co/blog/sota-performance-for-gpt-oss-120b-on-nvidia-gpus/
41048194,Show HN: Zerox – Document OCR with GPT-mini,"This started out as a weekend hack with gpt-4-mini, using the very basic strategy of &quot;just ask the ai to ocr the document&quot;.<p>But this turned out to be better performing than our current implementation of Unstructured&#x2F;Textract. At pretty much the same cost.<p>I&#x27;ve tested almost every variant of document OCR over the past year, especially trying things like table &#x2F; chart extraction. I&#x27;ve found the rules based extraction has always been lacking. Documents are meant to be a visual representation after all. With weird layouts, tables, charts, etc. Using a vision model just make sense!<p>In general, I&#x27;d categorize this solution as slow, expensive, and non deterministic. But 6 months ago it was impossible. And 6 months from now it&#x27;ll be fast, cheap, and probably more reliable!",246,themanmaran,1721753393,story,https://github.com/getomni-ai/zerox
38607540,FTC wants Microsoft's relationship with OpenAI under the microscope,,246,magoghm,1702341173,story,https://www.theregister.com/2023/12/11/microsoft_openai_investment_ftc/
45505626,Robin Williams' daughter pleads for people to stop sending AI videos of her dad,,246,dijksterhuis,1759856200,story,https://www.bbc.co.uk/news/articles/c0r0erqk18jo
42450950,Cultural Evolution of Cooperation Among LLM Agents,,246,Anon84,1734534052,story,https://arxiv.org/abs/2412.10270
42940284,Google removes pledge to not use AI for weapons from website,,245,filoeleven,1738709585,story,https://techcrunch.com/2025/02/04/google-removes-pledge-to-not-use-ai-for-weapons-from-website/
43905299,Accents in latent spaces: How AI hears accent strength in English,,245,ilyausorov,1746540477,story,https://accent-strength.boldvoice.com/
38918279,Duolingo Cuts 10% of Contractors as It Uses More AI to Create App Content,,245,leotravis10,1704748713,story,https://www.bloomberg.com/news/articles/2024-01-08/duolingo-cuts-10-of-contractors-in-move-to-greater-use-of-ai
39427775,OpenAI Deal Lets Employees Sell Shares at $86B Valuation,,245,upmind,1708335740,story,https://www.bloomberg.com/news/articles/2024-02-17/openai-deal-lets-employees-sell-shares-at-86-billion-valuation
40810986,"Launch HN: Hatchet (YC W24) – Open-source task queue, now with a cloud version","Hey HN - this is Alexander and Gabe from Hatchet (<a href=""https:&#x2F;&#x2F;hatchet.run"">https:&#x2F;&#x2F;hatchet.run</a>). We’re building a modern task queue as an alternative to tools like Celery for Python and BullMQ for Node. Our open-source repo is at <a href=""https:&#x2F;&#x2F;github.com&#x2F;hatchet-dev&#x2F;hatchet"">https:&#x2F;&#x2F;github.com&#x2F;hatchet-dev&#x2F;hatchet</a> and is 100% MIT licensed.<p>When we did a Show HN a few months ago (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39643136"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39643136</a>), our cloud version was invite-only and we were focused on our open-source offering.<p>Today we’re launching our self-serve cloud so that anyone can get started creating tasks on our platform - you can get started at <a href=""https:&#x2F;&#x2F;cloud.onhatchet.run"" rel=""nofollow"">https:&#x2F;&#x2F;cloud.onhatchet.run</a>, or you can use these credentials to access a demo (should be prefilled):<p><pre><code>  URL: https:&#x2F;&#x2F;demo.hatchet-tools.com    Email: hacker@news.ycombinator.com   Password: HatchetDemo123! </code></pre> People are currently using Hatchet for a bunch of use-cases: orchestrating RAG pipelines, queueing up user notifications, building agentic LLM workflows, or scheduling image generation tasks on GPUs.<p>We built this out of frustration with existing tools and a conviction that PostgreSQL is the right choice for a task queue. Beyond the fact that many developers are already using Postgres in their stack, which makes it easier to self-host Hatchet, it’s also easier to model higher-order concepts in Postgres, like chains of tasks (which we call workflows). In our system, the acknowledgement of the task, the task result, and the updates to higher-order models are done as part of the same Postgres transaction, which significantly reduces the risk of data loss&#x2F;race conditions when compared with other task queues (which usually pass acknowledgements through a broker, storing the task results elsewhere, and only then figuring out the next task in the chain).<p>We also became increasingly frustrated with tools like Celery and the challenges it introduces when using a modern Python stack (&gt; 3.5). We wrote up a list of these frustrations here: <a href=""https:&#x2F;&#x2F;docs.hatchet.run&#x2F;blog&#x2F;problems-with-celery"">https:&#x2F;&#x2F;docs.hatchet.run&#x2F;blog&#x2F;problems-with-celery</a>.<p>Since our Show HN, we’ve (partially or completely) addressed the most common pieces of feedback from the post, which we’ll outline here:<p>1. The most common ask was built-in support for fanout workflows — one task which triggers an arbitrary number of child tasks to run in parallel. We previously only had support for DAG executions. We generalized this concept and launched child workflows (<a href=""https:&#x2F;&#x2F;docs.hatchet.run&#x2F;home&#x2F;features&#x2F;child-workflows"">https:&#x2F;&#x2F;docs.hatchet.run&#x2F;home&#x2F;features&#x2F;child-workflows</a>). This is the first step towards a developer-friendly model of durable execution.<p>2. Support for HTTP-based triggers — we’ve built out support for webhook workers (<a href=""https:&#x2F;&#x2F;docs.hatchet.run&#x2F;home&#x2F;features&#x2F;webhooks"">https:&#x2F;&#x2F;docs.hatchet.run&#x2F;home&#x2F;features&#x2F;webhooks</a>), which allow you to trigger any workflow over an HTTP webhook. This is particularly useful for apps on Vercel, who are dealing with timeout limits of 60s, 300s, or 900s (depending on your tier).<p>3. Our RabbitMQ dependency — while we haven’t gotten rid of this completely, we’ve recently launched hatchet-lite (<a href=""https:&#x2F;&#x2F;docs.hatchet.run&#x2F;self-hosting&#x2F;hatchet-lite"">https:&#x2F;&#x2F;docs.hatchet.run&#x2F;self-hosting&#x2F;hatchet-lite</a>), which allows you to run the various Hatchet components in a single Docker image that bundles RabbitMQ along with a migration process, admin CLI, our REST API, and our gRPC engine. Hopefully the lite was a giveaway, but this is meant for local development and low-volume processing, on the order of hundreds per minute.<p>We’ve also launched more features, like support for global rate limiting, steps which only run on workflow failure, and custom event streaming.<p>We’ll be here the whole day for questions and feedback, and look forward to hearing your thoughts!",245,abelanger,1719498934,story,
40922739,RouteLLM: A framework for serving and evaluating LLM routers,,244,djhu9,1720571738,story,https://github.com/lm-sys/RouteLLM
43865097,"xAI dev leaks API key for private SpaceX, Tesla LLMs",,244,todsacerdoti,1746147362,story,https://krebsonsecurity.com/2025/05/xai-dev-leaks-api-key-for-private-spacex-tesla-llms/
44657556,Lumo: Privacy-first AI assistant,,244,pentagrama,1753266162,story,https://proton.me/blog/lumo-ai
45570973,America's future could hinge on whether AI slightly disappoints,,242,jxmorris12,1760376291,story,https://www.noahpinion.blog/p/americas-future-could-hinge-on-whether
42524848,Show HN: I made a web app to bring children's drawings to life,"Hey HN!<p>I used to spend hours drawing all kind of things as a kid. Sadly though, those drawings are long gone.<p>Inspired by this, I created DoodleDreams. A webapp that brings drawings to life using AI and stores them as memories. You can always look back at the drawings, see who made them, and even know the age they were drawn at.<p>I thought it was a fun way to preserve those memories. What do you think?<p>Viktor",242,vikmex,1735325415,story,https://doodledreams.cc
41323042,"LM Studio 0.3 – Discover, download, and run local LLMs",,241,fdb,1724350934,story,https://lmstudio.ai/blog/lmstudio-v0.3.0
43743993,Why is OpenAI buying Windsurf?,,241,theahura,1745159300,story,https://theahura.substack.com/p/tech-things-openai-buys-windsurf
38486394,The Inside Story of Microsoft's Partnership with OpenAI,,240,jyunwai,1701437200,story,https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai
44134896,Ask HN: What is the best LLM for consumer grade hardware?,"I have a 5060ti with 16GB VRAM. I’m looking for a model that can hold basic conversations, no physics or advanced math required. Ideally something that can run reasonably fast, near real time.",240,VladVladikoff,1748602939,story,
41851304,Meta's open AI hardware vision,,240,GavCo,1729015695,story,https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/
44850260,Curious about the training data of OpenAI's new GPT-OSS models? I was too,,239,flabber,1754773816,story,https://twitter.com/jxmnop/status/1953899426075816164
42091043,Claude AI to process secret government data through new Palantir deal,,239,lawls,1731105740,story,https://arstechnica.com/ai/2024/11/safe-ai-champ-anthropic-teams-up-with-defense-giant-palantir-in-new-deal/
41323454,What's Going on in Machine Learning? Some Minimal Models,,239,taywrobel,1724353547,story,https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/
39658610,Show HN: LlamaGym – fine-tune LLM agents with online reinforcement learning,,239,KhoomeiK,1710074443,story,https://github.com/KhoomeiK/LlamaGym
44450160,What to build instead of AI agents,,239,giuliomagnifico,1751500963,story,https://decodingml.substack.com/p/stop-building-ai-agents
45235676,"Will AI be the basis of many future industrial fortunes, or a net loser?",,239,saucymew,1757800895,story,https://joincolossus.com/article/ai-will-not-make-you-rich/
44473319,'Positive review only': Researchers hide AI prompts in papers,,239,ohjeez,1751728547,story,https://asia.nikkei.com/Business/Technology/Artificial-intelligence/Positive-review-only-Researchers-hide-AI-prompts-in-papers
43411725,Trapping misbehaving bots in an AI Labyrinth,,238,pabs3,1742391067,story,https://blog.cloudflare.com/ai-labyrinth/
39959380,The lifecycle of a code AI completion,,237,tosh,1712480103,story,https://sourcegraph.com/blog/the-lifecycle-of-a-code-ai-completion
38859749,TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones,,237,T-A,1704315230,story,https://github.com/DLYuanGod/TinyGPT-V
43975423,Show HN: HelixDB – Open-source vector-graph database for AI applications (Rust),"Hey HN, we want to share HelixDB (<a href=""https:&#x2F;&#x2F;github.com&#x2F;HelixDB&#x2F;helix-db&#x2F;"">https:&#x2F;&#x2F;github.com&#x2F;HelixDB&#x2F;helix-db&#x2F;</a>), a project a college friend and I are working on. It’s a new database that natively intertwines graph and vector types, without sacrificing performance. It’s written in Rust and our initial focus is on supporting RAG. Here’s a video runthrough: <a href=""https:&#x2F;&#x2F;screen.studio&#x2F;share&#x2F;szgQu3yq"" rel=""nofollow"">https:&#x2F;&#x2F;screen.studio&#x2F;share&#x2F;szgQu3yq</a>.<p>Why a hybrid? Vector databases are useful for similarity queries, while graph databases are useful for relationship queries. Each stores data in a way that’s best for its main type of query (e.g. key-value stores vs. node-and-edge tables). However, many AI-driven applications need <i>both</i> similarity and relationship queries. For example, you might use vector-based semantic search to retrieve relevant legal documents, and then use graph traversal to identify relationships between cases.<p>Developers of such apps have the quandary of needing to build on top of two different databases—a vector one and a graph one—plus you have to link them together and sync the data. Even then, your two databases aren&#x27;t designed to work together—for example, there’s no native way to perform joins or queries that span both systems. You’ll need to handle that logic at the application level.<p>Helix started when we realized that there are ways to integrate vector and graph data that are both fast and suitable for AI applications, especially RAG-based ones. See this cool research paper: <a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2408.04948v1"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2408.04948v1</a>. After reading that and some other papers on graph and hybrid RAG, we decided to build a hybrid DB. Our aim was to make something better to use from a developer standpoint, while also making it fast as hell.<p>After a few months of working on this as a side project, our benchmarking shows that we are on par with Pinecone and Qdrant for vectors, and our graph is up to three orders of magnitude faster than Neo4j.<p>Problems where a hybrid approach works particularly well include:<p>- Indexing codebases: you can vectorize code-snippets within a function (connected by edges) based on context and then create an AST (in a graph) from function calls, imports, dependencies, etc. Agents can look up code by similarity or keyword and then traverse the AST to get only the relevant code, which reduces hallucinations and prevents the LLM from guessing object shapes or variable&#x2F;function names.<p>- Molecule discovery: Model biological interactions (e.g., proteins → genes → diseases) using graph types and then embed molecule structures to find similar compounds or case studies.<p>- Enterprise knowledge management: you can represent organisational structure, projects, and people (e.g., employee → team → project) in graph form, then index internal documents, emails, or notes as vectors for semantic search and link them directly employees&#x2F;teams&#x2F;projects in the graph.<p>I naively assumed when learning about databases for the first time that queries would be compiled and executed like functions in traditional programming. Turns out I was wrong, but this creates unnecessary latency by sending extra data (the whole written query), compiling it at run time, and then executing it. With Helix, you write the queries in our query language (HelixQL), which is then transpiled into Rust code and built directly into the database server, where you can call a generated API endpoint.<p>Many people have a thing against “yet another query language” (doubtless for good reason!) but we went ahead and did it anyway, because we think it makes working with our database so much easier that it’s worth a bit of a learning curve. HelixQL takes from other query languages such as Gremlin, Cypher and SQL with some extra ideas added in. It is declarative while the traversals themselves are functional. This allows complete control over the traversal flow while also having a cleaner syntax. HelixQL returns JSON to make things easy for clients. Also, it uses a schema, so the queries are type-checked.<p>We took a crude approach to building the original graph engine as a way to get an MVP out, so we are now working on improving the graph engine by making traversals massively parallel and pipelined. This means data is only ever decoded from disk when it is needed, and parts of reads are all processed in parallel.<p>If you’d like to try it out in a simple RAG demo, you can follow this guide and run our Jupyter notebook: <a href=""https:&#x2F;&#x2F;github.com&#x2F;HelixDB&#x2F;helix-db&#x2F;tree&#x2F;main&#x2F;examples&#x2F;rag_demo"">https:&#x2F;&#x2F;github.com&#x2F;HelixDB&#x2F;helix-db&#x2F;tree&#x2F;main&#x2F;examples&#x2F;rag_d...</a><p>Many thanks! Comments and feedback welcome!",237,GeorgeCurtis,1747157198,story,https://github.com/HelixDB/helix-db/
45137658,"OpenAI eats jobs, then offers to help you find a new one at Walmart",,237,rntn,1757074642,story,https://www.theregister.com/2025/09/05/openai_jobs_board/
45833496,The trust collapse: Infinite AI content is awful,,237,arnon,1762423924,story,https://arnon.dk/the-trust-collapse-infinite-ai-content-is-awful/
44716414,Show HN: Companies use AI to take your calls. I built AI to make them for you,"We&#x27;re living in this weird asymmetry where companies use AI to talk to us, but we&#x27;re still manually dialing them. Companies everywhere are adopting AI voice agents lately. Big retail, family dentist clinics, local pharmacy. This year, I&#x27;ve been in a few calls where it&#x27;s super natural sounding AI, which has been pretty cool to experience. But then it got me thinking - why are we, the consumers, still the ones making calls if they&#x27;re using robots for theirs?<p>So I built Piper: basically AI that makes phone calls for you. You tell it what you need (book appointment, check on an order, dispute some charge, whatever), and it handles the entire conversation while you do actual work. Right now it&#x27;s a web app, Chrome extension is pending approval but soon you&#x27;ll be able to click any phone number anywhere and just let Piper handle it.<p>Technical stuff that was harder than expected:<p>Latency - every millisecond counts in conversation, had to optimize around kv cache, got it down to ~1000ms to first word over PSTN for telephony, which feels pretty natural<p>Keeping the voice agents on track - built custom context engineering logic that constantly updates the agent&#x27;s situational awareness, so it knows when it&#x27;s been transferred, when it&#x27;s on hold, etc<p>Done ~50 successful calls with early testers so far. Main failures are when they need complex verification or documents. Also had to take down our IVR navigation temporarily :&#x2F;, found some edge cases that were causing unnecessary transfers but working on fixing that.<p>I really think we&#x27;re heading toward this world where AI talks to AI for most routine things, and phone calls might be the first real example of this happening at scale!<p>you can check out the a voice demo on our website. <a href=""https:&#x2F;&#x2F;pipervoice.com"" rel=""nofollow"">https:&#x2F;&#x2F;pipervoice.com</a>",237,michaelphi,1753741034,story,https://www.pipervoice.com/
41722742,OpenAI completes deal that values company at $157B,,236,gmaster1440,1727888688,story,https://www.nytimes.com/2024/10/02/technology/openai-valuation-150-billion.html
38605453,Deep Learning – Foundations and Concepts (Chris Bishop),,236,armcat,1702328465,story,https://www.bishopbook.com/
40722317,Show HN: I made an open source and local translation app,"A few years ago, right after high school, I decided to try to make a simultaneous translation app for Android as a side project, it took longer than expected (about 2 years) and I had to make a lot of compromises (I had to use Google&#x27;s API and therefore make users use a developer key because at the time there were no free solutions for speech recognition and translation that had good quality). At the end of university, I decided to pick it up again and finally, using OpenAi&#x27;s Whisper for speech recognition and Meta&#x27;s NLLB for translation (with both running locally on the phone), I managed to make it free and totally open-source (as it was meant to be from the beginning). The app is still in beta, so I would love your feedback.",236,niedev,1718746011,story,https://github.com/niedev/RTranslator
45493287,The AI bubble is 17 times the size of the dot-com frenzy and four times subprime,,235,speckx,1759769172,story,https://www.morningstar.com/news/marketwatch/20251003175/the-ai-bubble-is-17-times-the-size-of-the-dot-com-frenzy-and-four-times-subprime-this-analyst-argues
39210126,I don't want anything your AI generates,,235,cdme,1706739068,story,https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/
38725167,Apple wants AI to run directly on its hardware instead of in the cloud,,235,thunderbong,1703185598,story,https://arstechnica.com/apple/2023/12/apple-wants-ai-to-run-directly-on-its-hardware-instead-of-in-the-cloud/
38587052,Show HN: Open source alternative to ChatGPT and ChatPDF-like AI tools,"Hey everyone,<p>We have been building SecureAI Tools -- an open-source application layer for ChatGPT and ChatPDF-like AI tools.<p>It works with locally running LLMs as well as with OpenAI-compatible APIs. For local LLMs, it supports Ollama which supports all the gguf&#x2F;ggml models.<p>Currently, it has two features: Chat-with-LLM, and Chat-with-PDFs. It is optimized for self-hosting use cases and comes with basic user management features.<p>Here are some quick demos:<p><pre><code>  * Chat with documents using OpenAI&#x27;s GPT3.5 model: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Br2D3G9O47s   * Chat with documents using a locally running Mistral model (M2 MacBook): https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=UvRHL6f_w74 </code></pre> Hope you all like it :)",234,d7y,1702162927,story,https://github.com/SecureAI-Tools/SecureAI-Tools
42789670,OpenAI's o1 Playing Codenames,,234,suveen_ellawela,1737526872,story,https://suveenellawela.com/thoughts/codenames-ai
45750425,OpenAI’s promise to stay in California helped clear the path for its IPO,<a href=""https:&#x2F;&#x2F;archive.ph&#x2F;NCZhi"" rel=""nofollow"">https:&#x2F;&#x2F;archive.ph&#x2F;NCZhi</a>,234,badprobe,1761759874,story,https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c
41236273,Launch HN: Trellis (YC W24) – AI-powered workflows for unstructured data,"Hey HN —  We&#x27;re Jacky and Mac from Trellis (<a href=""https:&#x2F;&#x2F;runtrellis.com&#x2F;"">https:&#x2F;&#x2F;runtrellis.com&#x2F;</a>). We’re building AI-powered ETL for unstructured data. Trellis transforms phone calls, PDFs, and chats into structured SQL format based on any schema you define in natural language. This helps data and ops teams automate manual data entry and run SQL queries on messy data.<p>There’s a demo video at <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ib3mRh2tnSo"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ib3mRh2tnSo</a> and a sandbox to try out (no sign-in required!) at <a href=""https:&#x2F;&#x2F;demo.runtrellis.com&#x2F;"">https:&#x2F;&#x2F;demo.runtrellis.com&#x2F;</a>.  An interesting historical archive of unstructured data we thought it would be interesting to run Trellis on top of are old Enron emails which famously took months to review. We’ve created a showcase demo here: <a href=""https:&#x2F;&#x2F;demo.runtrellis.com&#x2F;showcase&#x2F;enron-email-analysis"">https:&#x2F;&#x2F;demo.runtrellis.com&#x2F;showcase&#x2F;enron-email-analysis</a>, with some documentation here: <a href=""https:&#x2F;&#x2F;docs.runtrellis.com&#x2F;docs&#x2F;example-email-analytics"">https:&#x2F;&#x2F;docs.runtrellis.com&#x2F;docs&#x2F;example-email-analytics</a>.<p>Why we built this: At the Stanford AI lab where we met, we collaborated with many F500 data teams (including Amazon, Meta, and Standard Chartered), and repeatedly saw the same problem: 80% of enterprise data is unstructured, and traditional platforms can’t handle it. For example, a major commercial bank I work with couldn’t improve credit risk models because critical data was stuck in PDFs and emails.<p>We realized that our research from the AI lab could be turned into a solution with an abstraction layer that works as well for financial underwriting as it does for analysis of call center transcripts: an AI-powered ETL that takes in any unstructured data source and turns it into a schematically correct table.<p>Some interesting technical challenges we had to tackle along the way: (1) Supporting complex documents out of the box: We use LLM-based map-reduce to handle long documents and vision models for table and layout extraction. (2) Model Routing: We select the best model for each transformation to optimize cost and speed. For instance, in data extraction tasks, we could leverage simpler fine-tuned models that are specialized in returning structured JSONs of financial tables. (3) Data Validation and Schema Guarantees: We ensure accuracy with reference links and anomaly detection.<p>After launching Trellis, we’ve seen diverse use cases, especially in legacy industries where PDFs are treated as APIs. For example, financial services companies need to process complex documents like bonds and credit ratings into a structured format, and need to speed up underwriting and enable pass-through loan processing. Customer support and back-office operations need to accelerate onboarding by mapping documents across different schema and ERP systems, and ensure support agents follow SOPs (security questions, compliance disclosures, etc.). And many companies today want data preprocessing in ETL pipelines and data ingestion for RAG.<p>We’d love your feedback! Try it out at <a href=""https:&#x2F;&#x2F;demo.runtrellis.com&#x2F;"">https:&#x2F;&#x2F;demo.runtrellis.com&#x2F;</a>. To save and track your large data transformations, you can visit our dashboard and create an account  at <a href=""https:&#x2F;&#x2F;dashboard.runtrellis.com&#x2F;"">https:&#x2F;&#x2F;dashboard.runtrellis.com&#x2F;</a>. If you’re interested in integrating with our APIs, our quick start docs are here: <a href=""https:&#x2F;&#x2F;docs.runtrellis.com&#x2F;docs&#x2F;getting-started"">https:&#x2F;&#x2F;docs.runtrellis.com&#x2F;docs&#x2F;getting-started</a>. If you have any specific use cases in mind, we’d be happy to do a custom integration and onboarding—anything for HN. :)<p>Excited to hear about your experience wrangling with unstructured data in the past, workflows you want to automate, and what data integration you would like to see.",234,macklinkachorn,1723562087,story,
45345207,Sampling and structured outputs in LLMs,,234,SamLeBarbare,1758624041,story,https://parthsareen.com/blog.html#sampling.md
39769708,OpenAI's chatbot store is filling up with spam,,234,mfiguiere,1710956098,story,https://techcrunch.com/2024/03/20/openais-chatbot-store-is-filling-up-with-spam/
40238509,Show HN: Hacker Search – A semantic search engine for Hacker News,"Hi HN!<p>I&#x27;m Jonathan and I built Hacker Search (<a href=""https:&#x2F;&#x2F;hackersearch.net"" rel=""nofollow"">https:&#x2F;&#x2F;hackersearch.net</a>), a semantic search engine for Hacker News. Type a keyword or a description of what you&#x27;re interested in, and you&#x27;ll get top links from HN surfaced to you along with brief summaries.<p>Unlike HN&#x27;s otherwise very valuable search feature, Hacker Search doesn&#x27;t require you to get your keywords exactly right. That&#x27;s achieved by leveraging OpenAI&#x27;s latest embedding models alongside more traditional indexes extracted from the scraped and cleaned up contents of the links.<p>I think there are many more interesting things one could build atop the HN dataset in the age of LLMs (e.g. more explicitly searching for technical opinions, recommending stories to you based on your interests, and making the core search feature more useful). If any of those sound interesting to you, head over to <a href=""https:&#x2F;&#x2F;hackersearch.net&#x2F;signup"" rel=""nofollow"">https:&#x2F;&#x2F;hackersearch.net&#x2F;signup</a> to get notified when I launch them!<p>Note: at least one person has built something similar before (<a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36391655"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36391655</a>). Funnily enough, I only found out about this through my own implementation, and I based on my testing, I think Hacker Search generally performs better when doing keyword&#x2F;sentence searches (vs. whole document similarity lookup), thanks to the way the data is indexed.",233,jnnnthnn,1714669260,story,https://hackersearch.net
44149019,Google AI Edge – On-device cross-platform AI deployment,,233,nreece,1748759540,story,https://ai.google.dev/edge
45003420,Show HN: Clearcam – Add AI object detection to your IP CCTV cameras,"This runs YOLOv8 + bytetrack with Tinygrad detections (depending on user config) are saved and can be sent to the companion iOS app along with a notification, all video processing is done locally, all footage is encrypted before leaving your computer, and the sending notifications + videos part is optional. This uses tinygrad, so it runs well on my apple silicon macs and should be able to run on a lot of hardware (or will be able to when I remove other deps).",233,roryclear,1756035255,story,https://github.com/roryclear/clearcam
41153039,Self-Compressing Neural Networks,,233,bilsbie,1722773836,story,https://arxiv.org/abs/2301.13142
42933256,DoppelBot: Replace Your CEO with an LLM,,232,gk1,1738681701,story,https://modal.com/docs/examples/slack-finetune
38704830,Rite Aid banned from using AI facial recognition for five years,,232,commoner,1703039922,story,https://www.ftc.gov/news-events/news/press-releases/2023/12/rite-aid-banned-using-ai-facial-recognition-after-ftc-says-retailer-deployed-technology-without
44512350,The upcoming GPT-3 moment for RL,,232,jxmorris12,1752080370,story,https://www.mechanize.work/blog/the-upcoming-gpt-3-moment-for-rl/
40481578,Show HN: Boldly go where Gradient Descent has never gone before with DiscoGrad,"Trying to do gradient descent using automatic differentiation over branchy programs? Or to combine them with neural networks for end-to-end training? Then this might be interesting to you.<p>We develped DiscoGrad, a tool for automatic differentiation through C++ programs involving input-dependent control flow (e.g., &quot;if (f(x) &lt; c) { ... }&quot;, differentiating wrt. x) and randomness. Our initial motivation was to enable the use of gradient descent with simulations, which often rely heavily on such discrete branching. The latter makes plain autodiff mostly useless, since it can only account for the single path taken through the program. Our tool offers several backends that handle this situation, giving useful descent directions for optimization by accounting for alternative branches. Besides simulations, this problem arises in many other places, for example in deep learning when trying to combine imperative programs with neural networks.<p>In a nutshell, DiscoGrad applies an (LLVM-based) source-to-source transformation to your C++ program, adding some calls to our header library, which then handles the gradient computation. What sets it apart from similar tools&#x2F;estimators is that it&#x27;s fully automatic (no need to come up with a differentiable problem formulation&#x2F;reparametrization) and that the branching condition can be any function of the program inputs (no need to know upfront what distribution the condition follows).<p>We&#x27;re currently a team of two working on DiscoGrad as part of a research project, so don&#x27;t expect to see production-grade code quality, but we do intend for it to be more than a throwaway research prototype. Use cases we&#x27;ve successfully tested include calibrating simulation models of epidemics or evacuation scenarios via gradient descent, and combining simulations with neural networks in an end-to-end trainable fashion.<p>We hope you find this interesting and useful, and we&#x27;re happy to answer questions!",232,frankling_,1716725659,story,https://github.com/DiscoGrad/DiscoGrad
44524544,Show HN: Cactus – Ollama for Smartphones,"Hey HN, Henry and Roman here - we&#x27;ve been building a cross-platform framework for deploying LLMs, VLMs, Embedding Models and TTS models locally on smartphones.<p>Ollama enables deploying LLMs models locally on laptops and edge severs, Cactus enables deploying on phones. Deploying directly on phones facilitates building AI apps and agents capable of phone use without breaking privacy, supports real-time inference with no latency, we have seen personalised RAG pipelines for users and more.<p>Apple and Google actively went into local AI models recently with the launch of Apple Foundation Frameworks and Google AI Edge respectively. However, both are platform-specific and only support specific models from the company. To this end, Cactus:<p>- Is available in Flutter, React-Native &amp; Kotlin Multi-platform for cross-platform developers, since most apps are built with these today.<p>- Supports any GGUF model you can find on Huggingface; Qwen, Gemma, Llama, DeepSeek, Phi, Mistral, SmolLM, SmolVLM, InternVLM, Jan Nano etc.<p>- Accommodates from FP32 to as low as 2-bit quantized models, for better efficiency and less device strain.<p>- Have MCP tool-calls to make them performant, truly helpful (set reminder, gallery search, reply messages) and more.<p>- Fallback to big cloud models for complex, constrained or large-context tasks, ensuring robustness and high availability.<p>It&#x27;s completely open source. Would love to have more people try it out and tell us how to make it great!<p>Repo: <a href=""https:&#x2F;&#x2F;github.com&#x2F;cactus-compute&#x2F;cactus"">https:&#x2F;&#x2F;github.com&#x2F;cactus-compute&#x2F;cactus</a>",231,HenryNdubuaku,1752175259,story,https://github.com/cactus-compute/cactus
39467413,Launch HN: Danswer (YC W24) – Open-source AI search and chat over private data,"Hey HN! Chris and Yuhong here from Danswer (<a href=""https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer"">https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer</a>). We’re building an open source and self-hostable ChatGPT-style system that can access your team’s unique knowledge by connecting to 25 of the most common workplace tools (Slack, Google Drive, Jira, etc.). You ask questions in natural language and get back answers based on your team’s documents. Where relevant, answers are backed by citations and links to the exact documents used to generate them.<p>Quick Demo: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;hqSouur2FXw"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;hqSouur2FXw</a><p>Originally Danswer was a side project motivated by a challenge we experienced at work. We noticed that as teams scale, finding the right information becomes more and more challenging. I recall being on call and helping a customer recover from a mission critical failure but the error was related to some obscure legacy feature I had never used. For most projects, a simple question to ChatGPT would have solved it; but in this moment, ChatGPT was completely clueless without additional context (which I also couldn’t find).<p>We believe that within a few years, every org will be using team-specific knowledge assistants. We also understand that teams don’t want to tell us their secrets and not every team has the budget for yet another SaaS solution, so we open-sourced the project. It is just a set of containers that can be deployed on any cloud or on-premise. All of the data is processed and persisted on that same instance. Some teams have even opted to self-host open-source LLMs to truly airgap the system.<p>I also want to share a bit about the actual design of the system (<a href=""https:&#x2F;&#x2F;docs.danswer.dev&#x2F;system_overview"" rel=""nofollow"">https:&#x2F;&#x2F;docs.danswer.dev&#x2F;system_overview</a>). If you have questions about any parts of the flow such as the model choice, hyperparameters, prompting, etc. we’re happy to go into more depth in the comments.<p>The system revolves around a custom Retrieval Augmented Generation (RAG) pipeline we’ve built. During indexing time (we pull documents from connected sources every 10 minutes), documents are chunked and indexed into hybrid keyword+vector indices (<a href=""https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer&#x2F;blob&#x2F;main&#x2F;backend&#x2F;danswer&#x2F;indexing&#x2F;indexing_pipeline.py#L211"">https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer&#x2F;blob&#x2F;main&#x2F;backend&#x2F;dans...</a>).<p>For the vector index (which gives the system the flexibility to understand natural language queries), we use state of the art prefix-aware embedding models trained with contrastive loss. Optionally the system can be configured to go over each doc with multiple passes of different granularity to capture wide context vs fine details. We also supplement the vector search with a keyword based BM25 index + N-Grams so that the system performs well even in low data domains. Additionally we’ve added in learning from feedback and time based decay—see our custom ranking function (<a href=""https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer&#x2F;blob&#x2F;main&#x2F;backend&#x2F;danswer&#x2F;document_index&#x2F;vespa&#x2F;app_config&#x2F;schemas&#x2F;danswer_chunk.sd#L187"">https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer&#x2F;blob&#x2F;main&#x2F;backend&#x2F;dans...</a> – this flexibility is why we love Vespa as a Vector DB).<p>At query time, we preprocess the query with query-augmentation, contextual-rephrasing, as well as standard techniques like removing stopwords and lemmatization. Once the top documents are retrieved, we ask a smaller LLM to decide which of the chunks are “useful for answering the query” (this is something we haven’t seen much of elsewhere, but our tests have shown to be one of the biggest drivers for both precision and recall). Finally the most relevant passages are passed to the LLM along with the user query and chat history to produce the final answer. We post-process by checking guardrails and extracting citations to link the user to relevant documents. (<a href=""https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer&#x2F;blob&#x2F;main&#x2F;backend&#x2F;danswer&#x2F;prompts&#x2F;chat_prompts.py#L20"">https:&#x2F;&#x2F;github.com&#x2F;danswer-ai&#x2F;danswer&#x2F;blob&#x2F;main&#x2F;backend&#x2F;dans...</a>)<p>The Vector and Keyword indices are both stored locally and the NLP models run on the same instance (we’ve chosen ones that can run without GPU). The only exception is that the default Generative model is OpenAI’s GPT, however this can also be swapped out (<a href=""https:&#x2F;&#x2F;docs.danswer.dev&#x2F;gen_ai_configs&#x2F;overview"" rel=""nofollow"">https:&#x2F;&#x2F;docs.danswer.dev&#x2F;gen_ai_configs&#x2F;overview</a>).<p>We’ve seen teams use Danswer on problems like: Improving turnaround times for support by reducing time taken to find relevant documentation; Helping sales teams get customer context instantly by combing through calls and notes; Reducing lost engineering time from answering cross-team questions, building duplicate features due to inability to surface old tickets or code merges, and helping on-calls resolve critical issues faster by providing the complete history on an error in one place; Self-serving onboarding for new members who don’t know where to find information.<p>If you’d like to play around with things locally, check out the quickstart guide here: <a href=""https:&#x2F;&#x2F;docs.danswer.dev&#x2F;quickstart"" rel=""nofollow"">https:&#x2F;&#x2F;docs.danswer.dev&#x2F;quickstart</a>. If you already have Docker, you should be able to get things up and running in &lt;15 minutes. And for folks who want a zero-effort way of trying it out or don’t want to self-host, please visit our Cloud: <a href=""https:&#x2F;&#x2F;www.danswer.ai&#x2F;"">https:&#x2F;&#x2F;www.danswer.ai&#x2F;</a>",231,yuhongsun,1708611615,story,
40400224,Sam and Greg's response to OpenAI Safety researcher claims,,231,amrrs,1716050297,story,https://twitter.com/gdb/status/1791869138132218351
44971487,"AI crawlers, fetchers are blowing up websites; Meta, OpenAI are worst offenders",,231,rntn,1755776122,story,https://www.theregister.com/2025/08/21/ai_crawler_traffic/
44426233,Ask HN: What's the 2025 stack for a self-hosted photo library with local AI?,"First of all, this is purely a personal learning project for me, aiming to combine three of my passions: photography, software engineering, and my family memories. I have a large collection of family photos and want to build an interactive experience to explore them, ala Google or Apple Photo features.<p>My goal is to create a system with smart search capabilities, and one of the most important requirements is that it must run entirely on my local hardware. Privacy is key, but the main driver is the challenge and joy of building it myself (an obviously learn).<p>The key features I&#x27;m aiming for are:<p>Automatic identification and tagging of family members (local face recognition).<p>Generation of descriptive captions for each photo.<p>Natural language search (e.g., &quot;Show me photos of us at the beach in Luquillo from last summer&quot;).<p>I&#x27;ve already prompted AI tools for a high-level project plan, and they provided a solid blueprint (eg, Ollama with LLaVA, a vector DB like ChromaDB, you know it). Now, I&#x27;m highly interested in the real-world human experience. I&#x27;m looking for advice, learning stories, and the little details that only come from building something similar.<p>What tools, models, and best practices would you recommend for a project like this in 2025? Specifically, I&#x27;m curious about combining structured metadata (EXIF), face recognition data, and semantic vector search into a single, cohesive application.<p>Any and all advice would be deeply appreciated. Thanks!",230,jamesxv7,1751307050,story,
44941118,95% of generative AI pilots at companies are failing – MIT report,<a href=""https:&#x2F;&#x2F;mlq.ai&#x2F;media&#x2F;quarterly_decks&#x2F;v0.1_State_of_AI_in_Business_2025_Report.pdf"" rel=""nofollow"">https:&#x2F;&#x2F;mlq.ai&#x2F;media&#x2F;quarterly_decks&#x2F;v0.1_State_of_AI_in_Bus...</a>,230,amirkabbara,1755527763,story,https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/
43890538,Matrix-vector multiplication implemented in off-the-shelf DRAM for Low-Bit LLMs,,230,cpldcpu,1746401730,story,https://arxiv.org/abs/2503.23817
45458455,Which table format do LLMs understand best?,,230,oidar,1759460358,story,https://www.improvingagents.com/blog/best-input-data-format-for-llms
45984970,Microsoft AI CEO pushes back against critics after recent Windows AI backlash,,230,thewebguyd,1763585493,story,https://www.windowscentral.com/microsoft/windows-11/microsoft-ai-ceo-pushes-back-against-critics-after-recent-windows-ai-backlash-the-fact-that-people-are-unimpressed-is-mindblowing-to-me
42524190,"Parents of OpenAI Whistleblower Don't Believe He Died by Suicide, Order Autopsy",,230,miles,1735321112,story,https://sfist.com/2024/12/26/parents-of-openai-whistleblower-dont-believe-he-died-by-suicide-order-second-autopsy/
40834600,Chrome is adding `window.ai` – a Gemini Nano AI model right inside the browser,,229,modinfo,1719712601,story,https://twitter.com/rauchg/status/1806385778064564622
39224249,The FCC wants to criminalize AI robocall spam,,229,mikece,1706841228,story,https://www.theregister.com/2024/02/02/fcc_ai_robocall/
45733169,Our LLM-controlled office robot can't pass butter,"Hi HN! Our startup, Andon Labs, evaluates AI in the real world to measure capabilities and to see what can go wrong. For example, we previously made LLMs operate vending machines, and now we&#x27;re testing if they can control robots. There are two parts to this test:<p>1. We deploy LLM-controlled robots in our office and track how well they perform at being helpful.<p>2. We systematically test the robots on tasks in our office. We benchmark different LLMs against each other. You can read our paper &quot;Butter-Bench&quot; on arXiv: <a href=""https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2510.21860"" rel=""nofollow"">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2510.21860</a><p>The link in the title above (<a href=""https:&#x2F;&#x2F;andonlabs.com&#x2F;evals&#x2F;butter-bench"">https:&#x2F;&#x2F;andonlabs.com&#x2F;evals&#x2F;butter-bench</a>) leads to a blog post + leaderboard comparing which LLM is the best at our robotic tasks.",229,lukaspetersson,1761660805,story,https://andonlabs.com/evals/butter-bench
42342697,OpenAI Reinforcement Fine-Tuning Research Program,,229,thm,1733510276,story,https://openai.com/form/rft-research-program/
43532967,Show HN: WhatsApp MCP Server,"Hi HN – I built an open-source, self-hosted Model Context Protocol (MCP) server for WhatsApp: <a href=""https:&#x2F;&#x2F;github.com&#x2F;lharries&#x2F;whatsapp-mcp"" rel=""nofollow"">https:&#x2F;&#x2F;github.com&#x2F;lharries&#x2F;whatsapp-mcp</a><p>It connects to your personal WhatsApp account via the WhatsApp Web multi-device API (using whatsmeow from the Beeper team), and doesn&#x27;t rely on third-party APIs. All messages are stored locally in SQLite. Nothing is sent to the cloud unless you explicitly allow your LLM to access the data via tools – so you maintain full control and privacy.<p>The MCP server can:<p>- Search your messages, contacts, and groups<p>- Send WhatsApp messages to individuals or groups<p>Why build this?<p>99% of your life is stored in WhatsApp, by connecting an LLM to WhatsApp you get all this context. And your AI agent can execute tasks on your behalf by sending messages.",229,lharries,1743413574,story,https://github.com/lharries/whatsapp-mcp
44250774,EchoLeak – 0-Click AI Vulnerability Enabling Data Exfiltration from 365 Copilot,,228,pvg,1749669060,story,https://www.aim.security/lp/aim-labs-echoleak-blogpost
43887637,Dummy's Guide to Modern LLM Sampling,,228,nkko,1746375988,story,https://rentry.co/samplers
39059897,Yann LeCun: Human-level artificial intelligence is going to take a long time,,228,belter,1705692496,story,https://english.elpais.com/technology/2024-01-19/yann-lecun-chief-ai-scientist-at-meta-human-level-artificial-intelligence-is-going-to-take-a-long-time.html
43870998,Show HN: GPT-2 implemented using graphics shaders,"Back in the old days, people used to do general-purpose GPU programming by using shaders like GLSL. This is what inspired NVIDIA (and other companies) to eventually create CUDA (and friends). This is an implementation of GPT-2 using WebGL and shaders. Enjoy!",228,nathan-barry,1746199309,story,https://github.com/nathan-barry/gpt2-webgl
39717268,"Show HN: Open-source, browser-local data exploration using DuckDB-WASM and PRQL","Hey HN! We’ve built Pretzel, an open-source data exploration and visualization tool that runs fully in the browser and can handle large files (200 MB CSV on my 8gb MacBook air is snappy). It’s also reactive - so if, for example, you change a filter, all the data transform blocks after it re-evaluate automatically. You can try it here: <a href=""https:&#x2F;&#x2F;pretzelai.github.io&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;pretzelai.github.io&#x2F;</a> (static hosted webpage) or see a demo video here: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=73wNEun_L7w"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=73wNEun_L7w</a><p>You can play with the demo CSV that’s pre-loaded (GitHub data of text-editor adjacent projects) or upload your own CSV&#x2F;XLSX file. The tool runs fully in-browser—you can disconnect from the internet once the website loads—so feel free to use sensitive data if you like.<p>Here’s how it works: You upload a CSV file and then, explore your data as a series of successive data transforms and plots. For example, you might: (1) Remove some columns; (2)  Apply some filters (remove nulls, remove outliers, restrict time range etc); (3) Do a pivot (i.e, a group-by but fancier); (4) Plot a chart; (5) Download the chart and the the transformed data. See screenshot: <a href=""https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qO4yURI"" rel=""nofollow"">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qO4yURI</a><p>In the UI, each transform step appears as a “Block”. You can always see the result of the full transform in a table on the right. The transform blocks are editable - for instance in the example above, you can go to step 2, change some filters and the reactivity will take care of re-computing all the cells that follow, including the charts.<p>We wanted Pretzel to run locally in the browser <i>and</i> be extremely performant on large files. So, we parse CSVs with the fastest CSV parser (uDSV: <a href=""https:&#x2F;&#x2F;github.com&#x2F;leeoniya&#x2F;uDSV"">https:&#x2F;&#x2F;github.com&#x2F;leeoniya&#x2F;uDSV</a>) and use DuckDB-Wasm (<a href=""https:&#x2F;&#x2F;github.com&#x2F;duckdb&#x2F;duckdb-wasm"">https:&#x2F;&#x2F;github.com&#x2F;duckdb&#x2F;duckdb-wasm</a>) to do all the heavy lifting of processing the data. We also wanted to allow for chained data transformations where each new block operates on the result of the previous block. For this, we’re using PRQL (<a href=""https:&#x2F;&#x2F;prql-lang.org&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;prql-lang.org&#x2F;</a>) since it maps 1-1 with chained data transform blocks - each block maps to a chunk of PRQL which when combined, describes the full data transform chain. (PRQL doesn’t support DuckDB’s Pivot statement though so we had to make some CTE based hacks).<p>There’s also an AI block: This is the only (optional) feature that requires an internet connection but we’re working on adding local model support via Ollama. For now, you can use your own OpenAI API key or use an AI server we provide (GPT4 proxy; it’s loaded with a few credits), specify a transform in plain english and get back the SQL for the transform which you can edit.<p>Our roadmap includes allowing API calls to create new columns; support for an SQL block with nice autocomplete features, and a Python block (using Pyodide to run Python in the browser) on the results of the data transforms, much like a jupyter notebook.<p>There’s two of us and we’ve only spent about a week coding this and fixing major bugs so there are still some bugs to iron out. We’d <i>love</i> for you to try this and to get your feedback!",227,prasoonds,1710518561,story,https://github.com/pretzelai/pretzelai
40395107,Multi AI agent systems using OpenAI's assistants API,,227,metaskills,1715988332,story,https://github.com/metaskills/experts
41900402,ByteDance sacks intern for sabotaging AI project,,226,beedeebeedee,1729480914,story,https://www.bbc.com/news/articles/c7v62gg49zro
43988381,"Show HN: Muscle-Mem, a behavior cache for AI agents","Hi HN! Erik here from Pig.dev, and today I&#x27;d like to share a new project we&#x27;ve just open sourced:<p>Muscle Mem is an SDK that records your agent&#x27;s tool-calling patterns as it solves tasks, and will deterministically replay those learned trajectories whenever the task is encountered again, falling back to agent mode if edge cases are detected. Like a JIT compiler, for behaviors.<p>At Pig, we built computer-use agents for automating legacy Windows applications (healthcare, lending, manufacturing, etc).<p>A recurring theme we ran into was that businesses <i>already</i> had RPA (pure-software scripts), and it worked for them in most cases. The pull to agents as an RPA alternative was <i>not</i> to have an infinitely flexible &quot;AI Employees&quot; as tech Twitter&#x2F;X may want you to think, but simply because their RPA breaks under occasional edge-cases and agents can gracefully handle those cases.<p>Using a pure-agent approach proved to be highly wasteful. Window&#x27;s accessibility APIs are poor, so you&#x27;re generally stuck using pure-vision agents, which can run around $40&#x2F;hr in token costs and take 5x longer than a human to perform a workflow. At this point, you&#x27;re better off hiring a human.<p>The goal of Muscle-Mem is to get LLMs out of the hot path of repetitive automations, intelligently swapping between script-based execution for repeat cases, and agent-based automations for discovery and self-healing.<p>While inspired by computer-use environments, Muscle Mem is designed to generalize to any automation performing discrete tasks in dynamic environments. It took a great deal of thought to figure out an API that generalizes, which I cover more deeply in this blog: <a href=""https:&#x2F;&#x2F;erikdunteman.com&#x2F;blog&#x2F;muscle-mem&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;erikdunteman.com&#x2F;blog&#x2F;muscle-mem&#x2F;</a><p>Check out the repo, consider giving it a star, or dive deeper into the above blog. I look forward to your feedback!",226,edunteman,1747251506,story,https://github.com/pig-dot-dev/muscle-mem
45672199,"Look, Another AI Browser",,226,v3am,1761153338,story,https://manuelmoreale.com/thoughts/look-another-ai-browser
44843605,Let's properly analyze an AI article for once,,226,pabs3,1754706657,story,https://nibblestew.blogspot.com/2025/08/lets-properly-analyze-ai-article-for.html
38457815,Show HN: Hacky Meta Glasses GPT4 Vision Integration,Super hacky implementation due to the lack of an SDK. Fun project though.<p>In the foodlog demonstration I just made a fake fb account (sorry zucc) called &quot;Mye Food-Log&quot;.,226,devon_c,1701254587,story,https://github.com/dcrebbin/meta-vision-api
45927435,Oracle hit hard in Wall Street's tech sell-off over its AI bet,,226,1vuio0pswjnm7,1763132662,story,https://www.ft.com/content/583e9391-bdd0-433e-91e0-b1b93038d51e
40198766,SB-1047 will stifle open-source AI and decrease safety,,225,kmdupree,1714400953,story,https://www.answer.ai/posts/2024-04-29-sb1047.html
45799211,OpenAI signs $38B cloud computing deal with Amazon,,225,donohoe,1762179605,story,https://www.nytimes.com/2025/11/03/technology/openai-amazon-cloud-computing.html
42697346,Show HN: A blocklist to remove spam and bad websites from search results,"Hi HN!<p>I&#x27;ve been fed up with search results so much that I decided to make a giant blocklist to remove garbage links by using uBlacklist.<p>I browsed other blocklists and wasn&#x27;t very satisfied from what exists now; the goal of this one is to be super organized and transparent, explaining why each site was blocked via issues. Contributions welcome!<p>Even though around 100 domains are blocked so far, I already noticed a big improvement in casual searches. You&#x27;d be surprised how some AI generated websites can dominate the #1 page on DuckDuckGo.",225,popcar2,1736863601,story,https://github.com/popcar2/BadWebsiteBlocklist
43850377,JetBrains defends removal of negative reviews for unpopular AI Assistant,,225,przemub,1746045397,story,https://devclass.com/2025/04/30/jetbrains-defends-removal-of-negative-reviews-for-unpopular-ai-assistant/
45416353,Vercel CEO meets with Netanyahu to discuss AI education,,225,cramsession,1759166491,story,https://twitter.com/rauchg/status/1972669025525158031
45886131,"OpenAI may not use lyrics without license, German court rules",<a href=""https:&#x2F;&#x2F;archive.li&#x2F;qs8hq"" rel=""nofollow"">https:&#x2F;&#x2F;archive.li&#x2F;qs8hq</a>,224,aiz0Houp,1762860055,story,https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/
44399234,SymbolicAI: A neuro-symbolic perspective on LLMs,,224,futurisold,1751050197,story,https://github.com/ExtensityAI/symbolicai
41437846,Show HN: Hestus – AI Copilot for CAD,"Hello! We’re Kevin and Sohrab from Hestus (<a href=""https:&#x2F;&#x2F;www.hestus.co"" rel=""nofollow"">https:&#x2F;&#x2F;www.hestus.co</a>). We&#x27;re working on an AI copilot for CAD. Today we&#x27;re releasing a simple sketch helper for Fusion 360 and would love your feedback. Here’s a quick demo: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=L9n_eY-fM_E"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=L9n_eY-fM_E</a>.<p>Why we’re doing this: Mechanical engineers excel at generating initial design concepts but get bogged down translating ideas into final designs due to tedious, repetitive tasks. Our goal is to automate these mundane processes, allowing engineers to focus on the creative aspects of design.<p>Having worked at multiple hardware companies—from medical devices to space launch vehicles—we know how often “trivial” components such as manufacturing rigging, get brushed under the table in scheduling conversations.  These tasks aren’t necessarily complex, but they take time and still require the rigor of production components.  From finding the perfect fastener to making sure mounting holes align, we aim to simplify and accelerate the design process from the complex to the mundane.<p>We&#x27;re tackling this problem similarly to how coding copilots help programmers work faster. Initially, rudimentary coding assistants offered simple suggestions like auto-completing variables. Now, they understand complex tasks, write entire code blocks, and help fix bugs. We&#x27;re taking this step-by-step approach, starting with a beta that focuses on sketching.<p>Our sketch helper offers design suggestions, such as applying equality constraints to similarly sized circles or adding tangent constraints between lines and curves. While designers can do these tasks manually, they often require dozens of precise mouse clicks. Our software makes suggestions that you can preview and accept to streamline your workflow. Over time we aim to improve at anticipating your needs and expand beyond sketching to other design aspects like resolving interference issues, auto-generating bills of materials with purchase links, and offering manufacturability suggestions.<p>How this is different from other solutions: we&#x27;ve heard of complete generative part design solutions, but we don&#x27;t believe this top down approach is the best way to assist mechanical engineers. Engineers excel at and enjoy designing new concepts—we want to focus on streamlining the most tedious aspects. Crucially, we find that generative solutions often overlook manufacturability, a key aspect of design.<p>We invite you to try our sketch helper and share your thoughts! If you can think of any additional features that would make it more useful to you, we’d love to hear what they are. Any and all feedback is welcome!",224,kevinsane,1725389365,story,https://www.hestus.co/
42421157,Tenstorrent and the State of AI Hardware Startups,,224,zdw,1734231598,story,https://irrationalanalysis.substack.com/p/tenstorrent-and-the-state-of-ai-hardware
38603207,GigaGPT: GPT-3 sized models in 565 lines of code,,223,georgehill,1702318408,story,https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code
42537332,Can LLMs accurately recall the Bible?,,223,benkaiser,1735443892,story,https://benkaiser.dev/can-llms-accurately-recall-the-bible/
42571608,Meta Wants More AI Bots on Facebook and Instagram,,223,thm,1735791600,story,https://nymag.com/intelligencer/article/meta-wants-more-ai-bots-on-facebook-and-instagram.html
39335509,RLHF a LLM in <50 lines of Python,,223,patelajay285,1707664357,story,https://datadreamer.dev/docs/latest/pages/get_started/quick_tour/aligning.html
39458363,Neural Network Diffusion,,223,vagabund,1708543904,story,https://arxiv.org/abs/2402.13144
45122885,A high schooler writes about AI tools in the classroom,<a href=""https:&#x2F;&#x2F;archive.is&#x2F;twynO"" rel=""nofollow"">https:&#x2F;&#x2F;archive.is&#x2F;twynO</a>,223,dougb5,1756953462,story,https://www.theatlantic.com/technology/archive/2025/09/high-school-student-ai-education/684088/
41185783,GPT-4 LLM simulates people well enough to replicate social science experiments,,223,thoughtpeddler,1723066236,story,https://www.treatmenteffect.app/
42660184,How we used GPT-4o for image detection with 350 similar illustrations,,222,olup,1736542952,story,https://olup-blog.pages.dev/stories/image-detection-cars
42779330,Show HN: Using YOLO to Detect Office Chairs in 40M Hotel Photos,I used the YOLO object detection library from Ultralytics to scan over 40 million hotel photos and identify images with office chairs. This helped me create a map showing hotels suitable for remote work.<p>Map: <a href=""https:&#x2F;&#x2F;www.tripoffice.com&#x2F;maps"" rel=""nofollow"">https:&#x2F;&#x2F;www.tripoffice.com&#x2F;maps</a><p>Yolo: <a href=""https:&#x2F;&#x2F;www.ultralytics.com&#x2F;yolo"" rel=""nofollow"">https:&#x2F;&#x2F;www.ultralytics.com&#x2F;yolo</a><p>The whole process was done on a home Mac without the use of any LLMs. It&#x27;s based on traditional object detection technology.,222,nomad86,1737462146,story,
44513843,Biomni: A General-Purpose Biomedical AI Agent,,222,GavCo,1752088808,story,https://github.com/snap-stanford/Biomni
40997585,GPT-4o mini: advancing cost-efficient intelligence,,222,bryanh,1721322135,story,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/
40433511,"Wikimedia Enterprise – APIs for LLMs, AI Training, and More",,222,ks2048,1716323550,story,https://enterprise.wikimedia.com/
40222051,Show HN: I'm 16 and building an AI based startup called Factful with friends,"Hey HN! My name is Andrew, and I&#x27;m thrilled to share with you a project I&#x27;ve been working on called Factful.<p>I&#x27;m a high school student with a passion for tackling misinformation online. Inspired by the need for more reliable content verification tools, I decided to create Factful. It&#x27;s an AI-powered web app designed to revolutionize how individuals and organizations verify content.<p>Unlike traditional grammar checkers, Factful provides a comprehensive analysis that goes beyond just grammar. It evaluates context, factuality, coherence, and more to ensure the accuracy and credibility of content.<p>I believe that in today&#x27;s information age, it&#x27;s more crucial than ever to have tools like Factful to combat misinformation and promote content integrity. I&#x27;m excited to continue developing Factful and would love for you to check it out. Your feedback and support would mean the world to me. Thanks for taking the time to read about Factful, and please go check out our beta deployment of Factful (a little beyond the MVP) for free on our website!",222,helloduck1234,1714564810,story,https://factful.io/
44847741,The current state of LLM-driven development,,222,Signez,1754756236,story,http://blog.tolki.dev/posts/2025/08-07-llms/
42164141,Claude AI built me a React app to compare maps side by side,,222,caspg,1731850779,story,https://github.com/veloplanner/map-matrix
43692476,Launch HN: mrge.io (YC X25) – Cursor for code review,"Hey HN, we’re building mrge (<a href=""https:&#x2F;&#x2F;www.mrge.io&#x2F;home"">https:&#x2F;&#x2F;www.mrge.io&#x2F;home</a>), an AI code review platform to help teams merge code faster with fewer bugs. Our early users include Better Auth, Cal.com, and n8n—teams that handle a lot of PRs every day.<p>Here’s a demo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pglEoiv0BgY"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=pglEoiv0BgY</a><p>We (Allis and Paul) are engineers who faced this problem when we worked together at our last startup. Code review quickly became our biggest bottleneck—especially as we started using AI to code more. We had more PRs to review, subtle AI-written bugs slipped through unnoticed, and we (humans) increasingly found ourselves rubber-stamping PRs without deeply understanding the changes.<p>We’re building mrge to help solve that. Here’s how it works:<p>1. Connect your GitHub repo via our Github app in two clicks (and optionally download our desktop app). Gitlab support is on the roadmap!<p>2. AI Review: When you open a PR, our AI reviews your changes directly in an ephemeral and secure container. It has context into not just that PR, but your whole codebase, so it can pick up patterns and leave comments directly on changed lines. Once the review is done, the sandbox is torn down and your code deleted – we don’t store it for obvious reasons.<p>3. Human-friendly review workflow: Jump into our web app (it’s like Linear but for PRs). Changes are grouped logically (not alphabetically), with important diffs highlighted, visualized, and ready for faster human review.<p>The AI reviewer works a bit like Cursor in the sense that it navigates your codebase using the same tools a developer would—like jumping to definitions or grepping through code.<p>But a big challenge was that, unlike Cursor, mrge doesn’t run in your local IDE or editor. We had to recreate something similar entirely in the cloud.<p>Whenever you open a PR, mrge clones your repository and checks out your branch in a secure and isolated temporary sandbox.  We provision this sandbox with shell access and a Language Server Protocol (LSP) server. The AI reviewer then reviews your code, navigating the codebase exactly as a human reviewer would—using shell commands and common editor features like &quot;go to definition&quot; or &quot;find references&quot;. When the review finishes, we immediately tear down the sandbox and delete the code—we don’t want to permanently store it for obvious reasons.<p>We know cloud-based review isn&#x27;t for everyone, especially if security or compliance requires local deployments. But a cloud approach lets us run SOTA AI models without local GPU setups, and provide a consistent, single AI review per PR for an entire team.<p>The platform itself focuses entirely on making <i>human</i> code reviews easier. A big inspiration came from productivity-focused apps like Linear or Superhuman, products that show just how much thoughtful design can impact everyday workflows. We wanted to bring that same feeling into code review.<p>That’s one reason we built a desktop app. It allowed us to deliver a more polished experience, complete with keyboard shortcuts and a snappy interface.<p>Beyond performance, the main thing we care about is making it easier for humans to read and understand code. For example, traditional review tools sort changed files alphabetically—which forces reviewers to figure out the order in which they should review changes. In mrge, files are automatically grouped and ordered based on logical connections, letting reviewers immediately jump in.<p>We think the future of coding isn’t about AI replacing humans—it’s about giving us better tools to quickly understand high-level changes, abstracting more and more of the code itself. As code volume continues to increase, this shift is going to become increasingly important.<p>You can sign up now (<a href=""https:&#x2F;&#x2F;www.mrge.io&#x2F;home"">https:&#x2F;&#x2F;www.mrge.io&#x2F;home</a>). mrge is currently free while we&#x27;re still early. Our plan for later is to charge closed-source projects on a per-seat basis, and to continue giving mrge away for free to open source ones.<p>We’re very actively building and would love your honest feedback!",221,pomarie,1744724061,story,
44577018,OpenAI – vulnerability responsible disclosure,,221,requilence,1752622194,story,https://requilence.any.org/open-ai-vulnerability-responsible-disclosure
44935169,Llama-Scan: Convert PDFs to Text W Local LLMs,,221,nawazgafar,1755466847,story,https://github.com/ngafar/llama-scan
39982818,GPT-4 Turbo with Vision Generally Available,,220,davidbarker,1712688834,story,https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4
44302797,LLMs pose an interesting problem for DSL designers,,220,gopiandcode,1750187825,story,https://kirancodes.me/posts/log-lang-design-llms.html
44471695,Problems the AI industry is not addressing adequately,,220,baylearn,1751711598,story,https://www.thealgorithmicbridge.com/p/im-losing-all-trust-in-the-ai-industry
45155398,Unofficial Windows 11 requirements bypass tool allows disabling all AI features,,220,pinewurst,1757219238,story,https://www.neowin.net/news/unofficial-windows-11-requirements-bypass-tool-now-allows-you-to-disable-all-ai-features/
42726584,Test-driven development with an LLM for fun and profit,,219,crazylogger,1737041419,story,https://blog.yfzhou.fyi/posts/tdd-llm/
42793567,OpenAI has upped its lobbying efforts nearly sevenfold,,219,Brajeshwar,1737558334,story,https://www.technologyreview.com/2025/01/21/1110260/openai-ups-its-lobbying-efforts-nearly-seven-fold/
44645391,The Hater's Guide to the AI Bubble,,219,lukebennett,1753181838,story,https://www.wheresyoured.at/the-haters-gui/
39137502,AMD Publishes XDNA Linux Driver: Support for Ryzen AI on Linux,,219,pella,1706229491,story,https://www.phoronix.com/news/AMD-XDNA-Linux-Driver-Ryzen-AI
45968362,Solving a million-step LLM task with zero errors,,219,Anon84,1763483188,story,https://arxiv.org/abs/2511.09030
42098236,Physical Intelligence's first generalist policy AI can finally do your laundry,,218,Terretta,1731205592,story,https://www.physicalintelligence.company/blog/pi0
45464849,"OpenAI Is Just Another Boring, Desperate AI Startup",,218,speckx,1759509421,story,https://www.wheresyoured.at/sora2-openai/
41695840,Screenpipe: 24/7 local AI screen and mic recording,,218,thunderbong,1727694936,story,https://github.com/mediar-ai/screenpipe
38579170,"EU strikes deal to regulate ChatGPT, AI tech",,218,helsinkiandrew,1702102811,story,https://www.bloomberg.com/news/articles/2023-12-08/eu-strikes-deal-to-regulate-chatgpt-other-ai-in-landmark-act
42966958,Why LLMs still have problems with OCR,"Document ingestion and the launch of Gemini 2.0 caused a lot of buzz this week. As a team building in this space, this is something we researched thoroughly. Here’s our take: ingestion is a multistep pipeline, and maintaining confidence from LLM nondeterministic outputs over millions of pages is a problem.",218,ritvikpandey21,1738879443,story,https://www.runpulse.com/blog/why-llms-suck-at-ocr
44195961,Tokasaurus: An LLM inference engine for high-throughput workloads,,218,rsehrlich,1749158827,story,https://scalingintelligence.stanford.edu/blogs/tokasaurus/
42606231,Killed by LLM,,218,yz-exodao,1736122491,story,https://r0bk.github.io/killedbyllm/
41983409,Launch HN: Integuru (YC W24) – Reverse-engineer internal APIs using LLMs,"Hey HN! We’re Richard and Alan from Integuru (<a href=""https:&#x2F;&#x2F;integuru.ai"" rel=""nofollow"">https:&#x2F;&#x2F;integuru.ai</a>). We build low-latency integrations with platforms lacking official APIs. We take custom requests and manage creation, hosting, and authentication. To automate our work, we built an open-source AI agent that reverse-engineers internal APIs to generate integration code. Here’s a demo: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7OJ4w5BCpQ0"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=7OJ4w5BCpQ0</a>.<p>Many products need integrations with third-party services, but platforms often lack official APIs. Examples include logistics software, financial services, electronic health records (EHRs), and government websites. To build low-latency integrations, developers must reverse-engineer internal APIs, but this can get complicated. With Integuru, you can have easier access to integrations.<p>We started as recent college grads trying to make US income tax data accessible. We contacted banks, brokerages, payroll software, and more to request access to their APIs, but none took us seriously. We resorted to building integrations with these systems to extract documents like W-2s and 1099s. We initially used browser automation but ran into two big problems: our integrations (1) weren’t reliable due to changing UIs and (2) had slow execution speeds due to spinning up browsers and waiting for pages to load. We experimented with AI-based automation maintenance, but it didn’t solve slow speeds. So, we concluded that browser automation is useful when execution speed isn’t essential, but reverse engineering is often the only path for performant integrations.<p>Through reverse-engineering dozens of platforms, we noticed many internal API design patterns that LLMs could decipher. We built an agent to automate the creation of integrations. Today, Integuru can analyze a platform’s internal API designs and build an integration in minutes.<p>The agent mimics what a human does when reverse-engineering. Say you want to download utility bills from a utility website. You’d first use Integuru to generate a file of network requests and a file of cookies. You pair these two files with a prompt about your desired action—in this case, to download utility bills.<p>Integuru identifies the final request that downloads utility bills. The request URL might look like this: <a href=""https:&#x2F;&#x2F;www.example.com&#x2F;utility-bills?accountId=123&amp;userId=456"" rel=""nofollow"">https:&#x2F;&#x2F;www.example.com&#x2F;utility-bills?accountId=123&amp;userId=4...</a>. It then identifies parts of the request that depend on other requests. The example URL contains dynamic parts— accountId and userId—that usually are in the response of previous request(s). It then finds other requests whose response contains any of these and adds them to the dependency graph. The newly found request URLs might look like <a href=""https:&#x2F;&#x2F;www.example.com&#x2F;getAccountId"" rel=""nofollow"">https:&#x2F;&#x2F;www.example.com&#x2F;getAccountId</a>,  <a href=""https:&#x2F;&#x2F;www.example.com&#x2F;getUserId"" rel=""nofollow"">https:&#x2F;&#x2F;www.example.com&#x2F;getUserId</a>, and so on.<p>This process repeats until the most recently found request doesn’t depend on any other request. Integuru then traverses up the graph, starting from the requests without dependencies while converting each request into a runnable function.<p>Integuru supports a surprising number of use cases like downloading documents, sending money, creating virtual cards… People already use the agent to build low-latency APIs for platforms like Robinhood, transportation management systems (TMS), and more. However, the agent still has limitations due to current LLM capabilities and long-tail edge cases, but we’ve been giving each platform to the agent for the first try. When the agent does struggle, we find the generated graphs and code still helpful as references for us to complete the work manually.<p>The agent and all integrations are open-source under AGPL-3.0. We charge for services to (1) build custom integrations when the agent struggles or for your convenience, (2) handle hosting, and (3) manage authentication using authentication cookies from authenticated browser sessions. We charge per API call with an implementation fee for new platforms.<p>We’re currently working to increase the agent’s coverage and improve code generation. We will continue to iterate and want to one day allow developers to integrate with all platforms instantly.<p>Integuru is still an early effort. We’re passionate about automating integrations and would love your feedback!",218,richardzhang,1730206840,story,https://github.com/Integuru-AI/Integuru
45261930,Generative AI as Seniority-Biased Technological Change,,218,zeuch,1758029075,story,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555
39993626,Implementation of Google's Griffin Architecture – RNN LLM,,218,milliondreams,1712771231,story,https://github.com/google-deepmind/recurrentgemma
39619053,Show HN: Fructose – LLM calls as strongly typed functions,"Hi HN! Erik here from Banana (formerly the serverless GPU platform), excited to show you what we’ve been working on next:<p>Fructose<p>Fructose is a python package to call LLMs as strongly typed functions. It uses function type signatures to guide the generation and guarantee a correctly typed output, in whatever basic&#x2F;complex python datatype requested.<p>By guaranteeing output structure, we believe this will enable more complex applications to be built, interweaving code with LLMs with code. For now, we’ve shipped Fructose as a client-only library simply calling gpt-4 (by default) with json mode, pretty simple and not unlike other packages such as marvin and instructor, but we’re also working on our own lightweight formatting model that we’ll host and&#x2F;or distribute to the client, to help reduce token burn and increase accuracy.<p>We figure, no time like the present to show y’all what we’re working on! Questions, compliments, and roasts welcomed.",218,edunteman,1709749062,story,https://github.com/bananaml/fructose
44578070,LLM Daydreaming,,218,nanfinitum,1752632522,story,https://gwern.net/ai-daydreaming
46003144,FAWK: LLMs can write a language interpreter,,218,todsacerdoti,1763720929,story,https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html
44757247,OpenAI raises $8.3B at $300B valuation,<a href=""https:&#x2F;&#x2F;archive.md&#x2F;H9JNt"" rel=""nofollow"">https:&#x2F;&#x2F;archive.md&#x2F;H9JNt</a>,217,mfiguiere,1754058159,story,https://www.nytimes.com/2025/08/01/business/dealbook/openai-ai-mega-funding-deal.html
40937260,General Theory of Neural Networks,,217,rdlecler1,1720708460,story,https://robleclerc.substack.com/p/general-theory-of-neural-networks
44490863,"Launch HN: Morph (YC S23) – Apply AI code edits at 4,500 tokens/sec","Hey HN, I’m Tejas at Morph. We’ve built a blazing-fast model for applying AI-generated code edits directly into your files at 4,500+ tokens&#x2F;sec. No more slow full-file rewrites or brittle search-and-replace hacks.<p>Here&#x27;s a demo video: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LdT8epGHJPk"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LdT8epGHJPk</a>.<p>Why? AI spits out code that can’t reliably be inserted into existing code. Full file rewrites, brittle search-and-replace hacks are too slow, expensive, or error-prone.<p>Morph&#x27;s approach:<p>- Your agent outputs edits “lazily”, referencing unmodified lines in the existing file (ex: &#x2F;&#x2F; ...existing code...)<p>- Morph instantly applies these edits to a file using our Fast Apply model + speculative decoding against the original file, making AI patches fast, reliable, and production-ready.<p>This approach was pioneered by Cursor last year, but their models aren’t available as APIs—so we built Morph for developers everywhere (with a large free tier!)<p>Live demo (no signup): <a href=""https:&#x2F;&#x2F;morphllm.com&#x2F;dashboard"">https:&#x2F;&#x2F;morphllm.com&#x2F;dashboard</a> and docs: <a href=""https:&#x2F;&#x2F;docs.morphllm.com&#x2F;quickstart"">https:&#x2F;&#x2F;docs.morphllm.com&#x2F;quickstart</a><p>We have 2 Fast Apply models: morph-v3-fast - 4500+ tok&#x2F;sec, and morph-v3-large - 2500+ tok&#x2F;sec. These models power Fast Apply at create.xyz, databutton, continue.dev, and more!<p>We also provide retrieval models for embedding + reranking. Next Up: Inline Edit Model (Cmd-K): Extremely fast inline edits - keep dev flow state; and Morph Tab API: Our Next Edit Prediction model guesses your next code edit + action with sub-500ms latency. It&#x27;s currently in private beta, but you can request early access here: <a href=""https:&#x2F;&#x2F;morphllm.com&#x2F;tab"">https:&#x2F;&#x2F;morphllm.com&#x2F;tab</a><p>Hot takes:<p>1) Raw inference speed matters more than incremental accuracy gains for dev UX—agree or disagree?<p>2) Full-file rewrites by frontier models are legacy—Fast Apply edits win on speed, cost, reliability.<p>3) As benchmarks on narrow tasks saturate to 99%+, complexity is shifting from single frontier models to specialized inference-optimized models. As frontier models move upmarket, they&#x27;ll leave simple tasks behind, and they&#x27;ll be used to do tasks only frontier models can do<p>We’d love to hear your ideas and experiences with coding agents!",217,bhaktatejas922,1751899245,story,
40371467,Show HN: I made a Mac app to search my images and videos locally with ML,"Desktop Docs is a Mac app that lets you search all your photos and videos in seconds with AI.<p>Once you find the file you&#x27;re looking for you can resize it, export it to Adobe Premiere Pro, or drag and drop it into another app.<p>I built Desktop Docs because I keep tons of media files on my computer and I can never remember where I save stuff (lots of screenshots, memes, and downloads). The Apple Photos app also only supports photos in your iCloud.<p>Desktop Docs supports adding folders or individual files to an AI Library where you can search by the contents of your files, not just file titles.<p>You can search by objects (&quot;cardboard box&quot;), actions (&quot;man smiling&quot;, &quot;car driving&quot;), by emotion (&quot;surprised woman&quot;, &quot;sad cowboy&quot;), or the text in the frame (great for screenshots or memes).<p>It&#x27;s also 100% private. Make any media searchable without it ever leaving your computer.<p>How I built it: - 100% Javascript (I&#x27;m using Electron JS and React JS). - Embedding generation (CLIP from OpenAI is used to compute the image embeddings and text embeddings for user queries). - Redis (storing and doing KNN search on the embeddings with this DB). - Image&#x2F;video editing (the app ships with FFmpeg binaries to explode videos into individual frames and scale images).<p>Demo: <a href=""https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EIUgPNHOKKc"" rel=""nofollow"">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=EIUgPNHOKKc</a><p>If there are any features you&#x27;d like to see in Desktop Docs or want to learn more about how I built it, drop me a comment below. Happy to share more.",217,correa_brian,1715802286,story,https://desktopdocs.com
41582539,Show HN: I made crowdwave – imagine Twitter/Reddit but every post is a voicemail,"Hey it&#x27;s Andrew - author of <a href=""https:&#x2F;&#x2F;www.crowdwave.com"" rel=""nofollow"">https:&#x2F;&#x2F;www.crowdwave.com</a> here!<p>- crowdwave works best on your phone - unless you&#x27;ve got your headset and microphone plugged in to your desktop, in which case desktop works great too.<p>Here&#x27;s the story:<p>So about six months ago I saw this post on HN <a href=""https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39910119"">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39910119</a><p><a href=""https:&#x2F;&#x2F;afterthebeep.tel"" rel=""nofollow"">https:&#x2F;&#x2F;afterthebeep.tel</a> is really cool - it&#x27;s an anonymous voicemail box - you call the provided phone the number and leave a message. Blaine - the guy who runs the site (eventually) listens to and approves your message and writes a headline.  It was fun, and I found I kept going back to it and listening to the messages. I left a message once and several weeks later it appeared on the site. Blaine, from comments I read, didn&#x27;t seem in a hurry to take the site much further, which got me thinking...<p>And I simply could not get one question out of my head - &quot;what would happen if users could just hit record on their phone, instead of having to dial a phone number?&quot;.<p>When I get a software idea I get pretty obsessive and that question just kept gnawing at me.<p>So, like the any reasonable programmer would, I stopped working on the project I had been working on for literally YEARS and took a detour. Because that&#x27;s what you do isn&#x27;t it - you just drop those multiple years of work and pick up the shiny new thing.<p>I saw that afterthebeep is open source and I loved the UI design - the Windows 3.1 aesthetic really appealed to me - it seems perfect for voicemail, so I grabbed the open source code and started development. I couldn&#x27;t make much sense of the code - it was using tech I&#x27;m not familiar with, so I ditched it all except the layout and the graphics.<p>Fortunately, the project I had been working on for YEARS is basically a Twitter&#x2F;Reddit clone, so I ripped the UI out of the afterthebeep open source project and did open heart surgery until like some bizarre Frankenstein&#x27;s monster I had put the afterthebeep open source UI onto my code.<p>And I added in the functionality that I craved so much - a &quot;record&quot; button. Sigh.... relief. It was incredibly satisfying to hit record and see a message appear almost immediately. Nerd craving fulfilled.<p>But my satisfaction did not last long. I REALLY HAD TO fix that problem of getting the posts approved and headlines written. So I made a back end audio processing pipeline and fed the messages into an LLM, which ripped the text from the speech and I then shoved it into OpenAI and asked it to make nice headlines. And it worked beautifully - now you only have to wait 30 seconds to see your message with a nice headline!  Ahhhh..... sigh, satisfaction... (it wouldn&#x27;t be 2024 without an AI twist, would it now?).<p>But hang on! It would be <i>SO much better</i> if there was some sort of category system almost like subreddits - then people could post their messages into areas of interest. So I built the channel system and sat back.... job done.<p>Looking at the calendar, dreading to see..... I&#x27;ve dropped into obsessive coding mode and and I&#x27;ve been down this rabbit hole full time for MONTHS. I&#x27;m getting wary - and I&#x27;m also getting tired and sick of the effort - when&#x27;s this going to end?<p>But wait, another idea! How much more cool would it be if you could have your own user account, and follow and like and subscribe!  I&#x27;ve just GOT TO make that. AND surely it has to be multi language doesn&#x27;t it?  I mean Germans like talking too don&#x27;t they? And user profile pics, and channel banner images, and options and settings. And if you don&#x27;t put in terms and conditions and privacy and a cookie message then won&#x27;t the Eurpoeans turn up and arrest me? At this stage I&#x27;m like a drunken junkie wanting just one more thing, one more thing...... scope ain&#x27;t just creeping, the scope is up and racing away faster than Usain Bolt.<p>I&#x27;m now like nearly five months into this and packing all this functionality into a UI that both make sense and fits onto a tiny phone screen is becoming a huge challenge - a challenge I don&#x27;t know if I can actually solve - and if I can&#x27;t make the UI make sense then the whole thing will be unusable.  The UI MUST be minimal and yet still reveal to the user pretty much everything within fewer than five pages in total.  The UI had to work BEST on a phone.  That was a HUGE challenge, and I really didn&#x27;t know until the end of the project if I could do it at all. But finally the UI seemed to come together and it was a tight squeeze but fit onto the limited screen resolution of even my old iPhone 6s (yes it&#x27;s my main phone).<p>Then, a few days ago, after many months of grueling grind, there was nothing left on the todo list. crowdwave was done! All the features were done and I&#x27;d finally chased down that scope creep.<p>Which brings us to today. Give <a href=""https:&#x2F;&#x2F;www.crowdwave.com"" rel=""nofollow"">https:&#x2F;&#x2F;www.crowdwave.com</a> a go on your phone or desktop if you have microphone. It&#x27;s brand new so there WILL be bugs - hopefully not too severe. Thanks to Blaine at <a href=""https:&#x2F;&#x2F;blaines.world&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;blaines.world&#x2F;</a> for the inspiration!",217,andrewstuart,1726679242,story,https://www.crowdwave.com
45210399,Center for the Alignment of AI Alignment Centers,,217,louisbarclay,1757590943,story,https://alignmentalignment.ai
44366409,Gemini Robotics On-Device brings AI to local robotic devices,,216,meetpateltech,1750773910,story,https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/
38434914,Learnings from fine-tuning LLM on my Telegram messages,,216,furiousteabag,1701104977,story,https://asmirnov.xyz/doppelganger
42304333,"Certain names make ChatGPT grind to a halt, and we know why",,216,rbanffy,1733217306,story,https://arstechnica.com/information-technology/2024/12/certain-names-make-chatgpt-grind-to-a-halt-and-we-know-why/
43944974,LTXVideo 13B AI video generation,,216,zoudong376,1746878350,story,https://ltxv.video/
40711600,Google DeepMind shifts from research lab to AI product factory,,216,kjhughes,1718661779,story,https://www.bloomberg.com/news/articles/2024-06-17/google-deepmind-shifts-from-research-lab-to-ai-product-factory
41301673,Fine-tuning now available for GPT-4o,,216,davidbarker,1724172652,story,https://openai.com/index/gpt-4o-fine-tuning/
44565416,"Anthropic, Google, OpenAI and XAI Granted Up to $200M from Defense Department",,216,ChrisArchitect,1752527779,story,https://www.cnbc.com/2025/07/14/anthropic-google-openai-xai-granted-up-to-200-million-from-dod.html
38733384,How to make LLMs go fast,,215,tosh,1703246967,story,https://vgel.me/posts/faster-inference/
40857589,Show HN: SQL Explorer – Open-source reporting tool that Just Works,"I have been working on SQL Explorer, an open source, Django-based reporting and query tool for (gulp!) almost ten years. It&#x27;s a tool that fits just right for me and many others, and I love and use almost every day. Write SQL, share results, do some analysis, get insight. No surprises.<p>A live demo instance is here (no login or anything required):<p><a href=""https:&#x2F;&#x2F;demo.sqlexplorer.io&#x2F;"" rel=""nofollow"">https:&#x2F;&#x2F;demo.sqlexplorer.io&#x2F;</a><p>And here&#x27;s a fairly unprofessional, but very enthusiastic, video tour:<p><a href=""https:&#x2F;&#x2F;sql-explorer.s3.amazonaws.com&#x2F;Sql+Explorer+5.mp4"" rel=""nofollow"">https:&#x2F;&#x2F;sql-explorer.s3.amazonaws.com&#x2F;Sql+Explorer+5.mp4</a><p>The UI is constrained enough that there&#x27;s very little to learn, while there is still a surprising amount of functionality and flexibility to address a lot of use cases.<p>Some of the stuff I&#x27;m excited about in the latest version:<p>- Intuitive and obvious integration to ChatGPT &#x2F; the AI API of your choice. Doesn&#x27;t purport to be &#x27;magic&#x27;. Good prompting + relevant table scheme &amp; data automatically injected into the prompt.<p>- Create a new connection by uploading a CSV or SQLite DB as a new connection, and it&#x27;s instantly queryable. CSVs are parsed, types inferred, and a SQLite DB gets created (persisted to s3). - New and improved SQL editor with strong autocomplete (based on your schema), and some fancy keyboard shortcuts.<p>Some of the old stuff that is still great:<p>- Pivot tables in-browser, so you don&#x27;t have to open results in Excel for basic analysis. Unique URLs make everything shareable.<p>- Expose queries (optionally) as JSON endpoints. Great for prototyping APIs and scripts.<p>- All of the stuff you&#x27;d expect in a reporting tool (email reports, logging, favorites, exporting, etc.)<p>Hope you enjoy!",215,numlocked,1719934007,story,https://github.com/explorerhq/django-sql-explorer
38686221,Many options for running Mistral models in your terminal using LLM,,215,simonw,1702924055,story,https://simonwillison.net/2023/Dec/18/mistral/
43972425,"Launch HN: Miyagi (YC W25) turns YouTube videos into online, interactive courses","Hey HN, we’re Tyrone and Guang, founders of Miyagi Labs (<a href=""https:&#x2F;&#x2F;miyagilabs.ai"">https:&#x2F;&#x2F;miyagilabs.ai</a>), an AI-powered education platform that transforms educational YouTube videos into interactive courses. It helps you learn better through active practice and personalized feedback.<p>We use LLMs to automatically generate quizzes, practice questions, and real-time feedback from any educational video or resource—turning passive watching into active learning. Here’s a short demo: <a href=""https:&#x2F;&#x2F;youtu.be&#x2F;alO7FaorHOY"" rel=""nofollow"">https:&#x2F;&#x2F;youtu.be&#x2F;alO7FaorHOY</a>.<p>Improving education has always been tricky. Bloom’s 2-sigma problem (showing that a high-quality personal tutor is far more effective than conventional methods) has persisted, even as technology has advanced.<p>We met at MIT as CS majors and have always been passionate about education. Over the years, we’ve become teachers and experts in subjects like chess, algorithms, math, languages, and ninja warrior. A common theme was that we both heavily relied on YouTube to learn.<p>YouTube has incredible content for learning pretty much anything, but it’s buried in a lot of distractions. Also, passively watching videos is far less effective than taking notes, asking questions, and doing practice problems, which is what we aim to do with Miyagi Labs.<p>Our solution is essentially a multi-step function that takes in a YouTube playlist (or list of any resources) and outputs an entire course with summaries, questions, answers, and more. The pipeline is roughly: video&#x2F;resource —&gt; transcript&#x2F;text —&gt; chunks —&gt; summary and question —&gt; answers to questions, with some other features along the way.<p>We mostly use prompting and different models at each step to make the course as useful as possible. Certain topics require more practice problems vs. comprehension, and we’d use reasoning models for highly technical subjects.<p>We launched about three months ago and currently have 400+ courses and partnerships with some businesses and awesome creators. Some of our popular courses include 3Blue1Brown’s linear algebra course, a botany course on plants and ecology, and YC’s How to Start a Startup series.<p>Our product resembles classical MOOC-style course platforms in terms of UI, but is more interactive. It’s really easy to ask a question or receive custom feedback compared to a static course on Coursera. It’s also comparable to AI tutor sites, but we try to build more of a community and require less activation energy as a learner. We’re basically betting that AI can hugely improve education, but that students still want to learn from their favorite creators and want baseline shared resources for standard topics that are then augmented with personalized features.<p>You can try it here: <a href=""https:&#x2F;&#x2F;miyagilabs.ai"">https:&#x2F;&#x2F;miyagilabs.ai</a> (no login required for most courses—but if you sign up you can also create your own course).<p>We’d love your feedback on what kinds of videos&#x2F;resources you’d like to learn from, what’s missing from current learning tools, and if you know any creators or educators who would like to collaborate. Happy to hear any feedback and answer any questions!",215,bestwillcui,1747141002,story,
